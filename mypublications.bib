@inproceedings{abdalkareemWhyDevelopersUse2017,
  title = {Why {{Do Developers Use Trivial Packages}}? {{An Empirical Case Study}} on {{Npm}}},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Abdalkareem, Rabe and Nourry, Olivier and Wehaibi, Sultan and Mujahid, Suhaib and Shihab, Emad},
  date = {2017},
  series = {{{ESEC}}/{{FSE}} 2017},
  pages = {385--395},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/3106237.3106267},
  url = {http://doi.acm.org/10.1145/3106237.3106267},
  isbn = {978-1-4503-5105-8},
  venue = {Paderborn, Germany},
  keywords = {Code Reuse,Empirical Studies,JavaScript,Node.js,notion}
}

@incollection{abdollahiProbabilisticGraphicalModels2016,
  title = {Probabilistic Graphical Models for Fault Diagnosis in Complex Systems},
  booktitle = {Principles of {{Performance}} and {{Reliability Modeling}} and {{Evaluation}}},
  author = {Abdollahi, Ali and Pattipati, Krishna R and Kodali, Anuradha and Singh, Satnam and Zhang, Shigang and Luh, Peter B},
  date = {2016},
  pages = {109--139},
  publisher = {Springer},
  keywords = {notion}
}

@article{abouassiCoincidentalCorrectnessDefects4J2019,
  title = {Coincidental Correctness in the {{Defects4J}} Benchmark},
  author = {Abou Assi, Rawad and Trad, Chadi and Maalouf, Marwan and Masri, Wes},
  date = {2019},
  journaltitle = {Software Testing, Verification and Reliability},
  volume = {29},
  number = {3},
  pages = {e1696},
  doi = {10.1002/stvr.1696},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1696},
  abstract = {Summary Coincidental correctness (CC) arises when a defective program produces the correct output despite the fact that the defect within was exercised. Researchers have recognized the negative impact of CC, and the authors have previously conducted a study demonstrating its prevalence in test suites. However, that study was limited to system tests, and small subjects seeded with artificial defects. In this paper, we conduct a wider scope study of CC that addresses the following research questions in the context of the Defects4J benchmark. RQ1: Is CC prevalent in Defects4J? RQ2: Is CC affected by the testing levels in Defects4J? RQ3: Do CC tests induce peculiar infection paths in Defects4J? Furthermore, we use JTidy and NanoXML to address the following question. RQ4: Are the infections likely to be nullified within or outside the buggy method? To answer RQ1, we manually injected two code checkers for each of the 395 Defects4J defects: (i) a weak checker that detects weak CC tests by monitoring whether the defect was reached; and (ii) a strong checker that detects strong CC tests by monitoring whether the defect was reached and the program has transitioned into an infectious state. Our results showed that CC is prevalent in Defects4J, as we observed 38.1× more strong CC tests than failing tests and 60.5× more weak CC tests than failing tests. Testing has traditionally been classified into several levels that include unit, module, integration, system, and acceptance. Meanwhile, the test cases in Defects4J are not classified into any of the aforementioned testing levels. In addition, the boundaries between such levels are not clear because of the lack of a clear universal definition. Therefore, in order to answer RQ2, we derive the testing level of a test case from its method coverage information; specifically, we base it on the number and frequency of execution of the methods it covers. Our results showed that CC is present at all testing levels, but is more prevalent in high testing levels than in low testing levels. To answer RQ3, we contrasted the characteristics of the infection propagation paths induced by the Defects4J failing tests to those induced by the strong CC tests. We observed that the paths induced by the CC tests (i) were considerably longer on average and (ii) comprised a higher number of conditional, modulo, multiplication, division, and invocation statements. Finally, to answer RQ4, which relates to RQ2, we performed an experiment involving JTidy, NanoXML, and their associated high-level test suites. We used code checkers to determine whether, in the case of strong CC, the infections were nullified before exiting the buggy function or afterward. All of our observations showed that the infections were nullified after exiting the buggy function. \textbackslash copyright 2019 John Wiley \& Sons, Ltd.},
  keywords = {coincidental correctness,failed error propagation,fault masking,integration testing,notion,system testing,testing level,unit testing}
}

@article{abouassiHowDetrimentalCoincidental2021,
  title = {How Detrimental Is Coincidental Correctness to Coverage-Based Fault Detection and Localization? {{An}} Empirical Study},
  author = {Abou Assi, Rawad and Masri, Wes and Trad, Chadi},
  date = {2021},
  journaltitle = {Software Testing, Verification and Reliability},
  pages = {e1762},
  publisher = {Wiley Online Library},
  keywords = {notion}
}

@inproceedings{abreuAccuracySpectrumbasedFault2007,
  title = {On the {{Accuracy}} of {{Spectrum-based Fault Localization}}},
  booktitle = {Testing: {{Academic}} and {{Industrial Conference Practice}} and {{Research Techniques}} - {{MUTATION}} ({{TAICPART-MUTATION}} 2007)},
  author = {Abreu, R. and Zoeteweij, P. and family=Gemund, given=A. J. C., prefix=van, useprefix=false},
  date = {2007},
  pages = {89--98},
  doi = {10.1109/TAIC.PART.2007.13},
  keywords = {notion}
}

@article{abreuSimultaneousDebuggingSoftware2011,
  title = {Simultaneous Debugging of Software Faults},
  author = {Abreu, Rui and Zoeteweij, Peter and family=Gemund, given=Arjan J. C., prefix=van, useprefix=false},
  date = {2011},
  journaltitle = {Journal of Systems and Software},
  volume = {84},
  number = {4},
  pages = {573--586},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2010.11.915},
  url = {http://www.sciencedirect.com/science/article/pii/S0164121210003183},
  abstract = {(Semi-)automated diagnosis of software faults can drastically increase debugging efficiency, improving reliability and time-to-market. Current automatic diagnosis techniques are predominantly of a statistical nature and, despite typical defect densities, do not explicitly consider multiple faults, as also demonstrated by the popularity of the single-fault benchmark set of programs. We present a reasoning approach, called Zoltar-M(ultiple fault), that yields multiple-fault diagnoses, ranked in order of their probability. Although application of Zoltar-M to programs with many faults requires heuristics (trading-off completeness) to reduce the inherent computational complexity, theory as well as experiments on synthetic program models and multiple-fault program versions available from the software infrastructure repository (SIR) show that for multiple-fault programs this approach can outperform statistical techniques, notably spectrum-based fault localization (SFL). As a side-effect of this research, we present a new SFL variant, called Zoltar-S(ingle fault), that is optimal for single-fault programs, outperforming all other variants known to date.},
  keywords = {notion,Program spectra,Software fault diagnosis,Statistical and reasoning approaches}
}

@inproceedings{acharyaOptimalProbabilityEstimation2013,
  title = {Optimal {{Probability Estimation}} with {{Applications}} to {{Prediction}} and {{Classification}}},
  booktitle = {Proceedings of the 26th {{Annual Conference}} on {{Learning Theory}}},
  author = {Acharya, Jayadev and Jafarpour, Ashkan and Orlitsky, Alon and Suresh, Ananda Theertha},
  editor = {Shalev-Shwartz, Shai and Steinwart, Ingo},
  date = {2013-06-12},
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {30},
  pages = {764--796},
  publisher = {PMLR},
  location = {Princeton, NJ, USA},
  url = {https://proceedings.mlr.press/v30/Acharya13.html},
  abstract = {Via a unified viewpoint of probability estimation, classification,and prediction, we derive a uniformly-optimal combined-probability estimator, construct a classifier that uniformly approaches the error of the best possible label-invariant classifier, and improve existing results on pattern prediction and compression.},
  keywords = {notion}
}

@inproceedings{acharyaPracticalChangeImpact2011,
  title = {Practical {{Change Impact Analysis Based}} on {{Static Program Slicing}} for {{Industrial Software Systems}}},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Software Engineering}}},
  author = {Acharya, Mithun and Robinson, Brian},
  date = {2011},
  series = {{{ICSE}} '11},
  pages = {746--755},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/1985793.1985898},
  url = {http://doi.acm.org/10.1145/1985793.1985898},
  isbn = {978-1-4503-0445-0},
  venue = {Waikiki, Honolulu, HI, USA},
  keywords = {Change Impact Analysis,notion,Program Slicing,Static Program Slicing}
}

@article{aghamohammadiEnsemblebasedPredictiveMutation2021,
  title = {An Ensemble-Based Predictive Mutation Testing Approach That Considers Impact of Unreached Mutants},
  author = {Aghamohammadi, Alireza and Mirian-Hosseinabadi, Seyed-Hassan},
  date = {2021},
  journaltitle = {Software Testing, Verification and Reliability},
  pages = {e1784},
  publisher = {Wiley Online Library},
  keywords = {notion}
}

@article{aghaSurveyStatisticalModel2018,
  title = {A {{Survey}} of {{Statistical Model Checking}}},
  author = {Agha, Gul and Palmskog, Karl},
  date = {2018-01},
  journaltitle = {ACM Trans. Model. Comput. Simul.},
  volume = {28},
  number = {1},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {1049-3301},
  doi = {10.1145/3158668},
  url = {https://doi.org/10.1145/3158668},
  abstract = {Interactive, distributed, and embedded systems often behave stochastically, for example, when inputs, message delays, or failures conform to a probability distribution. However, reasoning analytically about the behavior of complex stochastic systems is generally infeasible. While simulations of systems are commonly used in engineering practice, they have not traditionally been used to reason about formal specifications. Statistical model checking (SMC) addresses this weakness by using a simulation-based approach to reason about precise properties specified in a stochastic temporal logic. A specification for a communication system may state that within some time bound, the probability that the number of messages in a queue will be greater than 5 must be less than 0.01. Using SMC, executions of a stochastic system are first sampled, after which statistical techniques are applied to determine whether such a property holds. While the output of sample-based methods are not always correct, statistical inference can quantify the confidence in the result produced. In effect, SMC provides a more widely applicable and scalable alternative to analysis of properties of stochastic systems using numerical and symbolic methods. SMC techniques have been successfully applied to analyze systems with large state spaces in areas such as computer networking, security, and systems biology. In this article, we survey SMC algorithms, techniques, and tools, while emphasizing current limitations and tradeoffs between precision and scalability.},
  keywords = {estimation,hypothesis testing,notion,simulation,Statistical model checking,temporal logic}
}

@article{agrawalDynamicProgramSlicing1990,
  title = {Dynamic {{Program Slicing}}},
  author = {Agrawal, Hiralal and Horgan, Joseph R.},
  date = {1990-06},
  journaltitle = {SIGPLAN Not.},
  volume = {25},
  number = {6},
  pages = {246--256},
  publisher = {ACM},
  location = {New York, NY, USA},
  issn = {0362-1340},
  doi = {10.1145/93548.93576},
  url = {http://doi.acm.org/10.1145/93548.93576},
  keywords = {notion}
}

@inproceedings{agrawalFaultLocalizationUsing1995,
  title = {Fault Localization Using Execution Slices and Dataflow Tests},
  booktitle = {Proceedings of {{Sixth International Symposium}} on {{Software Reliability Engineering}}. {{ISSRE}}'95},
  author = {Agrawal, H. and Horgan, J. R. and London, S. and Wong, W. E.},
  date = {1995-10},
  pages = {143--151},
  issn = {1071-9458},
  doi = {10.1109/ISSRE.1995.497652},
  keywords = {Cities and towns,Computer bugs,data flow analysis,dataflow tests,Debugging,execution dicing,execution slices,execution slicing,Fault detection,heuristic techniques,notion,program debugging,program fault localization,program purpose,program semantics,program structure,program testing,program understanding,Programming profession,reverse engineering,software reliability,software tool,software tools,test cases,Testing}
}

@article{ahmedLearningLenientParsing2021,
  title = {Learning Lenient Parsing \& Typing via Indirect Supervision},
  author = {Ahmed, Toufique and Devanbu, Premkumar and Hellendoorn, Vincent J.},
  date = {2021},
  journaltitle = {Empirical Software Engineering},
  volume = {26},
  number = {2},
  pages = {29},
  doi = {10.1007/s10664-021-09942-y},
  url = {https://doi.org/10.1007/s10664-021-09942-y},
  abstract = {Both professional coders and teachers frequently deal with imperfect (fragmentary, incomplete, ill-formed) code. Such fragments are common in StackOverflow; students also frequently produce ill-formed code, for which instructors, TAs (or students themselves) must find repairs. In either case, the developer experience could be greatly improved if such code could somehow be parsed \& typed; this makes such code more amenable to use within IDEs and allows early detection and repair of potential errors. We introduce a lenient parser, which can parse \& type fragments, even ones with simple errors. Training a machine learner to leniently parse and type imperfect code requires a large training set including many pairs of imperfect code and its repair (and/or type information); such training sets are limited by human effort and curation. In this paper, we present a novel, indirectly supervised, approach to train a lenient parser, without access to such human-curated training data. We leverage the huge corpus of mostly correct code available on Github, and the massive, efficient learning capacity of Transformer-based NN architectures. Using GitHub data, we first create a large dataset of fragments of code and corresponding tree fragments and type annotations; we then randomly corrupt the input fragments (while requiring correct output) by seeding errors that mimic corruptions found in StackOverflow and student data. Using this data, we train high-capacity transformer models to overcome both fragmentation and corruption. With this novel approach, we can achieve reasonable performance on parsing \& typing StackOverflow fragments; we also demonstrate that our approach performs well on shorter student error program and achieves best-in-class performance on longer programs that have more than 400 tokens. We also show that by blending DeepFix and our tool, we could achieve 77\% accuracy, which outperforms all previously reported student error correction tools.},
  isbn = {1573-7616},
  keywords = {notion}
}

@inproceedings{ahmedMandolineDynamicSlicing2021,
  title = {Mandoline: {{Dynamic Slicing}} of {{Android Applications}} with {{Trace-Based Alias Analysis}}},
  booktitle = {2021 14th {{IEEE Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}} ({{ICST}})},
  author = {Ahmed, Khaled and Lis, Mieszko and Rubin, Julia},
  date = {2021},
  pages = {105--115},
  publisher = {IEEE},
  keywords = {notion}
}

@inproceedings{aichernigProbabilisticBlackBoxReachability2017,
  title = {Probabilistic {{Black-Box Reachability Checking}}},
  booktitle = {{{RV}}},
  author = {Aichernig, Bernhard K. and Tappler, Martin},
  date = {2017},
  keywords = {notion}
}

@article{aichernigProbabilisticBlackboxReachability2019,
  title = {Probabilistic Black-Box Reachability Checking (Extended Version)},
  author = {Aichernig, Bernhard K. and Tappler, Martin},
  date = {2019},
  journaltitle = {Formal Methods in System Design},
  pages = {1--33},
  keywords = {notion}
}

@article{al-mohairHybridHumanSkin2015,
  title = {Hybrid {{Human Skin Detection Using Neural Network}} and {{K-Means Clustering Technique}}},
  author = {Al-Mohair, Hani K. and Saleh, Junita Mohamad and Suandi, Shahrel Azmin},
  date = {2015},
  journaltitle = {Applied Soft Computing},
  volume = {33},
  pages = {337--347},
  issn = {1568-4946},
  doi = {10.1016/j.asoc.2015.04.046},
  url = {http://www.sciencedirect.com/science/article/pii/S1568494615002732},
  abstract = {Abstract Human skin detection is an essential step in most human detection applications, such as face detection. The performance of any skin detection system depends on assessment of two components: feature extraction and detection method. Skin color is a robust cue used for human skin detection. However, the performance of color-based detection methods is constrained by the overlapping color spaces of skin and non-skin pixels. To increase the accuracy of skin detection, texture features can be exploited as additional cues. In this paper, we propose a hybrid skin detection method based on YIQ color space and the statistical features of skin. A Multilayer Perceptron artificial neural network, which is a universal classifier, is combined with the k-means clustering method to accurately detect skin. The experimental results show that the proposed method can achieve high accuracy with an F1-measure of 87.82\% based on images from the ECU database.},
  issue = {Supplement C},
  keywords = {Color space,k-Means,Neural networks,notion,Skin color detection,Texture analysis}
}

@misc{alasmariQuantitativeVerificationAdaptive2021,
  title = {Quantitative {{Verification}} with {{Adaptive Uncertainty Reduction}}},
  author = {Alasmari, Naif and Calinescu, Radu and Paterson, Colin and Mirandola, Raffaela},
  date = {2021},
  keywords = {notion}
}

@article{alfadelEmpiricalAnalysisSecurity2023,
  title = {Empirical Analysis of Security Vulnerabilities in {{Python}} Packages},
  author = {Alfadel, Mahmoud and Costa, Diego Elias and Shihab, Emad},
  date = {2023-03-25},
  journaltitle = {Empirical Software Engineering},
  volume = {28},
  number = {3},
  pages = {59},
  doi = {10.1007/s10664-022-10278-4},
  url = {https://doi.org/10.1007/s10664-022-10278-4},
  abstract = {Software ecosystems play an important role in modern software development, providing an open platform of reusable packages that speed up and facilitate development tasks. However, this level of code reusability supported by software ecosystems also makes the discovery of security vulnerabilities much more difficult, as software systems depend on an increasingly high number of packages. Recently, security vulnerabilities in the npm ecosystem, the ecosystem of Node.js packages, have been studied in the literature. As different software ecosystems embody different programming languages and particularities, we argue that it is also important to study other popular programming languages to build stronger empirical evidence about vulnerabilities in software ecosystems. In this paper, we present an empirical study of 1,396 vulnerability reports affecting 698 Python packages in the Python ecosystem (PyPi). In particular, we study the propagation and life span of security vulnerabilities, accounting for how long they take to be discovered and fixed. In addition, vulnerabilities in packages may affect software projects that depend on them (dependent projects), making them vulnerable too. We study a set of 2,224 GitHub Python projects, to better understand the prevalence of vulnerabilities in their dependencies and how fast it takes to update them. Our findings show that the discovered vulnerabilities in Python packages are increasing over time, and they take more than 3 years to be discovered. A large portion of these vulnerabilities (40.86\%) are only fixed after being publicly announced, giving ample time for attackers exploitation. Moreover, we find that more than half of the dependent projects rely on at least one vulnerable package, taking a considerably long time (7 months) to update to a non-vulnerable version. We find similarities in some characteristics of vulnerabilities in PyPi and npm and divergences that can be attributed to specific PyPi policies. By leveraging our findings, we provide a series of implications that can help the security of software ecosystems by improving the process of discovering, fixing and managing package vulnerabilities.},
  isbn = {1573-7616},
  keywords = {notion}
}

@misc{alhanahnahLightweightMultiStageCompilerAssisted2021,
  title = {Lightweight, {{Multi-Stage}}, {{Compiler-Assisted Application Specialization}}},
  author = {Alhanahnah, Mohannad and Jain, Rithik and Rastogi, Vaibhav and Jha, Somesh and Reps, Thomas},
  date = {2021},
  keywords = {notion}
}

@article{allamanisLearningRepresentPrograms2017,
  title = {Learning to {{Represent Programs}} with {{Graphs}}},
  author = {Allamanis, Miltiadis and Brockschmidt, Marc and Khademi, Mahmoud},
  date = {2017},
  journaltitle = {CoRR},
  volume = {abs/1711.00740},
  url = {http://arxiv.org/abs/1711.00740},
  keywords = {notion}
}

@inproceedings{almaghairbeAutomaticallyClassifyingTest2016,
  title = {Automatically {{Classifying Test Results}} by {{Semi-Supervised Learning}}},
  booktitle = {2016 {{IEEE}} 27th {{International Symposium}} on {{Software Reliability Engineering}} ({{ISSRE}})},
  author = {Almaghairbe, R. and Roper, M.},
  date = {2016-10},
  pages = {116--126},
  doi = {10.1109/ISSRE.2016.22},
  abstract = {A key component of software testing is deciding whether a test case has passed or failed: an expensive and error-prone manual activity. We present an approach to automatically classify passing and failing executions using semi-supervised learning on dynamic execution data (test inputs/outputs and execution traces). A small proportion of the test data is labelled as passing or failing and used in conjunction with the unlabelled data to build a classifier which labels the remaining outputs (classify them as passing or failing tests). A range of learning algorithms are investigated using several faulty versions of three systems along with varying types of data (inputs/outputs alone, or in combination with execution traces) and different labelling strategies (both failing and passing tests, and passing tests alone). The results show that in many cases labelling just a small proportion of the test cases - as low as 10\% - is sufficient to build a classifier that is able to correctly categorise the large majority of the remaining test cases. This has important practical potential: when checking the test results from a system a developer need only examine a small proportion of these and use this information to train a learning algorithm to automatically classify the remainder.},
  keywords = {notion,Semi-Supervised Learning,Test Classification}
}

@inproceedings{almullaLearningHowSearch2020,
  title = {Learning {{How}} to {{Search}}: {{Generating Exception-Triggering Tests Through Adaptive Fitness Function Selection}}},
  booktitle = {2020 {{IEEE}} 13th {{International Conference}} on {{Software Testing}}, {{Validation}} and {{Verification}} ({{ICST}})},
  author = {Almulla, H. and Gay, G.},
  date = {2020},
  pages = {63--73},
  doi = {10.1109/ICST46399.2020.00017},
  keywords = {notion}
}

@misc{alonCode2seqGeneratingSequences2018,
  title = {Code2seq: {{Generating Sequences}} from {{Structured Representations}} of {{Code}}},
  author = {Alon, Uri and Brody, Shaked and Levy, Omer and Yahav, Eran},
  date = {2018},
  keywords = {notion}
}

@article{alonCode2vecLearningDistributed2018,
  title = {Code2vec: {{Learning Distributed Representations}} of {{Code}}},
  author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
  date = {2018},
  journaltitle = {CoRR},
  volume = {abs/1803.09473},
  url = {http://arxiv.org/abs/1803.09473},
  keywords = {notion}
}

@online{alquierUserfriendlyIntroductionPACBayes2021a,
  title = {User-Friendly Introduction to {{PAC-Bayes}} Bounds},
  author = {Alquier, Pierre},
  date = {2021-10-21},
  url = {https://arxiv.org/abs/2110.11216v5},
  urldate = {2024-09-20},
  abstract = {Aggregated predictors are obtained by making a set of basic predictors vote according to some weights, that is, to some probability distribution. Randomized predictors are obtained by sampling in a set of basic predictors, according to some prescribed probability distribution. Thus, aggregated and randomized predictors have in common that they are not defined by a minimization problem, but by a probability distribution on the set of predictors. In statistical learning theory, there is a set of tools designed to understand the generalization ability of such procedures: PAC-Bayesian or PAC-Bayes bounds. Since the original PAC-Bayes bounds of D. McAllester, these tools have been considerably improved in many directions (we will for example describe a simplified version of the localization technique of O. Catoni that was missed by the community, and later rediscovered as "mutual information bounds"). Very recently, PAC-Bayes bounds received a considerable attention: for example there was workshop on PAC-Bayes at NIPS 2017, "(Almost) 50 Shades of Bayesian Learning: PAC-Bayesian trends and insights", organized by B. Guedj, F. Bach and P. Germain. One of the reason of this recent success is the successful application of these bounds to neural networks by G. Dziugaite and D. Roy. An elementary introduction to PAC-Bayes theory is still missing. This is an attempt to provide such an introduction.},
  langid = {english},
  organization = {arXiv.org},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/QBUD66GV/Alquier - 2021 - User-friendly introduction to PAC-Bayes bounds.pdf}
}

@article{alshahwanDetectingMalwareInformation2015,
  title = {Detecting {{Malware}} with {{Information Complexity}}},
  author = {Alshahwan, Nadia and Barr, Earl T. and Clark, David and Danezis, George},
  date = {2015-02},
  url = {https://arxiv.org/abs/1502.07661},
  abstract = {This work focuses on a specific front of the malware detection arms-race, namely the detection of persistent, disk-resident malware. We exploit normalised compression distance (NCD), an information theoretic measure, applied directly to binaries. Given a zoo of labelled malware and benign-ware, we ask whether a suspect program is more similar to our malware or to our benign-ware. Our approach classifies malware with 97.1\% accuracy and a false positive rate of 3\%. We achieve our results with off-the-shelf compressors and a standard machine learning classifier and without any specialised knowledge. An end-user need only collect a zoo of malware and benign-ware and then can immediately apply our techniques.},
  keywords = {Malware Detection,Normalised Compression Distance (NCD),notion}
}

@inproceedings{alshammariFlakeFlaggerPredictingFlakiness2021,
  title = {{{FlakeFlagger}}: {{Predicting Flakiness Without Rerunning Tests}}},
  booktitle = {International {{Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Alshammari, Abdulrahman and Morris, Christopher and Hiltonand, Michael and Bell, Jonathan},
  date = {2021},
  keywords = {notion}
}

@inproceedings{alvimMeasuringInformationLeakage2012,
  title = {Measuring {{Information Leakage Using Generalized Gain Functions}}},
  booktitle = {2012 {{IEEE}} 25th {{Computer Security Foundations Symposium}}},
  author = {Alvim, M'rio S. and Chatzikokolakis, Kostas and Palamidessi, Catuscia and Smith, Geoffrey},
  date = {2012},
  pages = {265--279},
  doi = {10.1109/CSF.2012.26},
  keywords = {Computer security,Databases,Entropy,Gain measurement,Lattices,notion,Probabilistic logic}
}

@article{amadiniSurveyStringConstraint2021,
  title = {A {{Survey}} on {{String Constraint Solving}}},
  author = {Amadini, Roberto},
  date = {2021-11},
  journaltitle = {ACM Comput. Surv.},
  volume = {55},
  number = {1},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {0360-0300},
  doi = {10.1145/3484198},
  url = {https://doi.org/10.1145/3484198},
  abstract = {String constraint solving refers to solving combinatorial problems involving constraints over string variables. String solving approaches have become popular over the past few years given the massive use of strings in different application domains like formal analysis, automated testing, database query processing, and cybersecurity.This article reports a comprehensive survey on string constraint solving by exploring the large number of approaches that have been proposed over the past few decades to solve string constraints.},
  keywords = {automata theory,constraint programming,notion,satisfiability modulo theories,software analysis,String constraint solving}
}

@online{AmazonEC2Cloud,
  title = {Amazon {{EC2}} - {{Cloud Compute Capacity}} - {{AWS}}},
  url = {https://aws.amazon.com/ec2/},
  urldate = {2024-11-12},
  abstract = {Amazon EC2 provides secure, resizable compute in the cloud, offering the broadest choice of processor, storage, networking, OS, and purchase model.},
  langid = {american},
  organization = {Amazon Web Services, Inc.}
}

@inproceedings{amidonProgramFractureRecombination2015,
  title = {Program Fracture and Recombination for Efficient Automatic Code Reuse},
  booktitle = {2015 {{IEEE High Performance Extreme Computing Conference}} ({{HPEC}})},
  author = {Amidon, P. and Davis, E. and Sidiroglou-Douskos, S. and Rinard, M.},
  date = {2015},
  pages = {1--6},
  doi = {10.1109/HPEC.2015.7396314},
  keywords = {notion}
}

@book{ammannIntroductionSoftwareTesting2016,
  title = {Introduction to {{Software Testing}}},
  author = {Ammann, P. and Offutt, J.},
  date = {2016},
  publisher = {Cambridge University Press},
  url = {https://books.google.de/books?id=58LeDQAAQBAJ},
  isbn = {978-1-316-77312-3},
  keywords = {notion}
}

@article{amtoftTheorySlicingImperative2020,
  title = {A {{Theory}} of {{Slicing}} for {{Imperative Probabilistic Programs}}},
  author = {Amtoft, Torben and Banerjee, Anindya},
  date = {2020},
  journaltitle = {ACM Transactions on Programming Languages and Systems (TOPLAS)},
  volume = {42},
  pages = {1--71},
  keywords = {notion}
}

@inproceedings{andersonInequalitiesBinomialPoisson1967,
  title = {Some Inequalities among Binomial and {{Poisson}} Probabilities},
  booktitle = {Proceedings of the {{Fifth Berkeley Symposium}} on {{Mathematical Statistics}} and {{Probability}}, {{Volume}} 1: {{Statistics}}},
  author = {Anderson, Theodore W and Samuels, Stephen M},
  date = {1967},
  volume = {5},
  pages = {1--13},
  publisher = {University of California Press},
  keywords = {notion}
}

@inproceedings{andresGeoindistinguishabilityDifferentialPrivacy2013,
  title = {Geo-Indistinguishability: Differential Privacy for Location-Based Systems},
  booktitle = {Proceedings of the 2013 {{ACM SIGSAC Conference}} on {{Computer}} \& {{Communications Security}}},
  author = {Andrés, Miguel E. and Bordenabe, Nicolás E. and Chatzikokolakis, Konstantinos and Palamidessi, Catuscia},
  date = {2013},
  series = {{{CCS}} '13},
  pages = {901--914},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2508859.2516735},
  url = {https://doi.org/10.1145/2508859.2516735},
  abstract = {The growing popularity of location-based systems, allowing unknown/untrusted servers to easily collect huge amounts of information regarding users' location, has recently started raising serious privacy concerns. In this paper we introduce geoind, a formal notion of privacy for location-based systems that protects the user's exact location, while allowing approximate information – typically needed to obtain a certain desired service – to be released.This privacy definition formalizes the intuitive notion of protecting the user's location within a radius r with a level of privacy that depends on r, and corresponds to a generalized version of the well-known concept of differential privacy. Furthermore, we present a mechanism for achieving geoind by adding controlled random noise to the user's location.We describe how to use our mechanism to enhance LBS applications with geo-indistinguishability guarantees without compromising the quality of the application results. Finally, we compare state-of-the-art mechanisms from the literature with ours. It turns out that, among all mechanisms independent of the prior, our mechanism offers the best privacy guarantees.},
  isbn = {978-1-4503-2477-9},
  venue = {Berlin, Germany},
  keywords = {differential privacy,location obfuscation,location privacy,location-based services,notion,planar laplace distribution}
}

@inproceedings{andrewhabibHowManyAll2018,
  title = {How {{Many}} of {{All Bugs Do We Find}}? {{A Study}} of {{Static Bug Detectors}}},
  booktitle = {In {{Proceedings}} of the 2018 33rd {{ACM}}/{{IEEE International Conference}} on {{Automated Software Engineering}} ({{ASE}} '18)},
  author = {Andrew Habib, Michael Pradel},
  date = {2018},
  keywords = {notion}
}

@inproceedings{androutsopoulosAnalysisRelationshipConditional2014,
  title = {An {{Analysis}} of the {{Relationship}} between {{Conditional Entropy}} and {{Failed Error Propagation}} in {{Software Testing}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Software Engineering}}},
  author = {Androutsopoulos, Kelly and Clark, David and Dan, Haitao and Hierons, Robert M. and Harman, Mark},
  date = {2014},
  series = {{{ICSE}} 2014},
  pages = {573--583},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2568225.2568314},
  url = {https://doi.org/10.1145/2568225.2568314},
  abstract = {Failed error propagation (FEP) is known to hamper software testing, yet it remains poorly understood. We introduce an information theoretic formulation of FEP that is based on measures of conditional entropy. This formulation considers the situation in which we are interested in the potential for an incorrect program state at statement s to fail to propagate to incorrect output. We define five metrics that differ in two ways: whether we only consider parts of the program that can be reached after executing s and whether we restrict attention to a single program path of interest .We give the results of experiments in which it was found that on average one in 10 tests suffered from FEP, earlier studies having shown that this figure can vary significantly between programs. The experiments also showed that our metrics are well-correlated with FEP. Our empirical study involved 30 programs, for which we executed a total of 7,140,000 test cases. The results reveal that the metrics differ in their performance but the Spearman rank correlation with failed error propagation is close to 0.95 for two of the metrics. These strong correlations in an experimental setting, in which all information about both FEP and conditional entropy is known, open up the possibility in the longer term of devising inexpensive information theory based metrics that allow us to minimise the effect of FEP.},
  isbn = {978-1-4503-2756-5},
  venue = {Hyderabad, India},
  keywords = {Information Theory,notion,Program Analysis}
}

@unpublished{anFonteFindingBug2022,
  title = {Fonte: {{Finding Bug Inducing Commits}} from {{Failures}}},
  author = {An, Gabin and Hong, Jingun and Kim, Naryeong and Yoo, Shin},
  date = {2022},
  eprint = {2212.06376},
  eprinttype = {arXiv},
  keywords = {notion}
}

@inproceedings{anPYGGIPythonGeneral2017,
  title = {PYGGI: Python General Framework for Genetic Improvement},
  shorttitle = {PYGGI},
  author = {An, Gabin and Kim, Jinhan and Lee, Seongmin and Yoo, Shin},
  date = {2017-12},
  journaltitle = {한국정보과학회 학술발표논문집},
  booktitle = {Korea Computer Congress 2017},
  year = {2017},
  pages = {536--538},
  url = {https://www.dbpia.co.kr},
  urldate = {2024-09-19},
  abstract = {논문, 학술저널 검색 플랫폼 서비스},
  langid = {korean-han},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/Y2VXKFTG/articleDetail.html}
}

@inproceedings{anReducingSearchSpace2021,
  title = {Reducing the Search Space of Bug Inducing Commits Using Failure Coverage},
  booktitle = {Proceedings of the 29th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {An, Gabin and Yoo, Shin},
  date = {2021},
  series = {{{ESEC}}/{{FSE}} 2021},
  pages = {1459--1462},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3468264.3473129},
  url = {https://doi.org/10.1145/3468264.3473129},
  abstract = {Knowing how exactly a bug has been introduced into the code can help developers debug the bug efficiently. However, techniques currently used to retrieve Bug Inducing Commits (BICs) from the repository timeline are limited in their accuracy. Automated bisection of the version history depends on the bug revealing test case being executable against all candidate previous versions, whereas blaming the last commits that touched the same parts as the fixing commit (à la SZZ) requires that the bug has already been fixed. We show that filtering commits using the coverage of the bug revealing test cases can effectively reduce the search space for both bisection and SZZ-like blame models by 87.6\% and 27.9\%, respectively, significantly reducing the cost of BIC retrieval. The application of our approach to bugs in Defects4J also reveals inconsistencies in some of their BICs known in the literature.},
  isbn = {978-1-4503-8562-6},
  venue = {Athens, Greece},
  keywords = {bug inducing commit,notion,test coverage}
}

@inproceedings{antonopoulosDecompositionInsteadSelfComposition2017,
  title = {Decomposition {{Instead}} of {{Self-Composition}} for {{Proving}} the {{Absence}} of {{Timing Channels}}},
  booktitle = {Proceedings of the 38th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Antonopoulos, Timos and Gazzillo, Paul and Hicks, Michael and Koskinen, Eric and Terauchi, Tachio and Wei, Shiyi},
  date = {2017},
  series = {{{PLDI}} 2017},
  pages = {362--375},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3062341.3062378},
  url = {https://doi.org/10.1145/3062341.3062378},
  abstract = {We present a novel approach to proving the absence of timing channels. The idea is to partition the program's execution traces in such a way that each partition component is checked for timing attack resilience by a time complexity analysis and that per-component resilience implies the resilience of the whole program. We construct a partition by splitting the program traces at secret-independent branches. This ensures that any pair of traces with the same public input has a component containing both traces. Crucially, the per-component checks can be normal safety properties expressed in terms of a single execution. Our approach is thus in contrast to prior approaches, such as self-composition, that aim to reason about multiple (k≥ 2) executions at once. We formalize the above as an approach called quotient partitioning, generalized to any k-safety property, and prove it to be sound. A key feature of our approach is a demand-driven partitioning strategy that uses a regex-like notion called trails to identify sets of execution traces, particularly those influenced by tainted (or secret) data. We have applied our technique in a prototype implementation tool called Blazer, based on WALA, PPL, and the brics automaton library. We have proved timing-channel freedom of (or synthesized an attack specification for) 24 programs written in Java bytecode, including 6 classic examples from the literature and 6 examples extracted from the DARPA STAC challenge problems.},
  isbn = {978-1-4503-4988-8},
  venue = {Barcelona, Spain},
  keywords = {Blazer,Decomposition,notion,Subtrails,Timing Attacks,Verification}
}

@article{antonopoulosDecompositionInsteadSelfComposition2017a,
  title = {Decomposition {{Instead}} of {{Self-Composition}} for {{Proving}} the {{Absence}} of {{Timing Channels}}},
  author = {Antonopoulos, Timos and Gazzillo, Paul and Hicks, Michael and Koskinen, Eric and Terauchi, Tachio and Wei, Shiyi},
  date = {2017-06},
  journaltitle = {SIGPLAN Not.},
  volume = {52},
  number = {6},
  pages = {362--375},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {0362-1340},
  doi = {10.1145/3140587.3062378},
  url = {https://doi.org/10.1145/3140587.3062378},
  abstract = {We present a novel approach to proving the absence of timing channels. The idea is to partition the program's execution traces in such a way that each partition component is checked for timing attack resilience by a time complexity analysis and that per-component resilience implies the resilience of the whole program. We construct a partition by splitting the program traces at secret-independent branches. This ensures that any pair of traces with the same public input has a component containing both traces. Crucially, the per-component checks can be normal safety properties expressed in terms of a single execution. Our approach is thus in contrast to prior approaches, such as self-composition, that aim to reason about multiple (k≥ 2) executions at once. We formalize the above as an approach called quotient partitioning, generalized to any k-safety property, and prove it to be sound. A key feature of our approach is a demand-driven partitioning strategy that uses a regex-like notion called trails to identify sets of execution traces, particularly those influenced by tainted (or secret) data. We have applied our technique in a prototype implementation tool called Blazer, based on WALA, PPL, and the brics automaton library. We have proved timing-channel freedom of (or synthesized an attack specification for) 24 programs written in Java bytecode, including 6 classic examples from the literature and 6 examples extracted from the DARPA STAC challenge problems.},
  keywords = {Blazer,Decomposition,notion,Subtrails,Timing Attacks,Verification}
}

@online{antunesReproducibilityReplicabilityRepeatability2024,
  title = {Reproducibility, {{Replicability}}, and {{Repeatability}}: {{A}} Survey of Reproducible Research with a Focus on High Performance Computing},
  shorttitle = {Reproducibility, {{Replicability}}, and {{Repeatability}}},
  author = {Antunes, Benjamin A. and Hill, David R. C.},
  date = {2024-09-13},
  eprint = {2402.07530},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2402.07530},
  url = {http://arxiv.org/abs/2402.07530},
  urldate = {2024-11-06},
  abstract = {Reproducibility is widely acknowledged as a fundamental principle in scientific research. Currently, the scientific community grapples with numerous challenges associated with reproducibility, often referred to as the ''reproducibility crisis.'' This crisis permeated numerous scientific disciplines. In this study, we examined the factors in scientific practices that might contribute to this lack of reproducibility. Significant focus is placed on the prevalent integration of computation in research, which can sometimes function as a black box in published papers. Our study primarily focuses on highperformance computing (HPC), which presents unique reproducibility challenges. This paper provides a comprehensive review of these concerns and potential solutions. Furthermore, we discuss the critical role of reproducible research in advancing science and identifying persisting issues within the field of HPC.},
  pubstate = {prepublished},
  keywords = {Computer Science - Software Engineering,notion},
  file = {/Users/bohrok/Zotero/storage/PFQK9LW4/Antunes and Hill - 2024 - Reproducibility, Replicability, and Repeatability A survey of reproducible research with a focus on.pdf;/Users/bohrok/Zotero/storage/XE2DAPFN/2402.html}
}

@misc{aounBugCharacteristicsQuantum2022,
  title = {Bug {{Characteristics}} in {{Quantum Software Ecosystem}}},
  author = {{aoun}, Mohamed Raed El and Li, Heng and Khomh, Foutse and Tidjon, Lionel},
  date = {2022},
  doi = {10.48550/ARXIV.2204.11965},
  url = {https://arxiv.org/abs/2204.11965},
  organization = {arXiv},
  keywords = {FOS: Computer and information sciences,notion,Software Engineering (cs.SE)}
}

@article{arcuriExperienceReportApplying2018,
  title = {An Experience Report on Applying Software Testing Academic Results in Industry: We Need Usable Automated Test Generation},
  author = {Arcuri, Andrea},
  date = {2018-08-01},
  journaltitle = {Empirical Software Engineering},
  volume = {23},
  number = {4},
  pages = {1959--1981},
  doi = {10.1007/s10664-017-9570-9},
  url = {https://doi.org/10.1007/s10664-017-9570-9},
  abstract = {What is the impact of software engineering research on current practices in industry? In this paper, I report on my direct experience as a PhD/post-doc working in software engineering research projects, and then spending the following five years as an engineer in two different companies (the first one being the same I worked in collaboration with during my post-doc). Given a background in software engineering research, what cutting-edge techniques and tools from academia did I use in my daily work when developing and testing the systems of these companies? Regarding validation and verification (my main area of research), the answer is rather short: as far as I can tell, only FindBugs. In this paper, I report on why this was the case, and discuss all the challenging, complex open problems we face in industry and which somehow are “neglected”in the academic circles. In particular, I will first discuss what actual tools I could use in my daily work, such as JaCoCo and Selenium. Then, I will discuss the main open problems I faced, particularly related to environment simulators, unit and web testing. After that, popular topics in academia are presented, such as UML, regression and mutation testing. Their lack of impact on the type of projects I worked on in industry is then discussed. Finally, from this industrial experience, I provide my opinions about how this situation can be improved, in particular related to how academics are evaluated, and advocate for a greater involvement into open-source projects.},
  isbn = {1573-7616},
  keywords = {notion}
}

@article{arcuriPracticalGuideUsing2011,
  title = {A Practical Guide for Using Statistical Tests to Assess Randomized Algorithms in Software Engineering},
  author = {Arcuri, Andrea and Briand, Lionel Claude},
  date = {2011},
  journaltitle = {2011 33rd International Conference on Software Engineering (ICSE)},
  pages = {1--10},
  keywords = {notion}
}

@inproceedings{arcuriRESTfulAPIAutomated2017,
  title = {{{RESTful API Automated Test Case Generation}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Software Quality}}, {{Reliability}} and {{Security}} ({{QRS}})},
  author = {Arcuri, A.},
  date = {2017-07},
  pages = {9--20},
  doi = {10.1109/QRS.2017.11},
  abstract = {Nowadays, web services play a major role in the development of enterprise applications. Many such applications are now developed using a service-oriented architecture (SOA), where microservices is one of its most popular kind. A RESTful web service will provide data via an API over the network using HTTP, possibly interacting with databases and other web services. Testing a RESTful API poses challenges, as inputs/outputs are sequences of HTTP requests/responses to a remote server. Many approaches in the literature do black-box testing, as the tested API is a remote service whose code is not available. In this paper, we consider testing from the point of view of the developers, which do have full access to the code that they are writing. Therefore, we propose a fully automated white-box testing approach, where test cases are automatically generated using an evolutionary algorithm. Tests are rewarded based on code coverage and fault finding metrics. We implemented our technique in a tool called EVOMASTER, which is open-source. Experiments on two open-source, yet non-trivial RESTful services and an industrial one, do show that our novel technique did automatically find 38 real bugs in those applications. However, obtained code coverage is lower than the one achieved by the manually written test suites already existing in those services. Research directions on how to further improve such approach are therefore discussed.},
  keywords = {notion,REST,SBSE,SBST,Service-Oriented Architecture (SOA),Web Service}
}

@misc{arjovskyInvariantRiskMinimization2019,
  title = {Invariant {{Risk Minimization}}},
  author = {Arjovsky, Martin and Bottou, Léon and Gulrajani, Ishaan and Lopez-Paz, David},
  date = {2019},
  keywords = {notion}
}

@inproceedings{armengol-estapeExeBenchMLScaleDataset2022,
  title = {{{ExeBench}}: {{An ML-Scale Dataset}} of {{Executable C Functions}}},
  booktitle = {Proceedings of the 6th {{ACM SIGPLAN International Symposium}} on {{Machine Programming}}},
  author = {Armengol-Estapé, Jordi and Woodruff, Jackson and Brauckmann, Alexander and Magalhães, José Wesley de Souza and O'Boyle, Michael F. P.},
  date = {2022},
  series = {{{MAPS}} 2022},
  pages = {50--59},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3520312.3534867},
  url = {https://doi.org/10.1145/3520312.3534867},
  abstract = {Machine-learning promises to transform compilation and software engineering, yet is frequently limited by the scope of available datasets. In particular, there is a lack of runnable, real-world datasets required for a range of tasks ranging from neural program synthesis to machine learning-guided program optimization. We introduce a new dataset, ExeBench, which attempts to address this. It tackles two key issues with real-world code: references to external types and functions and scalable generation of IO examples. ExeBench is the first publicly available dataset that pairs real-world C code taken from GitHub with IO examples that allow these programs to be run. We develop a toolchain that scrapes GitHub, analyzes the code, and generates runnable snippets of code. We analyze our benchmark suite using several metrics, and show it is representative of real-world code. ExeBench contains 4.5M compilable and 700k executable C functions. This scale of executable, real functions will enable the next generation of machine learning-based programming tasks.},
  isbn = {978-1-4503-9273-0},
  venue = {San Diego, CA, USA},
  keywords = {C,Code Dataset,Compilers,Machine Learning for Code,Mining Software Repositories,notion,Program Synthesis}
}

@inproceedings{arpDosDontsMachine2020,
  title = {Dos and {{Don}}'ts of {{Machine Learning}} in {{Computer Security}}},
  booktitle = {{{USENIX Security Symposium}}},
  author = {Arp, Dan and Quiring, Erwin and Pendlebury, Feargus and Warnecke, Alexander and Pierazzi, Fabio and Wressnegger, Christian and Cavallaro, Lorenzo and Rieck, Konrad},
  date = {2020},
  keywords = {notion}
}

@incollection{arunFindingNaturalNumber2010,
  title = {On {{Finding}} the {{Natural Number}} of {{Topics}} with {{Latent Dirichlet Allocation}}: {{Some Observations}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}: 14th {{Pacific-Asia Conference}}, {{PAKDD}} 2010, {{Hyderabad}}, {{India}}, {{June}} 21-24, 2010. {{Proceedings}}. {{Part I}}},
  author = {Arun, R. and Suresh, V. and Veni Madhavan, C. E. and Narasimha Murthy, M. N.},
  editor = {Zaki, Mohammed J. and Yu, Jeffrey Xu and Ravindran, B. and Pudi, Vikram},
  date = {2010},
  pages = {391--402},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-13657-3_43},
  url = {http://dx.doi.org/10.1007/978-3-642-13657-3_43},
  isbn = {978-3-642-13657-3},
  keywords = {Latent Dirichlet Allocation (LDA),notion}
}

@article{arztReviserEfficientlyUpdating2014,
  title = {Reviser: Efficiently Updating {{IDE-}}/{{IFDS-based}} Data-Flow Analyses in Response to Incremental Program Changes},
  author = {Arzt, Steven and Bodden, Eric},
  date = {2014},
  journaltitle = {Proceedings of the 36th International Conference on Software Engineering},
  url = {https://api.semanticscholar.org/CorpusID:17086979},
  keywords = {notion}
}

@inproceedings{aschermannIjonExploringDeep2020,
  title = {Ijon: {{Exploring}} Deep State Spaces via Fuzzing},
  booktitle = {2020 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  author = {Aschermann, Cornelius and Schumilo, Sergej and Abbasi, Ali and Holz, Thorsten},
  date = {2020},
  pages = {1597--1612},
  publisher = {IEEE},
  keywords = {notion}
}

@inproceedings{aschermannREDQUEENFuzzingInputState2019,
  title = {{{REDQUEEN}}: {{Fuzzing}} with {{Input-to-State Correspondence}}.},
  booktitle = {{{NDSS}}},
  author = {Aschermann, Cornelius and Schumilo, Sergej and Blazytko, Tim and Gawlik, Robert and Holz, Thorsten},
  date = {2019},
  volume = {19},
  pages = {1--15},
  keywords = {notion}
}

@article{assiHowDetrimentalCoincidental2021,
  title = {How Detrimental Is Coincidental Correctness to Coverage‐based Fault Detection and Localization? {{An}} Empirical Study},
  author = {Assi, Rawad Abou and Masri, Wes and Trad, Chadi},
  date = {2021},
  journaltitle = {Software Testing},
  volume = {31},
  keywords = {notion}
}

@inproceedings{aydinAutomatabasedModelCounting2015,
  title = {Automata-Based Model Counting for String Constraints},
  booktitle = {International {{Conference}} on {{Computer Aided Verification}}},
  author = {Aydin, Abdulbaki and Bang, Lucas and Bultan, Tevfik},
  date = {2015},
  pages = {255--272},
  publisher = {Springer},
  keywords = {notion}
}

@inproceedings{aydinParameterizedModelCounting2018,
  title = {Parameterized Model Counting for String and Numeric Constraints},
  booktitle = {Proceedings of the 2018 26th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Aydin, Abdulbaki and Eiers, William and Bang, Lucas and Brennan, Tegan and Gavrilov, Miroslav and Bultan, Tevfik and Yu, Fang},
  date = {2018},
  pages = {400--410},
  keywords = {notion}
}

@article{ayersAsmDBUnderstandingMitigating2019,
  title = {{{AsmDB}}: {{Understanding}} and {{Mitigating Front-End Stalls}} in {{Warehouse-Scale Computers}}},
  author = {Ayers, Grant and Nagendra, Nayana Prasad and August, David I. and Cho, Hyoun Kyu and Kanev, Svilen and Kozyrakis, Christoforos E. and Krishnamurthy, Trivikram and Litz, Heiner and Moseley, Tipp and Ranganathan, Parthasarathy},
  date = {2019},
  journaltitle = {2019 ACM/IEEE 46th Annual International Symposium on Computer Architecture (ISCA)},
  pages = {462--473},
  keywords = {notion}
}

@inproceedings{b.leLearningrankBasedFault2016,
  title = {A {{Learning-to-rank Based Fault Localization Approach Using Likely Invariants}}},
  booktitle = {Proceedings of the 25th {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {B. Le, Tien-Duy and Lo, David and Le Goues, Claire and Grunske, Lars},
  date = {2016},
  series = {{{ISSTA}} 2016},
  pages = {177--188},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2931037.2931049},
  url = {http://doi.acm.org/10.1145/2931037.2931049},
  isbn = {978-1-4503-4390-9},
  venue = {Saarbr\&\#252;cken, Germany},
  keywords = {Automated Debugging,Learning to Rank,notion,Program Invariant,Savant}
}

@inproceedings{baahCausalInferenceStatistical2010,
  title = {Causal Inference for Statistical Fault Localization},
  author = {Baah, George and Podgurski, Andy and Harrold, Mary},
  date = {2010-01},
  pages = {73--84},
  doi = {10.1145/1831708.1831717},
  keywords = {notion}
}

@report{baahMatchingTestCases2011,
  title = {Matching {{Test Cases}} for {{Effective Fault Localization}}},
  author = {Baah, George K. and Podgurski, Andy and Harrold, Mary Jean},
  date = {2011},
  institution = {Georgia Institute of Technology},
  keywords = {notion}
}

@inproceedings{baahMitigatingConfoundingEffects2011,
  title = {Mitigating the {{Confounding Effects}} of {{Program Dependences}} for {{Effective Fault Localization}}},
  booktitle = {Proceedings of the 19th {{ACM SIGSOFT Symposium}} and the 13th {{European Conference}} on {{Foundations}} of {{Software Engineering}}},
  author = {Baah, George K. and Podgurski, Andy and Harrold, Mary Jean},
  date = {2011},
  series = {{{ESEC}}/{{FSE}} '11},
  pages = {146--156},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2025113.2025136},
  url = {https://doi.org/10.1145/2025113.2025136},
  isbn = {978-1-4503-0443-6},
  venue = {Szeged, Hungary},
  keywords = {causal inference,debugging,fault localization,matching,notion,potential outcome model,program analysis}
}

@article{baahProbabilisticProgramDependence2010,
  title = {The {{Probabilistic Program Dependence Graph}} and {{Its Application}} to {{Fault Diagnosis}}},
  author = {Baah, G. K. and Podgurski, A. and Harrold, M. J.},
  date = {2010-07},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {36},
  number = {4},
  pages = {528--545},
  doi = {10.1109/TSE.2009.87},
  keywords = {Application software,Automatic control,Computer Society,fault diagnosis,Fault diagnosis,fault localization technique,graph theory,Graphical models,Information analysis,machine learning,notion,probabilistic analysis,probabilistic graphical models,Probabilistic graphical models,probabilistic program dependence graph,probability,Probability distribution,program analysis.,program diagnostics,reasoning,reasoning about programs,Runtime,Software engineering,Testing,uncertain program behavior,uncertainty handling}
}

@inproceedings{babicFUDGEFuzzDriver2019,
  title = {{{FUDGE}}: {{Fuzz Driver Generation}} at {{Scale}}},
  booktitle = {Proceedings of the 2019 27th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Babić, Domagoj and Bucur, Stefan and Chen, Yaohui and Ivančić, Franjo and King, Tim and Kusano, Markus and Lemieux, Caroline and Szekeres, László and Wang, Wei},
  date = {2019},
  series = {{{ESEC}}/{{FSE}} 2019},
  pages = {975--985},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3338906.3340456},
  url = {https://doi.org/10.1145/3338906.3340456},
  abstract = {At Google we have found tens of thousands of security and robustness bugs by fuzzing C and C++ libraries. To fuzz a library, a fuzzer requires a fuzz driver—which exercises some library code—to which it can pass inputs. Unfortunately, writing fuzz drivers remains a primarily manual exercise, a major hindrance to the widespread adoption of fuzzing. In this paper, we address this major hindrance by introducing the Fudge system for automated fuzz driver generation. Fudge automatically generates fuzz driver candidates for libraries based on existing client code. We have used Fudge to generate thousands of new drivers for a wide variety of libraries. Each generated driver includes a synthesized C/C++ program and a corresponding build script, and is automatically analyzed for quality. Developers have integrated over 200 of these generated drivers into continuous fuzzing services and have committed to address reported security bugs. Further, several of these fuzz drivers have been upstreamed to open source projects and integrated into the OSS-Fuzz fuzzing infrastructure. Running these fuzz drivers has resulted in over 150 bug fixes, including the elimination of numerous exploitable security vulnerabilities.},
  isbn = {978-1-4503-5572-8},
  venue = {Tallinn, Estonia},
  keywords = {automated test generation,code synthesis,fuzz testing,fuzzing,notion,program slicing,software security,testing}
}

@inproceedings{backesAutomaticDiscoveryQuantification2009,
  title = {Automatic Discovery and Quantification of Information Leaks},
  booktitle = {2009 30th {{IEEE Symposium}} on {{Security}} and {{Privacy}}},
  author = {Backes, Michael and Köpf, Boris and Rybalchenko, Andrey},
  date = {2009},
  pages = {141--153},
  publisher = {IEEE},
  keywords = {notion}
}

@article{backOverviewEvolutionaryAlgorithms1993,
  title = {An {{Overview}} of {{Evolutionary Algorithms}} for {{Parameter Optimization}}},
  author = {Bäck, Thomas and Schwefel, Hans-Paul},
  date = {1993},
  journaltitle = {Evolutionary Computation},
  volume = {1},
  number = {1},
  pages = {1--23},
  doi = {10.1162/evco.1993.1.1.1},
  keywords = {notion}
}

@article{baeConfidenceIntervalBayesian2017,
  title = {Confidence {{Interval}} of {{Bayesian Network}} and {{Global Sensitivity Analysis}}},
  author = {Bae, Sangjune and Kim, Nam-Ho and Park, Chanyoung and Kim, Zaeill},
  date = {2017},
  journaltitle = {AIAA Journal},
  volume = {55},
  pages = {3916--3924},
  keywords = {notion}
}

@misc{bagbabaEfficientFaultInjection2020,
  title = {Efficient {{Fault Injection}} Based on {{Dynamic HDL Slicing Technique}}},
  author = {Bagbaba, Ahmet Cagri and Jenihhin, Maksim and Raik, Jaan and Sauer, Christian},
  date = {2020},
  keywords = {notion}
}

@article{baiCausalInferenceBased2017,
  title = {Causal Inference Based Fault Localization for Numerical Software with {{NUMFL}}},
  author = {Bai, Zhuofu and Shu, Gang and Podgurski, Andy},
  date = {2017},
  journaltitle = {Software Testing, Verification and Reliability},
  volume = {27},
  number = {6},
  pages = {e1613},
  doi = {10.1002/stvr.1613},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1613},
  abstract = {Summary This paper presents NUMFL, a value-based causal inference technique for localizing faults in numerical software. NUMFL combines causal and statistical analyses to characterize the causal effects of individual numerical expressions on output errors. Given value-profiles for an expression's variables, NUMFL uses generalized propensity scores or covariate balancing propensity scores to reduce confounding bias caused by evaluation of other, faulty expressions. It estimates the average failure-causing effect of an expression using statistical regression models fit within generalized propensity score or covariate balancing propensity score subclasses (strata). This paper also reports on an empirical evaluation of NUMFL involving components from four Java numerical libraries, in which it was compared with five alternative statistical fault localization metrics. The results indicate that NUMFL is more effective than competitive statistical fault localization techniques. The results also indicate NUMFL that works surprisingly well with data from failing runs alone. Copyright \textbackslash copyright 2016 John Wiley \& Sons, Ltd.},
  keywords = {causal inference,confounding bias,covariate-balancing propensity score,failure-causing effect estimation,generalized propensity score,notion,value-based statistical fault localization}
}

@inproceedings{baierComputingConditionalProbabilities2014,
  title = {Computing {{Conditional Probabilities}} in {{Markovian Models Efficiently}}},
  booktitle = {{{TACAS}}},
  author = {Baier, Christel and Klein, Joachim and Klüppelholz, Sascha and Märcker, Steffen},
  date = {2014},
  keywords = {notion}
}

@inproceedings{baiImportanceBeingPositive2015,
  title = {The {{Importance}} of {{Being Positive}} in {{Causal Statistical Fault Localization}}: {{Important Properties}} of {{Baah}} et al.'s {{CSFL Regression Model}}},
  booktitle = {2015 {{IEEE}}/{{ACM}} 1st {{International Workshop}} on {{Complex Faults}} and {{Failures}} in {{Large Software Systems}} ({{COUFLESS}})},
  author = {Bai, Z. and Sun, S. and Podgurski, A.},
  date = {2015-05},
  pages = {7--13},
  issn = {null},
  doi = {10.1109/COUFLESS.2015.9},
  keywords = {causal inference,causal statistical fault localization,conditional probability,CSFL regression model,Flow graphs,inference mechanisms,Java,Mathematical model,Measurement,notion,positivity,positivity violation,Probabilistic logic,Probability,regression analysis,software fault tolerance,statistical debugging,statistical fault localization,XML}
}

@inproceedings{baiNUMFLLocalizingFaults2015,
  title = {{{NUMFL}}: {{Localizing Faults}} in {{Numerical Software Using}} a {{Value-Based Causal Model}}},
  booktitle = {2015 {{IEEE}} 8th {{International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}} ({{ICST}})},
  author = {Bai, Z. and Shu, G. and Podgurski, A.},
  date = {2015},
  pages = {1--10},
  keywords = {notion}
}

@inproceedings{bandyopadhyayMitigatingEffectCoincidental2012,
  title = {Mitigating the Effect of Coincidental Correctness in Spectrum Based Fault Localization},
  booktitle = {2012 {{IEEE Fifth International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}}},
  author = {Bandyopadhyay, Aritra},
  date = {2012},
  pages = {479--482},
  publisher = {IEEE},
  keywords = {notion}
}

@inproceedings{bandyopadhyayProximityBasedWeighting2011,
  title = {Proximity Based Weighting of Test Cases to Improve Spectrum Based Fault Localization},
  booktitle = {2011 26th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}} 2011)},
  author = {Bandyopadhyay, Aritra and Ghosh, Sudipto},
  date = {2011},
  pages = {420--423},
  doi = {10.1109/ASE.2011.6100088},
  keywords = {notion}
}

@inproceedings{bandyopadhyayTesterFeedbackDriven2012,
  title = {Tester Feedback Driven Fault Localization},
  booktitle = {2012 {{IEEE Fifth International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}}},
  author = {Bandyopadhyay, Aritra and Ghosh, Sudipto},
  date = {2012},
  pages = {41--50},
  publisher = {IEEE},
  keywords = {notion}
}

@inproceedings{banerjeeCompositionalTaintAnalysis2023,
  title = {Compositional {{Taint Analysis}} for {{Enforcing Security Policies}} at {{Scale}}},
  booktitle = {Proceedings of the 31st {{ACM Joint European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Banerjee, Subarno and Cui, Siwei and Emmi, Michael and Filieri, Antonio and Hadarean, Liana and Li, Peixuan and Luo, Linghui and Piskachev, Goran and Rosner, Nicolás and Sengupta, Aritra and Tripp, Omer and Wang, Jingbo},
  date = {2023},
  series = {{{ESEC}}/{{FSE}} 2023},
  pages = {1985--1996},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3611643.3613889},
  url = {https://doi.org/10.1145/3611643.3613889},
  abstract = {Automated static dataflow analysis is an effective technique for detecting security critical issues like sensitive data leak, and vulnerability to injection attacks. Ensuring high precision and recall requires an analysis that is context, field and object sensitive. However, it is challenging to attain high precision and recall and scale to large industrial code bases. Compositional style analyses in which individual software components are analyzed separately, independent from their usage contexts, compute reusable summaries of components. This is an essential feature when deploying such analyses in CI/CD at code-review time or when scanning deployed container images. In both these settings the majority of software components stay the same between subsequent scans. However, it is not obvious how to extend such analyses to check the kind of contextual taint specifications that arise in practice, while maintaining compositionality. In this work we present contextual dataflow modeling, an extension to the compositional analysis to check complex taint specifications and significantly increasing recall and precision. Furthermore, we show how such high-fidelity analysis can scale in production using three key optimizations: (i) discarding intermediate results for previously-analyzed components, an optimization exploiting the compositional nature of our analysis; (ii) a scope-reduction analysis to reduce the scope of the taint analysis w.r.t. the taint specifications being checked, and (iii) caching of analysis models. We show a 9.85\% reduction in false positive rate on a comprehensive test suite comprising the OWASP open-source benchmarks as well as internal real-world code samples. We measure the performance and scalability impact of each individual optimization using open source JVM packages from the Maven central repository and internal AWS service codebases. This combination of high precision, recall, performance, and scalability has allowed us to enforce security policies at scale both internally within Amazon as well as for external customers.},
  isbn = {9798400703270},
  venue = {{$<$}conf-loc{$>$}, {$<$}city{$>$}San Francisco{$<$}/city{$>$}, {$<$}state{$>$}CA{$<$}/state{$>$}, {$<$}country{$>$}USA{$<$}/country{$>$}, {$<$}/conf-loc{$>$}},
  keywords = {notion,software security,static analysis in the cloud,taint analysis}
}

@inproceedings{bangStringAnalysisSide2016,
  title = {String {{Analysis}} for {{Side Channels}} with {{Segmented Oracles}}},
  booktitle = {Proceedings of the 2016 24th {{ACM SIGSOFT International Symposium}} on {{Foundations}} of {{Software Engineering}}},
  author = {Bang, Lucas and Aydin, Abdulbaki and Phan, Quoc-Sang and Păsăreanu, Corina S. and Bultan, Tevfik},
  date = {2016},
  series = {{{FSE}} 2016},
  pages = {193--204},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2950290.2950362},
  url = {https://doi.org/10.1145/2950290.2950362},
  abstract = {We present an automated approach for detecting and quantifying side channels in Java programs, which uses symbolic execution, string analysis and model counting to compute information leakage for a single run of a program. We further extend this approach to compute information leakage for multiple runs for a type of side channels called segmented oracles, where the attacker is able to explore each segment of a secret (for example each character of a password) independently. We present an efficient technique for segmented oracles that computes information leakage for multiple runs using only the path constraints generated from a single run symbolic execution. Our implementation uses the symbolic execution tool Symbolic PathFinder (SPF), SMT solver Z3, and two model counting constraint solvers LattE and ABC. Although LattE has been used before for analyzing numeric constraints, in this paper, we present an approach for using LattE for analyzing string constraints. We also extend the string constraint solver ABC for analysis of both numeric and string constraints, and we integrate ABC in SPF, enabling quantitative symbolic string analysis.},
  isbn = {978-1-4503-4218-6},
  venue = {Seattle, WA, USA},
  keywords = {notion,Side-channel analysis,String constraints,Symbolic execution}
}

@article{baoAbacusPreciseSideChannel2021,
  title = {Abacus: {{Precise Side-Channel Analysis}}},
  author = {Bao, Qinkun and Wang, Zihao and Li, Xiaoting and Larus, James R. and Wu, Dinghao},
  date = {2021},
  journaltitle = {2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},
  pages = {797--809},
  url = {https://api.semanticscholar.org/CorpusID:232221353},
  keywords = {notion}
}

@inproceedings{baralEmpiricalAnalysisBlind2020,
  title = {An {{Empirical Analysis}} of {{Blind Tests}}},
  booktitle = {2020 {{IEEE}} 13th {{International Conference}} on {{Software Testing}}, {{Validation}} and {{Verification}} ({{ICST}})},
  author = {Baral, K. and Offutt, J.},
  date = {2020},
  pages = {254--262},
  doi = {10.1109/ICST46399.2020.00034},
  keywords = {notion}
}

@inproceedings{barbotCouplingImportanceSampling2012,
  title = {Coupling and Importance Sampling for Statistical Model Checking},
  booktitle = {Tools and {{Algorithms}} for the {{Construction}} and {{Analysis}} of {{Systems}}: 18th {{International Conference}}, {{TACAS}} 2012, {{Held}} as {{Part}} of the {{European Joint Conferences}} on {{Theory}} and {{Practice}} of {{Software}}, {{ETAPS}} 2012, {{Tallinn}}, {{Estonia}}, {{March}} 24–{{April}} 1, 2012. {{Proceedings}} 18},
  author = {Barbot, Benoît and Haddad, Serge and Picaronny, Claudine},
  date = {2012},
  pages = {331--346},
  publisher = {Springer},
  keywords = {notion}
}

@inproceedings{bareinboimRecoveringSelectionBias2014,
  title = {Recovering from {{Selection Bias}} in {{Causal}} and {{Statistical Inference}}},
  booktitle = {Proceedings of the {{Twenty-Eighth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Bareinboim, Elias and Tian, Jin and Pearl, Judea},
  date = {2014},
  series = {{{AAAI}}'14},
  pages = {2410--2416},
  publisher = {AAAI Press},
  location = {Québec City, Québec, Canada},
  abstract = {Selection bias is caused by preferential exclusion of units from the samples and represents a major obstacle to valid causal and statistical inferences; it cannot be removed by randomized experiments and can rarely be detected in either experimental or observational studies. In this paper, we provide complete graphical and algorithmic conditions for recovering conditional probabilities from selection biased data. We also provide graphical conditions for recoverability when unbiased data is available over a subset of the variables. Finally, we provide a graphical condition that generalizes the backdoor criterion and serves to recover causal effects when the data is collected under preferential selection.},
  keywords = {notion}
}

@inproceedings{barnettHelpingDevelopersHelp2015,
  title = {Helping {{Developers Help Themselves}}: {{Automatic Decomposition}} of {{Code Review Changesets}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Software Engineering}} - {{Volume}} 1},
  author = {Barnett, Mike and Bird, Christian and Brunet, João and Lahiri, Shuvendu K.},
  date = {2015},
  series = {{{ICSE}} '15},
  pages = {134--144},
  publisher = {IEEE Press},
  location = {Piscataway, NJ, USA},
  url = {http://dl.acm.org/citation.cfm?id=2818754.2818773},
  isbn = {978-1-4799-1934-5},
  venue = {Florence, Italy},
  keywords = {notion}
}

@article{barpandaDynamicSlicingDistributed2011,
  title = {Dynamic Slicing of Distributed Object-Oriented Programs},
  author = {Barpanda, S. S. and Mohapatra, D. P.},
  date = {2011-10},
  journaltitle = {IET Software},
  volume = {5},
  number = {5},
  pages = {425--433},
  issn = {1751-8814},
  doi = {10.1049/iet-sen.2010.0141},
  keywords = {decomposition technique,dependence analysis,distributed object oriented program,distributed processing,dynamic slice computing,graph colouring,notion,object-oriented programming,program slicing}
}

@article{barracloughTrajectorybasedStrictSemantics2010,
  title = {A Trajectory-Based Strict Semantics for Program Slicing},
  author = {Barraclough, Richard W. and Binkley, David and Danicic, Sebastian and Harman, Mark and Hierons, Robert M. and Kiss, Ákos and Laurence, Mike and Ouarbya, Lahcen},
  date = {2010},
  journaltitle = {Theoretical Computer Science},
  volume = {411},
  number = {11},
  pages = {1372--1386},
  issn = {0304-3975},
  doi = {http://dx.doi.org/10.1016/j.tcs.2009.10.025},
  url = {http://www.sciencedirect.com/science/article/pii/S0304397509007944},
  abstract = {We define a program semantics that is preserved by dependence-based slicing algorithms. It is a natural extension, to non-terminating programs, of the semantics introduced by Weiser (which only considered terminating ones) and, as such, is an accurate characterisation of the semantic relationship between a program and the slice produced by these algorithms. Unlike other approaches, apart from Weiser's original one, it is based on strict standard semantics which models the `normal' execution of programs on a von Neumann machine and, thus, has the advantage of being intuitive. This is essential since one of the main applications of slicing is program comprehension. Although our semantics handles non-termination, it is defined wholly in terms of finite trajectories, without having to resort to complex, counter-intuitive, non-standard models of computation. As well as being simpler, unlike other approaches to this problem, our semantics is substitutive. Substitutivity is an important property because it greatly enhances the ability to reason about correctness of meaning-preserving program transformations such as slicing.},
  keywords = {notion,Program Slicing}
}

@inproceedings{barrAutomatedSoftwareTransplantation2015,
  title = {Automated {{Software Transplantation}}},
  booktitle = {Proceedings of the 2015 {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Barr, Earl T. and Harman, Mark and Jia, Yue and Marginean, Alexandru and Petke, Justyna},
  date = {2015},
  series = {{{ISSTA}} 2015},
  pages = {257--269},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2771783.2771796},
  url = {http://doi.acm.org/10.1145/2771783.2771796},
  isbn = {978-1-4503-3620-8},
  venue = {Baltimore, MD, USA},
  keywords = {Automated software transplantation,autotransplantation,genetic improvement,notion}
}

@article{barrOracleProblemSoftware2015,
  title = {The {{Oracle Problem}} in {{Software Testing}}: {{A Survey}}},
  author = {Barr, Earl and Harman, Mark and McMinn, Phil and Shahbaz, Muzammil and Yoo, Shin},
  date = {2015-05},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {41},
  number = {5},
  pages = {507--525},
  issn = {0098-5589},
  keywords = {notion,Software Testing,Survey}
}

@inproceedings{barrPlasticSurgeryHypothesis2014,
  title = {The {{Plastic Surgery Hypothesis}}},
  booktitle = {Proceedings of the {{22Nd ACM SIGSOFT International Symposium}} on {{Foundations}} of {{Software Engineering}}},
  author = {Barr, Earl T. and Brun, Yuriy and Devanbu, Premkumar and Harman, Mark and Sarro, Federica},
  date = {2014},
  series = {{{FSE}} 2014},
  pages = {306--317},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2635868.2635898},
  url = {http://doi.acm.org/10.1145/2635868.2635898},
  isbn = {978-1-4503-3056-5},
  venue = {Hong Kong, China},
  keywords = {Automated Program Repair,notion,Plastic Surgery Hypothesis}
}

@article{barvinokPolynomialTimeAlgorithm1994,
  title = {A Polynomial Time Algorithm for Counting Integral Points in Polyhedra When the Dimension Is Fixed},
  author = {Barvinok, Alexander I},
  date = {1994},
  journaltitle = {Mathematics of Operations Research},
  volume = {19},
  number = {4},
  pages = {769--779},
  publisher = {INFORMS},
  keywords = {notion}
}

@unpublished{basiosDarwinianDataStructure2017,
  title = {Darwinian Data Structure Selection},
  author = {Basios, Michail and Li, Lingbo and Wu, Fan and Kanthan, Leslie and Lawrence, Donald and Barr, Earl},
  date = {2017},
  eprint = {1706.03232},
  eprinttype = {arXiv},
  keywords = {notion}
}

@article{bastaniSynthesizingProgramInput2017,
  title = {Synthesizing Program Input Grammars},
  author = {Bastani, Osbert and Sharma, Rahul and Aiken, Alex and Liang, Percy},
  date = {2017},
  journaltitle = {ACM SIGPLAN Notices},
  volume = {52},
  number = {6},
  pages = {95--110},
  publisher = {ACM New York, NY, USA},
  keywords = {notion}
}

@incollection{beckertIntegrationStaticDynamic2020,
  title = {Integration of {{Static}} and {{Dynamic Analysis Techniques}} for {{Checking Noninterference}}},
  booktitle = {Deductive {{Software Verification}}: {{Future Perspectives}}: {{Reflections}} on the {{Occasion}} of 20 {{Years}} of {{KeY}}},
  author = {Beckert, Bernhard and Herda, Mihai and Kirsten, Michael and Tyszberowicz, Shmuel},
  date = {2020},
  pages = {287--312},
  publisher = {Springer},
  keywords = {notion}
}

@inproceedings{bellDeFlakerAutomaticallyDetecting2018,
  title = {{{DeFlaker}}: {{Automatically}} Detecting Flaky Tests},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Software Engineering}}},
  author = {Bell, Jonathan and Legunsen, Owolabi and Hilton, Michael and Eloussi, Lamyaa and Yung, Tifany and Marinov, Darko},
  date = {2018},
  keywords = {notion}
}

@inproceedings{benedicteConfidenceAssessmentWCET2016,
  title = {A Confidence Assessment of {{WCET}} Estimates for Software Time Randomized Caches},
  booktitle = {2016 {{IEEE}} 14th {{International Conference}} on {{Industrial Informatics}} ({{INDIN}})},
  author = {Benedicte, Pedro and Kosmidis, Leonidas and Quiñones, Eduardo and Abella, Jaume and Cazorla, Francisco J.},
  date = {2016},
  pages = {90--97},
  doi = {10.1109/INDIN.2016.7819140},
  keywords = {notion}
}

@article{bentonSelfBoostedAutomatedProgram2021,
  title = {Self-{{Boosted Automated Program Repair}}},
  author = {Benton, Samuel and Zhang, Mengshi and Li, Xia and Zhang, Lingming},
  date = {2021},
  journaltitle = {ArXiv},
  volume = {abs/2104.04611},
  keywords = {notion}
}

@article{besseyFewBillionLines2010,
  title = {A {{Few Billion Lines}} of {{Code Later}}: {{Using Static Analysis}} to {{Find Bugs}} in the {{Real World}}},
  author = {Bessey, Al and Block, Ken and Chelf, Ben and Chou, Andy and Fulton, Bryan and Hallem, Seth and Henri-Gros, Charles and Kamsky, Asya and McPeak, Scott and Engler, Dawson},
  date = {2010-02},
  journaltitle = {Commun. ACM},
  volume = {53},
  number = {2},
  pages = {66--75},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {0001-0782},
  doi = {10.1145/1646353.1646374},
  url = {https://doi.org/10.1145/1646353.1646374},
  keywords = {notion}
}

@inproceedings{beszedesDynamicSlicingMethod2001,
  title = {Dynamic Slicing Method for Maintenance of Large {{C}} Programs},
  booktitle = {Proceedings {{Fifth European Conference}} on {{Software Maintenance}} and {{Reengineering}}},
  author = {Beszedes, A. and Gergely, T. and Szabo, Z. Mihaly and Csirik, J. and Gyimothy, T.},
  date = {2001-03},
  pages = {105--113},
  issn = {null},
  doi = {10.1109/CSMR.2001.914974},
  keywords = {Artificial intelligence,backward dynamic slice computation,C language,Computer applications,Costs,debugging,Debugging,dynamic program slicing method,execution history size,forward global method,function calls,large C program maintenance,large-scale systems,Lattices,memory locations,memory requirements,notion,pointers,program instructions,program slicing,program testing,reverse engineering,Reverse engineering,Safety,software maintenance,Software maintenance,Testing,upper bound,Upper bound}
}

@inproceedings{beszedesGraphLessDynamicDependenceBased2006,
  title = {Graph-{{Less Dynamic Dependence-Based Dynamic Slicing Algorithms}}},
  booktitle = {2006 {{Sixth IEEE International Workshop}} on {{Source Code Analysis}} and {{Manipulation}}},
  author = {Beszedes, A. and Gergely, T. and Gyimothy, T.},
  date = {2006-09},
  pages = {21--30},
  issn = {null},
  doi = {10.1109/SCAM.2006.17},
  keywords = {Buildings,Computer aided instruction,Conferences,Data structures,dynamic slicing algorithms,execution,Heuristic algorithms,History,Java,notion,program dependences.,Program slicing,Software algorithms,Software engineering,Terminology,trace}
}

@inproceedings{bethardWhoShouldCite2010,
  title = {Who {{Should I Cite}}: {{Learning Literature Search Models}} from {{Citation Behavior}}},
  booktitle = {Proceedings of the 19th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Bethard, Steven and Jurafsky, Dan},
  date = {2010},
  series = {{{CIKM}} '10},
  pages = {609--618},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/1871437.1871517},
  url = {http://doi.acm.org/10.1145/1871437.1871517},
  isbn = {978-1-4503-0099-5},
  venue = {Toronto, ON, Canada},
  keywords = {Citation Recommendation,notion,Retrieval Models}
}

@article{bettsImplementingEvaluatingCandidateBased2017,
  title = {Implementing and {{Evaluating Candidate-Based Invariant Generation}}},
  author = {Betts, A. and Chong, N. and Donaldson, A. and Deligiannis, P. and Ketema, J.},
  date = {2017},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {PP},
  number = {99},
  pages = {1--1},
  issn = {0098-5589},
  doi = {10.1109/TSE.2017.2718516},
  abstract = {The discovery of inductive invariants lies at the heart of static program verification. This paper describes our efforts to apply candidate-based invariant generation in GPUVerify, a static checker of programs that run on GPUs. We study a large set of GPU programs that contain loops, drawn from a number of open source suites and vendor SDKs. We describe the methodology we used to incrementally improve the invariant generation capabilities of GPUVerify to handle these benchmarks, through candidate-based invariant generation, whereby potential program invariants are speculated using cheap static analysis and subsequently either refuted or proven. We also describe a set of experiments that we used to examine the effectiveness of our rules for candidate generation. We believe that our methodology for devising and evaluation candidate generation rules may serve as a useful framework for other researchers. To speed up invariant generation, we have investigated four under-approximating program analyses that aim to reject false candidates quickly and a framework whereby these analyses can run in sequence or in parallel. We describe experimental results showing speedups across two experimental platforms.},
  keywords = {GPUs,Invariant Generation,notion,Program Invariant}
}

@article{bevilacquaNeuralAlgorithmicReasoning2023,
  title = {Neural {{Algorithmic Reasoning}} with {{Causal Regularisation}}},
  author = {Bevilacqua, Beatrice and Nikiforou, Kyriacos and Ibarz, Borja and Bica, Ioana and Paganini, Michela and Blundell, Charles and Mitrovic, Jovana and Velivckovi'c, Petar},
  date = {2023},
  journaltitle = {ArXiv},
  volume = {abs/2302.10258},
  keywords = {notion}
}

@inproceedings{bevilacquaSizeInvariantGraphRepresentations2021,
  title = {Size-{{Invariant Graph Representations}} for {{Graph Classification Extrapolations}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Bevilacqua, Beatrice and Zhou, Yangze and Ribeiro, Bruno},
  date = {2021},
  keywords = {notion}
}

@misc{beyerBenchmarkSetProgram2021,
  title = {Towards a {{Benchmark Set}} for {{Program Repair Based}} on {{Partial Fixes}}},
  author = {Beyer, Dirk and Grunske, Lars and Lemberger, Thomas and Tang, Minxing},
  date = {2021},
  keywords = {notion}
}

@article{bezemerEmpiricalStudyUnspecified2017,
  title = {An Empirical Study of Unspecified Dependencies in Make-Based Build Systems},
  author = {Bezemer, Cor-Paul and McIntosh, Shane and Adams, Bram and German, Daniel M. and Hassan, Ahmed E.},
  date = {2017-04-05},
  journaltitle = {Empirical Software Engineering},
  issn = {1573-7616},
  doi = {10.1007/s10664-017-9510-8},
  url = {https://doi.org/10.1007/s10664-017-9510-8},
  abstract = {Software developers rely on a build system to compile their source code changes and produce deliverables for testing and deployment. Since the full build of large software systems can take hours, the incremental build is a cornerstone of modern build systems. Incremental builds should only recompile deliverables whose dependencies have been changed by a developer. However, in many organizations, such dependencies still are identified by build rules that are specified and maintained (mostly) manually, typically using technologies like make. Incomplete rules lead to unspecified dependencies that can prevent certain deliverables from being rebuilt, yielding incomplete results, which leave sources and deliverables out-of-sync. In this paper, we present a case study on unspecified dependencies in the make-based build systems of the glib, openldap, linux and qt open source projects. To uncover unspecified dependencies in make-based build systems, we use an approach that combines a conceptual model of the dependencies specified in the build system with a concrete model of the files and processes that are actually exercised during the build. Our approach provides an overview of the dependencies that are used throughout the build system and reveals unspecified dependencies that are not yet expressed in the build system rules. During our analysis, we find that unspecified dependencies are common. We identify 6 common causes in more than 1.2 million unspecified dependencies.},
  keywords = {Build System,notion,Unspecified Dependencies}
}

@inproceedings{bianchiReproducingConcurrencyFailures2017,
  title = {Reproducing {{Concurrency Failures}} from {{Crash Stacks}}},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Bianchi, Francesco A. and Pezzè, Mauro and Terragni, Valerio},
  date = {2017},
  series = {{{ESEC}}/{{FSE}} 2017},
  pages = {705--716},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/3106237.3106292},
  url = {http://doi.acm.org/10.1145/3106237.3106292},
  isbn = {978-1-4503-5105-8},
  venue = {Paderborn, Germany},
  keywords = {Concurrent Program,Debugging,notion,Software Crashes,Test Generation}
}

@article{bianchiSurveyRecentTrends2018,
  title = {A {{Survey}} of {{Recent Trends}} in {{Testing Concurrent Software Systems}}},
  author = {Bianchi, Francesco Adalberto and Margara, Alessandro and Pezzè, Mauro},
  date = {2018},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {44},
  number = {8},
  pages = {747--783},
  doi = {10.1109/TSE.2017.2707089},
  keywords = {notion}
}

@inproceedings{biggerstaffConceptAssignmentProblem1993,
  title = {The Concept Assignment Problem in Program Understanding},
  booktitle = {Proceedings of 1993 15th {{International Conference}} on {{Software Engineering}}},
  author = {Biggerstaff, T.J. and Mitbander, B.G. and Webster, D.},
  date = {1993},
  pages = {482--498},
  doi = {10.1109/ICSE.1993.346017},
  keywords = {notion}
}

@article{binkleyComparisonTreeLineoriented2019,
  title = {A Comparison of Tree- and Line-Oriented Observational Slicing},
  author = {Binkley, David and Gold, Nicolas and Islam, Syed and Krinke, Jens and Yoo, Shin},
  date = {2019-10-01},
  journaltitle = {Empirical Software Engineering},
  volume = {24},
  number = {5},
  pages = {3077--3113},
  doi = {10.1007/s10664-018-9675-9},
  url = {https://doi.org/10.1007/s10664-018-9675-9},
  abstract = {Observation-based slicing and its generalization observational slicing are recently-introduced, language-independent dynamic slicing techniques. They both construct slices based on the dependencies observed during program execution, rather than static or dynamic dependence analysis. The original implementation of the observation-based slicing algorithm used lines of source code as its program representation. A recent variation, developed to slice modelling languages (such as Simulink), used an XML representation of an executable model. We ported the XML slicer to source code by constructing a tree representation of traditional source code through the use of srcML. This work compares the tree- and line-based slicers using four experiments involving twenty different programs, ranging from classic benchmarks to million-line production systems. The resulting slices are essentially the same size for the majority of the programs and are often identical. However, structural constraints imposed by the tree representation sometimes force the slicer to retain enclosing control structures. It can also “bog down”trying to delete single-token subtrees. This occasionally makes the tree-based slices larger and the tree-based slicer slower than a parallelised version of the line-based slicer. In addition, a Java versus C comparison finds that the two languages lead to similar slices, but Java code takes noticeably longer to slice. The initial experiments suggest two improvements to the tree-based slicer: the addition of a size threshold, for ignoring small subtrees, and subtree replacement. The former enables the slicer to run 3.4 times faster while producing slices that are only about 9\% larger. At the same time the subtree replacement reduces size by about 8–12\% and allows the tree-based slicer to produce more natural slices.},
  isbn = {1573-7616},
  keywords = {notion}
}

@inproceedings{binkleyORBSLanguageindependentProgram2014,
  title = {{{ORBS}}: {{Language-independent Program Slicing}}},
  booktitle = {Proceedings of the {{22Nd ACM SIGSOFT International Symposium}} on {{Foundations}} of {{Software Engineering}}},
  author = {Binkley, David and Gold, Nicolas and Harman, Mark and Islam, Syed and Krinke, Jens and Yoo, Shin},
  date = {2014},
  series = {{{FSE}} 2014},
  pages = {109--120},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2635868.2635893},
  url = {http://doi.acm.org/10.1145/2635868.2635893},
  isbn = {978-1-4503-3056-5},
  venue = {Hong Kong, China},
  keywords = {notion,ORBS}
}

@inproceedings{binkleyORBSLimitsStatic2015,
  title = {{{ORBS}} and the Limits of Static Slicing},
  booktitle = {2015 {{IEEE}} 15th {{International Working Conference}} on {{Source Code Analysis}} and {{Manipulation}} ({{SCAM}})},
  author = {Binkley, D. and Gold, N. and Harman, M. and Islam, S. and Krinke, J. and Yoo, S.},
  date = {2015-09},
  pages = {1--10},
  doi = {10.1109/SCAM.2015.7335396},
  abstract = {Observation-based slicing is a recently-introduced, language-independent slicing technique based on the dependencies observable from program behaviour. Due to the well-known limits of dynamic analysis, we may only compute an under-approximation of the true observation-based slice. However, because the observation-based slice captures all possible dependence that can be observed, even such approximations can yield insight into the limitations of static slicing. For example, a static slice, S, that is strictly smaller than the corresponding observation based slice is potentially unsafe. We present the results of three sets of experiments on 12 different programs, including benchmarks and larger programs, which investigate the relationship between static and observation-based slicing. We show that, in extreme cases, observation-based slices can find the true minimal static slice, where static techniques cannot. For more typical cases, our results illustrate the potential for observation-based slicing to highlight limitations in static slicers. Finally, we report on the sensitivity of observation-based slicing to test quality.},
  keywords = {notion,ORBS,Program Slicing}
}

@article{binkleyProgramSlicing1996,
  title = {Program {{Slicing}}},
  author = {Binkley, David W. and Gallagher, Keith Brian},
  date = {1996},
  journaltitle = {Advances in Computers},
  volume = {43},
  pages = {1--50},
  issn = {0065-2458},
  doi = {http://dx.doi.org/10.1016/S0065-2458(08)60641-5},
  url = {http://www.sciencedirect.com/science/article/pii/S0065245808606415},
  abstract = {Program slicing is a technique for reducing the amount of information that needs to be absorbed by a programmer. Given a point of “interest” in a program, described by a variable and a statement, a program slice gives all the statements that contributed to the value of the variable at the point, and elides unnecessary statements. This chapter surveys techniques for computing program slices and the applications of program slicing to development, testing, maintenance, and metrics.},
  keywords = {notion,Program Slicing,Survey}
}

@article{binkleySemanticsGuidedRegression1997,
  title = {Semantics Guided Regression Test Cost Reduction},
  author = {Binkley, D.},
  date = {1997-08},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {23},
  number = {8},
  pages = {498--516},
  issn = {2326-3881},
  doi = {10.1109/32.624306},
  keywords = {Automatic testing,calling context slice,certified program,common execution patterns,Computer Society,Costs,interprocedural slice,modified program retesting,notion,old test cases,procedure calls,program components,program differences,program slices,program statement executions,program testing,Prototypes,semantic change,semantics-guided regression test cost reduction,software cost estimation,Software debugging,Software engineering,software maintenance,Software maintenance,Software prototyping,Software testing,Software tools,subroutines}
}

@inproceedings{binkleySurveyEmpiricalResults2003,
  title = {A {{Survey}} of {{Empirical Results}} on {{Program Slicing}}},
  booktitle = {{{ADVANCES IN COMPUTERS}}, 62:105–178},
  author = {Binkley, David and Harman, Mark},
  date = {2003},
  pages = {105--178},
  keywords = {notion}
}

@inproceedings{binkleyTreeOrientedVsLineOriented2017,
  title = {Tree-{{Oriented}} vs. {{Line-Oriented Observation-Based Slicing}}},
  booktitle = {2017 {{IEEE}} 17th {{International Working Conference}} on {{Source Code Analysis}} and {{Manipulation}} ({{SCAM}})},
  author = {Binkley, D. and Gold, N. and Islam, S. and Krinke, J. and Yoo, S.},
  date = {2017-09},
  pages = {21--30},
  issn = {2470-6892},
  doi = {10.1109/SCAM.2017.11},
  keywords = {C++ code,C++ language,C++ languages,C++ syntax,code structure,Heuristic algorithms,language-independent slicing technique,line-of-text level,line-oriented observation,notion,Observational Slicing,ORBS,program slicing,Slicing,slicing observation,Software algorithms,Software packages,srcML,traditional source code,Trajectory,tree data structures,tree slicer,tree structure,tree-based XML representation,XML,XML representation}
}

@inproceedings{binkleyUncoveringDependenceClusters2015,
  title = {Uncovering Dependence Clusters and Linchpin Functions},
  booktitle = {2015 {{IEEE International Conference}} on {{Software Maintenance}} and {{Evolution}} ({{ICSME}})},
  author = {Binkley, D. and Beszédes, Á and Islam, S. and Jász, J. and Vancsics, B.},
  date = {2015-09},
  pages = {141--150},
  issn = {null},
  doi = {10.1109/ICSM.2015.7332460},
  keywords = {coarser function-level,Complexity theory,feature extraction,fine-grained statement-level relation,Flow graphs,function-level control-flow reachability,linchpin function,Maintenance engineering,Measurement,notion,program entity,program testing,software maintenance,Software maintenance,software refactoring,software testing,source code (software),source code entity,system dependence graph,Testing}
}

@inproceedings{binkleyUnderstandingLDASource2014,
  title = {Understanding {{LDA}} in {{Source Code Analysis}}},
  booktitle = {Proceedings of the {{22Nd International Conference}} on {{Program Comprehension}}},
  author = {Binkley, David and Heinz, Daniel and Lawrie, Dawn and Overfelt, Justin},
  date = {2014},
  series = {{{ICPC}} 2014},
  pages = {26--36},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2597008.2597150},
  url = {http://doi.acm.org/10.1145/2597008.2597150},
  isbn = {978-1-4503-2879-1},
  venue = {Hyderabad, India},
  keywords = {Latent Dirichlet Allocation (LDA),notion}
}

@inproceedings{biondiComparativeAnalysisLeakage2015,
  title = {Comparative Analysis of Leakage Tools on Scalable Case Studies},
  booktitle = {International {{SPIN Workshop}} on {{Model Checking}} of {{Software}}},
  author = {Biondi, Fabrizio and Legay, Axel and Quilbeuf, Jean},
  date = {2015},
  pages = {263--281},
  publisher = {Springer},
  keywords = {notion}
}

@article{biondiHybridStatisticalEstimation2019,
  title = {Hybrid Statistical Estimation of Mutual Information and Its Application to Information Flow},
  author = {Biondi, Fabrizio and Kawamoto, Yusuke and Legay, Axel and Traonouez, Louis-Marie},
  date = {2019},
  journaltitle = {Formal Aspects of Computing},
  volume = {31},
  pages = {165--206},
  publisher = {Springer},
  keywords = {notion}
}

@inproceedings{biondiHyLeakHybridAnalysis2017,
  title = {{{HyLeak}}: Hybrid Analysis Tool for Information Leakage},
  booktitle = {Automated {{Technology}} for {{Verification}} and {{Analysis}}: 15th {{International Symposium}}, {{ATVA}} 2017, {{Pune}}, {{India}}, {{October}} 3–6, 2017, {{Proceedings}} 15},
  author = {Biondi, Fabrizio and Kawamoto, Yusuke and Legay, Axel and Traonouez, Louis-Marie},
  date = {2017},
  pages = {156--163},
  publisher = {Springer},
  keywords = {notion}
}

@inproceedings{biondiScalableApproximationQuantitative2018,
  title = {Scalable Approximation of Quantitative Information Flow in Programs},
  booktitle = {Verification, {{Model Checking}}, and {{Abstract Interpretation}}: 19th {{International Conference}}, {{VMCAI}} 2018, {{Los Angeles}}, {{CA}}, {{USA}}, {{January}} 7-9, 2018, {{Proceedings}} 19},
  author = {Biondi, Fabrizio and Enescu, Michael A and Heuser, Annelie and Legay, Axel and Meel, Kuldeep S and Quilbeuf, Jean},
  date = {2018},
  pages = {71--93},
  publisher = {Springer},
  keywords = {notion}
}

@book{biondiUserManualHyLeak2019,
  title = {User {{Manual}} for {{HyLeak}} v.1.0: {{Hybrid Analysis Tool}} for {{Information Leakage}}},
  author = {Biondi, Fabrizio and Kawamoto, Yusuke and Legay, Axel and Traonouez, Louis-Marie},
  date = {2019},
  url = {https://project.inria.fr/hyleak/download/},
  keywords = {notion}
}

@article{blackburnDaCapoBenchmarksJava2006,
  title = {The {{DaCapo Benchmarks}}: {{Java Benchmarking Development}} and {{Analysis}}},
  author = {Blackburn, Stephen M. and Garner, Robin and Hoffmann, Chris and Khang, Asjad M. and McKinley, Kathryn S. and Bentzur, Rotem and Diwan, Amer and Feinberg, Daniel and Frampton, Daniel and Guyer, Samuel Z. and Hirzel, Martin and Hosking, Antony and Jump, Maria and Lee, Han and Moss, J. Eliot B. and Phansalkar, Aashish and Stefanović, Darko and VanDrunen, Thomas and family=Dincklage, given=Daniel, prefix=von, useprefix=true and Wiedermann, Ben},
  date = {2006-10},
  journaltitle = {SIGPLAN Not.},
  volume = {41},
  number = {10},
  pages = {169--190},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {0362-1340},
  doi = {10.1145/1167515.1167488},
  url = {https://doi.org/10.1145/1167515.1167488},
  abstract = {Since benchmarks drive computer science research and industry product development, which ones we use and how we evaluate them are key questions for the community. Despite complex runtime tradeoffs due to dynamic compilation and garbage collection required for Java programs, many evaluations still use methodologies developed for C, C++, and Fortran. SPEC, the dominant purveyor of benchmarks, compounded this problem by institutionalizing these methodologies for their Java benchmark suite. This paper recommends benchmarking selection and evaluation methodologies, and introduces the DaCapo benchmarks, a set of open source, client-side Java benchmarks. We demonstrate that the complex interactions of (1) architecture, (2) compiler, (3) virtual machine, (4) memory management, and (5) application require more extensive evaluation than C, C++, and Fortran which stress (4) much less, and do not require (3). We use and introduce new value, time-series, and statistical metrics for static and dynamic properties such as code complexity, code size, heap composition, and pointer mutations. No benchmark suite is definitive, but these metrics show that DaCapo improves over SPEC Java in a variety of ways, including more complex code, richer object behaviors, and more demanding memory system requirements. This paper takes a step towards improving methodologies for choosing and evaluating benchmarks to foster innovation in system design and implementation for Java and other managed languages.},
  keywords = {benchmark,DaCapo,Java,methodology,notion,SPEC}
}

@inproceedings{blackburnDaCapoBenchmarksJava2006a,
  title = {The {{DaCapo Benchmarks}}: {{Java Benchmarking Development}} and {{Analysis}}},
  booktitle = {Proceedings of the 21st {{Annual ACM SIGPLAN Conference}} on {{Object-Oriented Programming Systems}}, {{Languages}}, and {{Applications}}},
  author = {Blackburn, Stephen M. and Garner, Robin and Hoffmann, Chris and Khang, Asjad M. and McKinley, Kathryn S. and Bentzur, Rotem and Diwan, Amer and Feinberg, Daniel and Frampton, Daniel and Guyer, Samuel Z. and Hirzel, Martin and Hosking, Antony and Jump, Maria and Lee, Han and Moss, J. Eliot B. and Phansalkar, Aashish and Stefanović, Darko and VanDrunen, Thomas and family=Dincklage, given=Daniel, prefix=von, useprefix=true and Wiedermann, Ben},
  date = {2006},
  series = {{{OOPSLA}} '06},
  pages = {169--190},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1167473.1167488},
  url = {https://doi.org/10.1145/1167473.1167488},
  abstract = {Since benchmarks drive computer science research and industry product development, which ones we use and how we evaluate them are key questions for the community. Despite complex runtime tradeoffs due to dynamic compilation and garbage collection required for Java programs, many evaluations still use methodologies developed for C, C++, and Fortran. SPEC, the dominant purveyor of benchmarks, compounded this problem by institutionalizing these methodologies for their Java benchmark suite. This paper recommends benchmarking selection and evaluation methodologies, and introduces the DaCapo benchmarks, a set of open source, client-side Java benchmarks. We demonstrate that the complex interactions of (1) architecture, (2) compiler, (3) virtual machine, (4) memory management, and (5) application require more extensive evaluation than C, C++, and Fortran which stress (4) much less, and do not require (3). We use and introduce new value, time-series, and statistical metrics for static and dynamic properties such as code complexity, code size, heap composition, and pointer mutations. No benchmark suite is definitive, but these metrics show that DaCapo improves over SPEC Java in a variety of ways, including more complex code, richer object behaviors, and more demanding memory system requirements. This paper takes a step towards improving methodologies for choosing and evaluating benchmarks to foster innovation in system design and implementation for Java and other managed languages.},
  isbn = {1-59593-348-4},
  venue = {Portland, Oregon, USA},
  keywords = {benchmark,DaCapo,Java,methodology,notion,SPEC}
}

@inproceedings{blazytkoAURORAStatisticalCrash2020,
  title = {{{AURORA}}: {{Statistical Crash Analysis}} for {{Automated Root Cause Explanation}}},
  booktitle = {29th {{USENIX Security Symposium}} ({{USENIX Security}} 20)},
  author = {Blazytko, Tim and Schlögel, Moritz and Aschermann, Cornelius and Abbasi, Ali and Frank, Joel and Wörner, Simon and Holz, Thorsten},
  date = {2020-08},
  pages = {235--252},
  publisher = {USENIX Association},
  url = {https://www.usenix.org/conference/usenixsecurity20/presentation/blazytko},
  isbn = {978-1-939133-17-5},
  keywords = {notion}
}

@inproceedings{blazytkoGRIMOIRESynthesizingStructure2019,
  title = {\{\vphantom\}{{GRIMOIRE}}\vphantom\{\}: {{Synthesizing}} Structure While Fuzzing},
  booktitle = {28th \{\vphantom\}{{USENIX}}\vphantom\{\} {{Security Symposium}} (\{\vphantom\}{{USENIX}}\vphantom\{\} {{Security}} 19)},
  author = {Blazytko, Tim and Bishop, Matt and Aschermann, Cornelius and Cappos, Justin and Schlögel, Moritz and Korshun, Nadia and Abbasi, Ali and Schweighauser, Marco and Schinzel, Sebastian and Schumilo, Sergej and others},
  date = {2019},
  pages = {1985--2002},
  keywords = {notion}
}

@article{bleiLatentDirichletAllocation2003,
  title = {Latent Dirichlet Allocation},
  author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  date = {2003},
  journaltitle = {Journal of machine Learning research},
  volume = {3},
  pages = {993--1022},
  issue = {Jan},
  keywords = {notion}
}

@article{bleiProbabilisticTopicModels2012,
  title = {Probabilistic {{Topic Models}}},
  author = {Blei, David M.},
  date = {2012-04},
  journaltitle = {Commun. ACM},
  volume = {55},
  number = {4},
  pages = {77--84},
  publisher = {ACM},
  location = {New York, NY, USA},
  issn = {0001-0782},
  doi = {10.1145/2133806.2133826},
  url = {http://doi.acm.org/10.1145/2133806.2133826},
  keywords = {notion,Probabilistic Latent Semantic Indexing (PLSA)}
}

@inproceedings{blumscheinFindingCutsStatic2024,
  title = {Finding {{Cuts}} in {{Static Analysis Graphs}} to {{Debloat Software}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Blumschein, Christoph and Niephaus, Fabio and Stancu, Codruţ and Wimmer, Christian and Lincke, Jens and Hirschfeld, Robert},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {603--614},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3680306},
  url = {https://dl.acm.org/doi/10.1145/3650212.3680306},
  urldate = {2024-09-21},
  abstract = {As software projects grow increasingly more complex, debloating gains traction.   While static analyses yield a coarse over-approximation of reachable code, approaches based on dynamic execution traces risk program correctness.   By allowing the developer to reconsider only a few methods and still achieve a significant reduction in code size, cut-based debloating can minimize the risk.   In this paper, we propose the idea of finding small cuts in the rule graphs produced by static analysis.   After introducing an analysis with suitable semantics, we discuss how to encode its rules into a directed hypergraph.   We then present an algorithm for efficiently finding the most effective single cut in the graph.   The execution time of the proposed operations allows for the deployment in interactive tools.   Finally, we show that our graph model is able to expose methods worthwhile to reconsider.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/LLT8C28H/Blumschein et al. - 2024 - Finding Cuts in Static Analysis Graphs to Debloat Software.pdf}
}

@unpublished{blundellModelfreeEpisodicControl2016,
  title = {Model-Free Episodic Control},
  author = {Blundell, Charles and Uria, Benigno and Pritzel, Alexander and Li, Yazhe and Ruderman, Avraham and Leibo, Joel Z and Rae, Jack and Wierstra, Daan and Hassabis, Demis},
  date = {2016},
  eprint = {1606.04460},
  eprinttype = {arXiv},
  keywords = {notion}
}

@misc{bogomolovAuthorshipAttributionSource2020,
  title = {Authorship {{Attribution}} of {{Source Code}}: {{A Language-Agnostic Approach}} and {{Applicability}} in {{Software Engineering}}},
  author = {Bogomolov, Egor and Kovalenko, Vladimir and Bacchelli, Alberto and Bryksin, Timofey},
  date = {2020},
  keywords = {notion}
}

@inproceedings{bohmeAssurancesSoftwareTesting2019,
  title = {Assurances in Software Testing: {{A}} Roadmap},
  booktitle = {2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}}: {{New Ideas}} and {{Emerging Results}} ({{ICSE-NIER}})},
  author = {Böhme, Marcel},
  date = {2019},
  pages = {5--8},
  publisher = {IEEE},
  keywords = {notion}
}

@inproceedings{bohmeBoostingFuzzerEfficiency2020,
  title = {Boosting {{Fuzzer Efficiency}}: {{An Information Theoretic Perspective}}},
  booktitle = {Proceedings of the 28th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Böhme, Marcel and Manès, Valentin J. M. and Cha, Sang Kil},
  date = {2020},
  series = {{{ESEC}}/{{FSE}} 2020},
  pages = {678--689},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3368089.3409748},
  url = {https://doi.org/10.1145/3368089.3409748},
  abstract = {In this paper, we take the fundamental perspective of fuzzing as a learning process. Suppose before fuzzing, we know nothing about the behaviors of a program P: What does it do? Executing the first test input, we learn how P behaves for this input. Executing the next input, we either observe the same or discover a new behavior. As such, each execution reveals ”some amount” of information about P's behaviors. A classic measure of information is Shannon's entropy. Measuring entropy allows us to quantify how much is learned from each generated test input about the behaviors of the program. Within a probabilistic model of fuzzing, we show how entropy also measures fuzzer efficiency. Specifically, it measures the general rate at which the fuzzer discovers new behaviors. Intuitively, efficient fuzzers maximize information. From this information theoretic perspective, we develop Entropic, an entropy-based power schedule for greybox fuzzing which assigns more energy to seeds that maximize information. We implemented Entropic into the popular greybox fuzzer LibFuzzer. Our experiments with more than 250 open-source programs (60 million LoC) demonstrate a substantially improved efficiency and confirm our hypothesis that an efficient fuzzer maximizes information. Entropic has been independently evaluated and invited for integration into main-line LibFuzzer. Entropic now runs on more than 25,000 machines fuzzing hundreds of security-critical software systems simultaneously and continuously.},
  isbn = {978-1-4503-7043-1},
  venue = {Virtual Event, USA},
  keywords = {efficiency,entropy,fuzzing,information theory,notion,software testing}
}

@inproceedings{bohmeCoREBenchStudyingComplexity2014,
  title = {{{CoREBench}}: Studying Complexity of Regression Errors},
  booktitle = {Proceedings of the 2014 {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Böhme, Marcel and Roychoudhury, Abhik},
  date = {2014},
  series = {{{ISSTA}} 2014},
  pages = {105--115},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2610384.2628058},
  url = {https://doi.org/10.1145/2610384.2628058},
  abstract = {Intuitively we know, some software errors are more complex than others. If the error can be fixed by changing one faulty statement, it is a simple error. The more substantial the fix must be, the more complex we consider the error. In this work, we formally define and quantify the complexity of an error w.r.t. the complexity of the error's least complex, correct fix. As a concrete measure of complexity for such fixes, we introduce Cyclomatic Change Complexity which is inspired by existing program complexity metrics. Moreover, we introduce CoREBench, a collection of 70 regression errors systematically extracted from several open-source C-projects and compare their complexity with that of the seeded errors in the two most popular error benchmarks, SIR and the Siemens Suite. We find that seeded errors are significantly less complex, i.e., require significantly less substantial fixes, compared to actual regression errors. For example, among the seeded errors more than 42\% are simple compared to 8\% among the actual ones. This is a concern for the external validity of studies based on seeded errors and we propose CoREBench for the controlled study of regression testing, debugging, and repair techniques.},
  isbn = {978-1-4503-2645-2},
  venue = {San Jose, CA, USA},
  keywords = {Coupling Effect,Error Complexity,notion,Regression}
}

@article{bohmeCoverageBasedGreyboxFuzzing2019,
  title = {Coverage-{{Based Greybox Fuzzing}} as {{Markov Chain}}},
  author = {Böhme, Marcel and Pham, Van-Thuan and Roychoudhury, Abhik},
  date = {2019},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {45},
  number = {5},
  pages = {489--506},
  doi = {10.1109/TSE.2017.2785841},
  keywords = {notion}
}

@inproceedings{bohmeDirectedGreyboxFuzzing2017,
  title = {Directed {{Greybox Fuzzing}}},
  booktitle = {Proceedings of the 2017 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Böhme, Marcel and Pham, Van-Thuan and Nguyen, Manh-Dung and Roychoudhury, Abhik},
  date = {2017},
  series = {{{CCS}} '17},
  pages = {2329--2344},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3133956.3134020},
  url = {https://doi.org/10.1145/3133956.3134020},
  abstract = {Existing Greybox Fuzzers (GF) cannot be effectively directed, for instance, towards problematic changes or patches, towards critical system calls or dangerous locations, or towards functions in the stack-trace of a reported vulnerability that we wish to reproduce. In this paper, we introduce Directed Greybox Fuzzing (DGF) which generates inputs with the objective of reaching a given set of target program locations efficiently. We develop and evaluate a simulated annealing-based power schedule that gradually assigns more energy to seeds that are closer to the target locations while reducing energy for seeds that are further away. Experiments with our implementation AFLGo demonstrate that DGF outperforms both directed symbolic-execution-based whitebox fuzzing and undirected greybox fuzzing. We show applications of DGF to patch testing and crash reproduction, and discuss the integration of AFLGo into Google's continuous fuzzing platform OSS-Fuzz. Due to its directedness, AFLGo could find 39 bugs in several well-fuzzed, security-critical projects like LibXML2. 17 CVEs were assigned.},
  isbn = {978-1-4503-4946-8},
  venue = {Dallas, Texas, USA},
  keywords = {coverage-based greybox fuzzing,crash reproduction,directed testing,notion,patch testing,reachability,verifying true positives}
}

@inproceedings{bohmeEstimatingResidualRisk2021,
  title = {Estimating {{Residual Risk}} in {{Greybox Fuzzing}}},
  booktitle = {Proceedings of the 29th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Böhme, Marcel and Liyanage, Danushka and Wüstholz, Valentin},
  date = {2021},
  series = {{{ESEC}}/{{FSE}} 2021},
  pages = {230--241},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3468264.3468570},
  url = {https://doi.org/10.1145/3468264.3468570},
  abstract = {For any errorless fuzzing campaign, no matter how long, there is always some residual risk that a software error would be discovered if only the campaign was run for just a bit longer. Recently, greybox fuzzing tools have found widespread adoption. Yet, practitioners can only guess when the residual risk of a greybox fuzzing campaign falls below a specific, maximum allowable threshold. In this paper, we explain why residual risk cannot be directly estimated for greybox campaigns, argue that the discovery probability (i.e., the probability that the next generated input increases code coverage) provides an excellent upper bound, and explore sound statistical methods to estimate the discovery probability in an ongoing greybox campaign. We find that estimators for blackbox fuzzing systematically and substantially under-estimate the true risk. An engineer—who stops the campaign when the estimators purport a risk below the maximum allowable risk—is vastly misled. She might need execute a campaign that is orders of magnitude longer to achieve the allowable risk. Hence, the key challenge we address in this paper is adaptive bias: The probability to discover a specific error actually increases over time. We provide the first probabilistic analysis of adaptive bias, and introduce two novel classes of estimators that tackle adaptive bias. With our estimators, the engineer can decide with confidence when to abort the campaign.},
  isbn = {978-1-4503-8562-6},
  venue = {Athens, Greece},
  keywords = {assurance,correctness,estimation,notion,software testing,statistics},
  file = {/Users/bohrok/Zotero/storage/NKAFNU9K/Böhme et al. - 2021 - Estimating Residual Risk in Greybox Fuzzing.pdf}
}

@article{bohmeFuzzingChallengesReflections2020,
  title = {Fuzzing: {{Challenges}} and Reflections},
  author = {Böhme, Marcel and Cadar, Cristian and Roychoudhury, Abhik},
  date = {2020},
  journaltitle = {IEEE Software},
  volume = {38},
  number = {3},
  pages = {79--86},
  publisher = {IEEE},
  keywords = {notion}
}

@inproceedings{bohmeFuzzingExponentialCost2020,
  title = {Fuzzing: {{On}} the {{Exponential Cost}} of {{Vulnerability Discovery}}},
  booktitle = {Proceedings of the 28th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Böhme, Marcel and Falk, Brandon},
  date = {2020},
  series = {{{ESEC}}/{{FSE}} 2020},
  pages = {713--724},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3368089.3409729},
  url = {https://doi.org/10.1145/3368089.3409729},
  abstract = {We present counterintuitive results for the scalability of fuzzing. Given the same non-deterministic fuzzer, finding the same bugs linearly faster requires linearly more machines. For instance, with twice the machines, we can find all known bugs in half the time. Yet, finding linearly more bugs in the same time requires exponentially more machines. For instance, for every new bug we want to find in 24 hours, we might need twice more machines. Similarly for coverage. With exponentially more machines, we can cover the same code exponentially faster, but uncovered code only linearly faster. In other words, re-discovering the same vulnerabilities is cheap but finding new vulnerabilities is expensive. This holds even under the simplifying assumption of no parallelization overhead. We derive these observations from over four CPU years worth of fuzzing campaigns involving almost three hundred open source programs, two state-of-the-art greybox fuzzers, four measures of code coverage, and two measures of vulnerability discovery. We provide a probabilistic analysis and conduct simulation experiments to explain this phenomenon.},
  isbn = {978-1-4503-7043-1},
  venue = {Virtual Event, USA},
  keywords = {efficiency,fuzzing,notion,scalability,software testing}
}

@unpublished{bohmeGuaranteesSoftwareSecurity2024,
  title = {Guarantees in {{Software Security}}},
  author = {Böhme, Marcel},
  date = {2024},
  eprint = {2402.01944},
  eprinttype = {arXiv},
  keywords = {notion}
}

@inproceedings{bohmeHumanLoopAutomaticProgram2020,
  title = {Human-{{In-The-Loop Automatic Program Repair}}},
  booktitle = {2020 {{IEEE}} 13th {{International Conference}} on {{Software Testing}}, {{Validation}} and {{Verification}} ({{ICST}})},
  author = {Böhme, M. and Geethal, C. and Pham, V.-T.},
  date = {2020},
  pages = {274--285},
  doi = {10.1109/ICST46399.2020.00036},
  keywords = {notion}
}

@unpublished{bohmeMCPAProgramAnalysis2019,
  title = {{{MCPA}}: {{Program}} Analysis as Machine Learning},
  author = {Böhme, Marcel},
  date = {2019},
  eprint = {1911.04687},
  eprinttype = {arXiv},
  keywords = {notion}
}

@article{bohmeProbabilisticAnalysisEfficiency2016,
  title = {A {{Probabilistic Analysis}} of the {{Efficiency}} of {{Automated Software Testing}}},
  author = {Böhme, Marcel and Paul, Soumya},
  date = {2016},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {42},
  pages = {345--360},
  keywords = {notion}
}

@online{bohmeSoftwareSecurityAnalysis2024,
  title = {Software {{Security Analysis}} in 2030 and {{Beyond}}: {{A Research Roadmap}}},
  shorttitle = {Software {{Security Analysis}} in 2030 and {{Beyond}}},
  author = {Böhme, Marcel and Bodden, Eric and Bultan, Tevfik and Cadar, Cristian and Liu, Yang and Scanniello, Giuseppe},
  date = {2024-09-26},
  url = {https://arxiv.org/abs/2409.17844v1},
  urldate = {2024-10-14},
  abstract = {As our lives, our businesses, and indeed our world economy become increasingly reliant on the secure operation of many interconnected software systems, the software engineering research community is faced with unprecedented research challenges, but also with exciting new opportunities. In this roadmap paper, we outline our vision of Software Security Analysis for the software systems of the future. Given the recent advances in generative AI, we need new methods to evaluate and maximize the security of code co-written by machines. As our software systems become increasingly heterogeneous, we need practical approaches that work even if some functions are automatically generated, e.g., by deep neural networks. As software systems depend evermore on the software supply chain, we need tools that scale to an entire ecosystem. What kind of vulnerabilities exist in future systems and how do we detect them? When all the shallow bugs are found, how do we discover vulnerabilities hidden deeply in the system? Assuming we cannot find all security flaws, how can we nevertheless protect our system? To answer these questions, we start our research roadmap with a survey of recent advances in software security, then discuss open challenges and opportunities, and conclude with a long-term perspective for the field.},
  langid = {english},
  organization = {arXiv.org},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/VECQBCTQ/Böhme et al. - 2024 - Software Security Analysis in 2030 and Beyond A Research Roadmap.pdf}
}

@article{bohmeSTADSSoftwareTesting2018,
  title = {{{STADS}}: {{Software}} Testing as Species Discovery},
  shorttitle = {{{STADS}}},
  author = {Böhme, Marcel},
  date = {2018-06-27},
  journaltitle = {ACM Trans. Softw. Eng. Methodol.},
  volume = {27},
  number = {2},
  pages = {7:1--7:52},
  issn = {1049-331X},
  doi = {10.1145/3210309},
  url = {https://dl.acm.org/doi/10.1145/3210309},
  urldate = {2024-10-15},
  abstract = {A fundamental challenge of software testing is the statistically well-grounded extrapolation from program behaviors observed during testing. For instance, a security researcher who has run the fuzzer for a week has currently no means (1)\&nbsp;to estimate the total number of feasible program branches, given that only a fraction has been covered so far; (2)\&nbsp;to estimate the additional time required to cover 10\% more branches (or to estimate the coverage achieved in one more day, respectively); or (3)\&nbsp;to assess the residual risk that a vulnerability exists when no vulnerability has been discovered. Failing to discover a vulnerability does not mean that none exists—even if the fuzzer was run for a week (or a year). Hence, testing provides no formal correctness guarantees.In this article, I establish an unexpected connection with the otherwise unrelated scientific field of ecology and introduce a statistical framework that models Software Testing and Analysis as Discovery of Species (STADS). For instance, in order to study the species diversity of arthropods in a tropical rain forest, ecologists would first sample a large number of individuals from that forest, determine their species, and extrapolate from the properties observed in the sample to properties of the whole forest. The estimations (1)\&nbsp;of the total number of species, (2)\&nbsp;of the additional sampling effort required to discover 10\% more species, or (3)\&nbsp;of the probability to discover a new species are classical problems in ecology. The STADS framework draws from over three decades of research in ecological biostatistics to address the fundamental extrapolation challenge for automated test generation. Our preliminary empirical study demonstrates a good estimator performance even for a fuzzer with adaptive sampling bias—AFL, a state-of-the-art vulnerability detection tool. The STADS framework provides statistical correctness guarantees with quantifiable accuracy.},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/XFEWST7F/Böhme - 2018 - STADS Software Testing as Species Discovery.pdf}
}

@inproceedings{bohmeStatisticalReasoningPrograms2022,
  title = {Statistical Reasoning about Programs},
  booktitle = {Proceedings of the {{ACM}}/{{IEEE}} 44th {{International Conference}} on {{Software Engineering}}: {{New Ideas}} and {{Emerging Results}}},
  author = {Böhme, Marcel},
  date = {2022-10-17},
  series = {{{ICSE-NIER}} '22},
  pages = {76--80},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3510455.3512796},
  url = {https://dl.acm.org/doi/10.1145/3510455.3512796},
  urldate = {2024-11-12},
  abstract = {We discuss the advent of a new program analysis paradigm that allows anyone to make precise statements about the behavior of programs as they run in production across hundreds and millions of machines or devices. The scale-oblivious, in vivo program analysis leverages an almost inconceivable rate of user-generated program executions across large fleets to analyze programs of arbitrary size and composition with negligible performance overhead.In this paper, we reflect on the program analysis problem, the prevalent paradigm, and the practical reality of program analysis at large software companies. We illustrate the new paradigm using several success stories and suggest a number of exciting new research directions.},
  isbn = {978-1-4503-9224-2},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/KRED3L26/Böhme - 2022 - Statistical reasoning about programs.pdf}
}

@article{bonehEstimatingPredictionFunction1998,
  title = {Estimating the {{Prediction Function}} and the {{Number}} of {{Unseen Species}} in {{Sampling}} with {{Replacement}}},
  author = {Boneh, Shahar and Boneh, Arnon and Caron, R. J.},
  date = {1998},
  journaltitle = {Journal of the American Statistical Association},
  volume = {93},
  number = {441},
  pages = {372--379},
  publisher = {Taylor \& Francis},
  doi = {10.1080/01621459.1998.10474118},
  url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1998.10474118},
  keywords = {notion}
}

@article{borgesCompositionalSolutionSpace2014,
  title = {Compositional Solution Space Quantification for Probabilistic Software Analysis},
  author = {Borges, Mateus and Filieri, Antonio and family=Amorim, given=Marcelo, prefix=d', useprefix=true and Pasareanu, Corina S. and Visser, Willem},
  date = {2014},
  journaltitle = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  url = {https://api.semanticscholar.org/CorpusID:6376244},
  keywords = {notion}
}

@inproceedings{borgesIterativeDistributionAwareSampling2015,
  title = {Iterative {{Distribution-Aware Sampling}} for {{Probabilistic Symbolic Execution}}},
  booktitle = {Proceedings of the 2015 10th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Borges, Mateus and Filieri, Antonio and D'Amorim, Marcelo and Păsăreanu, Corina S.},
  date = {2015},
  series = {{{ESEC}}/{{FSE}} 2015},
  pages = {866--877},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2786805.2786832},
  url = {https://doi.org/10.1145/2786805.2786832},
  abstract = {Probabilistic symbolic execution aims at quantifying the probability of reaching program events of interest assuming that program inputs follow given probabilistic distributions. The technique collects constraints on the inputs that lead to the target events and analyzes them to quantify how likely it is for an input to satisfy the constraints. Current techniques either handle only linear constraints or only support continuous distributions using a “discretization” of the input domain, leading to imprecise and costly results. We propose an iterative distribution-aware sampling approach to support probabilistic symbolic execution for arbitrarily complex mathematical constraints and continuous input distributions. We follow a compositional approach, where the symbolic constraints are decomposed into sub-problems whose solution can be solved independently. At each iteration the convergence rate of the com- putation is increased by automatically refocusing the analysis on estimating the sub-problems that mostly affect the accuracy of the results, as guided by three different ranking strategies. Experiments on publicly available benchmarks show that the proposed technique improves on previous approaches in terms of scalability and accuracy of the results.},
  isbn = {978-1-4503-3675-8},
  venue = {Bergamo, Italy},
  keywords = {Monte Carlo Sampling,notion,Probabilistic Analysis,Symbolic Execution}
}

@inproceedings{borzacchielloFuzzingSymbolicExpressions2021,
  title = {Fuzzing {{Symbolic Expressions}}},
  booktitle = {2021 {{IEEE}}/{{ACM}} 43rd {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Borzacchiello, Luca and Coppa, Emilio and Demetrescu, Camil},
  date = {2021},
  pages = {711--722},
  doi = {10.1109/ICSE43902.2021.00071},
  keywords = {notion}
}

@inproceedings{bowesBugVisCommitSlicing2020,
  title = {{{BugVis}}: {{Commit Slicing}} for {{Fault Visualisation}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Program Comprehension}}},
  author = {Bowes, David and Petrić, Jean and Hall, Tracy},
  date = {2020},
  series = {{{ICPC}} '20},
  pages = {436--440},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3387904.3389299},
  url = {https://doi.org/10.1145/3387904.3389299},
  abstract = {In this paper we present BugVis, our tool which allows the visualisation of the lifetime of a code fault. The commit history of the fault from insertion to fix is visualised. Unlike previous similar tools, BugVis visualises only the lines of each commit involved in the fault. The visualisation creates a commit slice throughout the history of the fault which enables comprehension of the evolution of the code involved in the fault.},
  isbn = {978-1-4503-7958-8},
  venue = {Seoul, Republic of Korea},
  keywords = {bug,commit,fault,fix,notion,visualisation}
}

@inproceedings{bozhkoWhatReallyPWCET2023,
  title = {What {{Really}} Is {{pWCET}}? {{A Rigorous Axiomatic Proposal}}},
  shorttitle = {What {{Really}} Is {{pWCET}}?},
  booktitle = {2023 {{IEEE Real-Time Systems Symposium}} ({{RTSS}})},
  author = {Bozhko, Sergey and Marković, Filip and family=Brüggen, given=Georg, prefix=von der, useprefix=true and Brandenburg, Björn B.},
  date = {2023-12},
  pages = {13--26},
  issn = {2576-3172},
  doi = {10.1109/RTSS59052.2023.00012},
  url = {https://ieeexplore.ieee.org/document/10406031},
  urldate = {2024-10-18},
  abstract = {The concept of a probabilistic worst-case execution time (pWCET) has gradually emerged from the work of many authors over the course of 2–3 decades. Intuitively, pWCET is a simplifying model abstraction that safely over-approximates the ground-truth probabilistic execution time (pET) of a real-time task. In particular, when analyzing the cumulative processor demand of multiple jobs, the pWCET abstraction is intended to allow for the use of techniques from probability theory that require random variables to be independent and identically distributed (IID), even though the underlying ground-truth pET random variables are usually not independent. However, while powerful, the pWCET concept is subtle and difficult to define precisely, and easily misinterpreted. To place the pWCET concept on firm, unambiguous mathematical foundations, this paper proposes the first rigorous, axiomatic definition of pWCET that is suitable for formal proof. In addition, an adequacy property is stated that formally captures the intuitive notion of an “IID upper bound on pET.” The proposed pWCET definition is shown to satisfy this adequacy condition, and thereby is the first notion of pWCET for which the IID guarantee is formally established. All definitions and proofs have been verified with the Coq proof assistant.},
  eventtitle = {2023 {{IEEE Real-Time Systems Symposium}} ({{RTSS}})},
  keywords = {Coq,IID,notion,Probabilistic logic,Probability Theory,Proposals,pWCET,Random variables,Real-time systems,Real-Time Systems,Task analysis,Testing,Upper bound},
  file = {/Users/bohrok/Zotero/storage/FGDLEJPI/Bozhko et al. - 2023 - What Really is pWCET A Rigorous Axiomatic Proposal.pdf;/Users/bohrok/Zotero/storage/DJBUG68X/10406031.html}
}

@inproceedings{brandtMindGapWhat2024,
  title = {Mind the {{Gap}}: {{What Working With Developers}} on {{Fuzz Tests Taught Us About Coverage Gaps}}},
  booktitle = {Proceedings of the 46th {{International Conference}} on {{Software Engineering}}: {{Software Engineering}} in {{Practice}}},
  author = {Brandt, Carolin and Castelluccio, Marco and Holler, Christian and Kratzer, Jason and Zaidman, Andy and Bacchelli, Alberto},
  date = {2024},
  series = {{{ICSE-SEIP}} '24},
  pages = {157--167},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3639477.3639721},
  url = {https://doi.org/10.1145/3639477.3639721},
  abstract = {Can fuzzers generate partial tests that developers find useful enough to complete into functional tests (e.g., by adding assertions)? To address this question, we develop a prototype within the Mozilla ecosystem and open 13 bug reports proposing partial generated tests for currently uncovered code. We found that the majority of the reactions focus on whether the targeted coverage gap is actually worth testing. To investigate further which coverage gaps developers find relevant to close, we design an automated filter to exclude irrelevant coverage gaps before generating tests. From conversations with 13 developers about whether the remaining coverage gaps are worth closing when a partially generated test is available, we learn that the filtering indeed removes clearly non-test-worthy gaps. The developers propose a variety of additional strategies to address the coverage gaps and how to make fuzz tests and reports more useful for developers.},
  isbn = {9798400705014},
  venue = {Lisbon, Portugal},
  keywords = {notion}
}

@unpublished{brazSoftwareSecurityModern2022,
  title = {Software {{Security}} during {{Modern Code Review}}: {{The Developer}}'s {{Perspective}}},
  author = {Braz, Larissa and Bacchelli, Alberto},
  date = {2022},
  eprint = {2208.04261},
  eprinttype = {arXiv},
  keywords = {notion}
}

@inproceedings{brennanConstraintNormalizationParameterized2017,
  title = {Constraint {{Normalization}} and {{Parameterized Caching}} for {{Quantitative Program Analysis}}},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Brennan, Tegan and Tsiskaridze, Nestan and Rosner, Nicolás and Aydin, Abdulbaki and Bultan, Tevfik},
  date = {2017},
  series = {{{ESEC}}/{{FSE}} 2017},
  pages = {535--546},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/3106237.3106303},
  url = {http://doi.acm.org/10.1145/3106237.3106303},
  isbn = {978-1-4503-5105-8},
  venue = {Paderborn, Germany},
  keywords = {Constraint Caching,Model Counting,notion,Program Analysis,Quantitative Program Analysis,String Constraints}
}

@inproceedings{brillingerDataAnalysesUsing2004,
  title = {Some Data Analyses Using Mutual Information},
  author = {Brillinger, David R.},
  date = {2004},
  keywords = {notion}
}

@article{brinAnatomyLargescaleHypertextual1998,
  title = {The Anatomy of a Large-Scale Hypertextual {{Web}} Search Engine},
  author = {Brin, Sergey and Page, Lawrence},
  date = {1998},
  journaltitle = {Computer Networks and ISDN Systems},
  volume = {30},
  number = {1},
  pages = {107--117},
  issn = {0169-7552},
  doi = {10.1016/S0169-7552(98)00110-X},
  url = {https://www.sciencedirect.com/science/article/pii/S016975529800110X},
  abstract = {In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/ To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of Web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the Web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and Web proliferation, creating a Web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale Web search engine — the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.},
  keywords = {Google,Information retrieval,notion,PageRank,Search engines,World Wide Web}
}

@article{bronsteinGeometricDeepLearning2021,
  title = {Geometric {{Deep Learning}}: {{Grids}}, {{Groups}}, {{Graphs}}, {{Geodesics}}, and {{Gauges}}},
  author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Velickovic, Petar},
  date = {2021},
  journaltitle = {CoRR},
  volume = {abs/2104.13478},
  url = {https://arxiv.org/abs/2104.13478},
  keywords = {notion}
}

@inproceedings{brownCareFeedingWildcaught2017,
  title = {The {{Care}} and {{Feeding}} of {{Wild-caught Mutants}}},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Brown, David Bingham and Vaughn, Michael and Liblit, Ben and Reps, Thomas},
  date = {2017},
  series = {{{ESEC}}/{{FSE}} 2017},
  pages = {511--522},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/3106237.3106280},
  url = {http://doi.acm.org/10.1145/3106237.3106280},
  isbn = {978-1-4503-5105-8},
  venue = {Paderborn, Germany},
  keywords = {Mutation Testing,notion,Repository Mining,Wild Mutant}
}

@article{brownIntervalEstimationBinomial2001,
  title = {Interval {{Estimation}} for a {{Binomial Proportion}}},
  author = {Brown, Lawrence D. and Cai, Tianwen Tony and Dasgupta, Anirban},
  date = {2001},
  journaltitle = {Statistical Science},
  volume = {16},
  pages = {101--133},
  keywords = {notion}
}

@inproceedings{bruceJShrinkDepthInvestigation2020,
  title = {{{JShrink}}: {{In-Depth Investigation}} into {{Debloating Modern Java Applications}}},
  booktitle = {Proceedings of the 28th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Bruce, Bobby R. and Zhang, Tianyi and Arora, Jaspreet and Xu, Guoqing Harry and Kim, Miryung},
  date = {2020},
  series = {{{ESEC}}/{{FSE}} 2020},
  pages = {135--146},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3368089.3409738},
  url = {https://doi.org/10.1145/3368089.3409738},
  abstract = {Modern software is bloated. Demand for new functionality has led developers to include more and more features, many of which become unneeded or unused as software evolves. This phenomenon, known as software bloat, results in software consuming more resources than it otherwise needs to. How to effectively and automatically debloat software is a long-standing problem in software engineering. Various debloating techniques have been proposed since the late 1990s. However, many of these techniques are built upon pure static analysis and have yet to be extended and evaluated in the context of modern Java applications where dynamic language features are prevalent. To this end, we develop an end-to-end bytecode debloating framework called JShrink. It augments traditional static reachability analysis with dynamic profiling and type dependency analysis and renovates existing bytecode transformations to account for new language features in modern Java. We highlight several nuanced technical challenges that must be handled properly and examine behavior preservation of debloated software via regression testing. We find that (1) JShrink is able to debloat our real-world Java benchmark suite by up to 47\% (14\% on average); (2) accounting for dynamic language features is indeed crucial to ensure behavior preservation—reducing 98\% of test failures incurred by a purely static equivalent, Jax, and 84\% for ProGuard; and (3) compared with purely dynamic approaches, integrating static analysis with dynamic profiling makes the debloated software more robust to unseen test executions—in 22 out of 26 projects, the debloated software ran successfully under new tests.},
  isbn = {978-1-4503-7043-1},
  venue = {Virtual Event, USA},
  keywords = {debloating,Java bytecode,notion,reachability analysis,size reduction}
}

@article{brumleyRemoteTimingAttacks2005,
  title = {Remote Timing Attacks Are Practical},
  author = {Brumley, David and Boneh, Dan},
  date = {2005},
  journaltitle = {Computer Networks},
  volume = {48},
  number = {5},
  pages = {701--716},
  issn = {1389-1286},
  doi = {10.1016/j.comnet.2005.01.010},
  url = {https://www.sciencedirect.com/science/article/pii/S1389128605000125},
  abstract = {Timing attacks are usually used to attack weak computing devices such as smartcards. We show that timing attacks apply to general software systems. Specifically, we devise a timing attack against OpenSSL. Our experiments show that we can extract private keys from an OpenSSL-based web server running on a machine in the local network. Our results demonstrate that timing attacks against network servers are practical and therefore security systems should defend against them.},
  keywords = {Chinese remainder,Montgomery reductions,notion,RSA,SSL,Timing attack}
}

@inproceedings{buiInferCodeSelfSupervisedLearning2021,
  title = {{{InferCode}}: {{Self-Supervised Learning}} of {{Code Representations}} by {{Predicting Subtrees}}},
  booktitle = {Proceedings of the 43rd {{International Conference}} on {{Software Engineering}}},
  author = {Bui, Nghi D. Q. and Yu, Yijun and Jiang, Lingxiao},
  date = {2021},
  keywords = {notion}
}

@inproceedings{bultanQuantifyingInformationLeakage2019,
  title = {Quantifying {{Information Leakage Using Model Counting Constraint Solvers}}},
  booktitle = {Verified {{Software}}: {{Theories}}, {{Tools}}, {{Experiments}}},
  author = {Bultan, Tevfik},
  date = {2019},
  url = {https://api.semanticscholar.org/CorpusID:212727288},
  keywords = {notion}
}

@article{bungeEstimatingNumberSpecies1993,
  title = {Estimating the Number of Species: A Review},
  author = {Bunge, John and Fitzpatrick, Michael},
  date = {1993},
  journaltitle = {Journal of the American Statistical Association},
  volume = {88},
  number = {421},
  pages = {364--373},
  publisher = {Taylor \& Francis},
  keywords = {notion}
}

@article{burnhamEstimationSizeClosed1978,
  title = {Estimation of the Size of a Closed Population When Capture Probabilities Vary among Animals},
  author = {Burnham, Kenneth P. and Overton, Walter Scott},
  date = {1978},
  journaltitle = {Biometrika},
  volume = {65},
  pages = {625--633},
  keywords = {notion}
}

@inproceedings{busseCombiningStaticAnalysis2022,
  title = {Combining {{Static Analysis Error Traces}} with {{Dynamic Symbolic Execution}} ({{Experience Paper}})},
  booktitle = {Proceedings of the 31st {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Busse, Frank and Gharat, Pritam and Cadar, Cristian and Donaldson, Alastair F.},
  date = {2022},
  series = {{{ISSTA}} 2022},
  pages = {568--579},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3533767.3534384},
  url = {https://doi.org/10.1145/3533767.3534384},
  abstract = {This paper reports on our experience implementing a technique for sifting through static analysis reports using dynamic symbolic execution. Our insight is that if a static analysis tool produces a partial trace through the program under analysis, annotated with conditions that the analyser believes are important for the bug to trigger, then a dynamic symbolic execution tool may be able to exploit the trace by (a) guiding the search heuristically so that paths that follow the trace most closely are prioritised for exploration, and (b) pruning the search using the conditions associated with each step of the trace. This may allow the bug to be quickly confirmed using dynamic symbolic execution, if it turns out to be a true positive, yielding an input that triggers the bug. To experiment with this approach, we have implemented the idea in a tool chain that allows the popular open-source static analysis tools Clang Static Analyzer (CSA) and Infer to be combined with the popular open-source dynamic symbolic execution engine KLEE. Our findings highlight two interesting negative results. First, while fault injection experiments show the promise of our technique, they also reveal that the traces provided by static analysis tools are not that useful in guiding search. Second, we have systematically applied CSA and Infer to a large corpus of real-world applications that are suitable for analysis with KLEE, and find that the static analysers are rarely able to find non-trivial true positive bugs for this set of applications. We believe our case study can inform static analysis and dynamic symbolic execution tool developers as to where improvements may be necessary, and serve as a call to arms for researchers interested in combining symbolic execution and static analysis to identify more suitable benchmark suites for evaluation of research ideas.},
  isbn = {978-1-4503-9379-9},
  venue = {Virtual, South Korea},
  keywords = {Clang Static Analyzer,Infer,KLEE,notion,software testing,static analysis,symbolic execution}
}

@inproceedings{busseRunningSymbolicExecution2020,
  title = {Running Symbolic Execution Forever},
  booktitle = {Proceedings of the 29th {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Busse, Frank and Nowack, Martin and Cadar, Cristian},
  date = {2020},
  series = {{{ISSTA}} 2020},
  pages = {63--74},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3395363.3397360},
  url = {https://doi.org/10.1145/3395363.3397360},
  abstract = {When symbolic execution is used to analyse real-world applications, it often consumes all available memory in a relatively short amount of time, sometimes making it impossible to analyse an application for an extended period. In this paper, we present a technique that can record an ongoing symbolic execution analysis to disk and selectively restore paths of interest later, making it possible to run symbolic execution indefinitely. To be successful, our approach addresses several essential research challenges related to detecting divergences on re-execution, storing long-running executions efficiently, changing search heuristics during re-execution, and providing a global view of the stored execution. Our extensive evaluation of 93 Linux applications shows that our approach is practical, enabling these applications to run for days while continuing to explore new execution paths.},
  isbn = {978-1-4503-8008-9},
  venue = {Virtual Event, USA},
  keywords = {KLEE,memoization,notion,symbolic execution}
}

@inproceedings{cadarKLEEUnassistedAutomatic2008,
  title = {{{KLEE}}: {{Unassisted}} and {{Automatic Generation}} of {{High-Coverage Tests}} for {{Complex Systems Programs}}},
  booktitle = {Proceedings of the 8th {{USENIX Conference}} on {{Operating Systems Design}} and {{Implementation}}},
  author = {Cadar, Cristian and Dunbar, Daniel and Engler, Dawson},
  date = {2008},
  series = {{{OSDI}}'08},
  pages = {209--224},
  publisher = {USENIX Association},
  location = {USA},
  abstract = {We present a new symbolic execution tool, KLEE, capable of automatically generating tests that achieve high coverage on a diverse set of complex and environmentally-intensive programs. We used KLEE to thoroughly check all 89 stand-alone programs in the GNU COREUTILS utility suite, which form the core user-level environment installed on millions of Unix systems, and arguably are the single most heavily tested set of open-source programs in existence. KLEE-generated tests achieve high line coverage – on average over 90\% per tool (median: over 94\%) – and significantly beat the coverage of the developers' own hand-written test suite. When we did the same for 75 equivalent tools in the BUSYBOX embedded system suite, results were even better, including 100\% coverage on 31 of them.We also used KLEE as a bug finding tool, applying it to 452 applications (over 430K total lines of code), where it found 56 serious bugs, including three in COREUTILS that had been missed for over 15 years. Finally, we used KLEE to crosscheck purportedly identical BUSYBOX and COREUTILS utilities, finding functional correctness errors and a myriad of inconsistencies.},
  venue = {San Diego, California},
  keywords = {notion}
}

@article{caiD^22AbsFrameworkDynamic2022,
  title = {D\textasciicircum{{22Abs}}: {{A Framework}} for {{Dynamic Dependence Analysis}} of {{Distributed Programs}}},
  author = {Cai, Haipeng and Fu, Xiaoqin},
  date = {2022},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {48},
  number = {12},
  pages = {4733--4761},
  doi = {10.1109/TSE.2021.3124795},
  keywords = {notion}
}

@article{caiMakingNeuralProgramming2017,
  title = {Making {{Neural Programming Architectures Generalize}} via {{Recursion}}},
  author = {Cai, Jonathon and Shin, Richard and Song, Dawn},
  date = {2017},
  journaltitle = {CoRR},
  volume = {abs/1704.06611},
  eprint = {1704.06611},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/1704.06611},
  keywords = {notion}
}

@article{calcagnoCompositionalShapeAnalysis2009,
  title = {Compositional {{Shape Analysis}} by {{Means}} of {{Bi-abduction}}},
  author = {Calcagno, Cristiano and Distefano, Dino and O'Hearn, Peter and Yang, Hongseok},
  date = {2009-01},
  journaltitle = {SIGPLAN Not.},
  volume = {44},
  number = {1},
  pages = {289--300},
  publisher = {ACM},
  location = {New York, NY, USA},
  issn = {0362-1340},
  doi = {10.1145/1594834.1480917},
  url = {http://doi.acm.org/10.1145/1594834.1480917},
  keywords = {Bi-abduction,notion,Program Analysis,Proof Theory}
}

@inproceedings{calcagnoInferAutomaticProgram2011,
  title = {Infer: {{An Automatic Program Verifier}} for {{Memory Safety}} of {{C Programs}}},
  booktitle = {{{NASA Formal Methods}}},
  author = {Calcagno, Cristiano and Distefano, Dino},
  date = {2011},
  keywords = {notion}
}

@inproceedings{canelasUnderstandingMisconfigurationsROS2024,
  title = {Understanding {{Misconfigurations}} in {{ROS}}: {{An Empirical Study}} and {{Current Approaches}}},
  shorttitle = {Understanding {{Misconfigurations}} in {{ROS}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Canelas, Paulo and Schmerl, Bradley and Fonseca, Alcides and Timperley, Christopher S.},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {1161--1173},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3680350},
  url = {https://dl.acm.org/doi/10.1145/3650212.3680350},
  urldate = {2024-09-21},
  abstract = {The Robot Operating System (ROS) is a popular framework and ecosystem that allows developers to build robot software systems from reusable, off-the-shelf components. Systems are often built by customizing and connecting components via configuration files. While reusable components theoretically allow rapid prototyping, ensuring proper configuration and connection is challenging, as evidenced by numerous questions on developer forums. Developers must abide to the often unchecked and unstated assumptions of individual components. Failure to do so can result in misconfigurations that are only discovered during field deployment, at which point errors may lead to unpredictable and dangerous behavior. Despite misconfigurations having been studied in the broader context of software engineering, robotics software (and ROS in particular) poses domain-specific challenges with potentially disastrous consequences. To understand and improve the reliability of ROS projects, it is critical to identify the types of misconfigurations faced by developers. To that end, we perform a study of ROS Answers, a Q\&amp;A platform, to identify and categorize misconfigurations that occur during ROS development. We then conduct a literature review to assess the coverage of these misconfigurations by existing detection techniques. In total, we find 12 high-level categories and 50 sub-categories of misconfigurations. Of these categories, 27 are not covered by existing techniques. To conclude, we discuss how to tackle those misconfigurations in future work.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/EQMZTFC4/Canelas et al. - 2024 - Understanding Misconfigurations in ROS An Empirical Study and Current Approaches.pdf}
}

@article{caoDeepFDAutomatedFault2022,
  title = {{{DeepFD}}: {{Automated Fault Diagnosis}} and {{Localization}} for {{Deep Learning Programs}}},
  author = {Cao, Jialun and Li, Meiziniu and Chen, Xiao and Wen, Ming and Tian, Yongqiang and Wu, Bo and Cheung, S. C.},
  date = {2022},
  journaltitle = {ArXiv},
  volume = {abs/2205.01938},
  keywords = {notion}
}

@article{caoRealtimeMultiPerson2D2016,
  title = {Realtime {{Multi-Person 2D Pose Estimation}} Using {{Part Affinity Fields}}},
  author = {Cao, Zhe and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
  date = {2016},
  journaltitle = {CoRR},
  volume = {abs/1611.08050},
  url = {http://arxiv.org/abs/1611.08050},
  keywords = {notion}
}

@article{carraraEstimationMutualInformation2019,
  title = {On the {{Estimation}} of {{Mutual Information}}},
  author = {Carrara, Nicholas and Ernst, Jesse},
  date = {2019},
  journaltitle = {Proceedings},
  keywords = {notion}
}

@inproceedings{carzanigaMeasuringSoftwareRedundancy2015,
  title = {Measuring {{Software Redundancy}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Software Engineering}} - {{Volume}} 1},
  author = {Carzaniga, Antonio and Mattavelli, Andrea and Pezzè, Mauro},
  date = {2015},
  series = {{{ICSE}} '15},
  pages = {156--166},
  publisher = {IEEE Press},
  location = {Piscataway, NJ, USA},
  url = {http://dl.acm.org/citation.cfm?id=2818754.2818776},
  isbn = {978-1-4799-1934-5},
  venue = {Florence, Italy},
  keywords = {notion,Software Redundancy}
}

@article{casalnuovoStudyingDifferenceNatural2019,
  title = {Studying the Difference between Natural and Programming Language Corpora},
  author = {Casalnuovo, Casey and Sagae, Kenji and Devanbu, Prem},
  date = {2019-08-01},
  journaltitle = {Empirical Software Engineering},
  volume = {24},
  number = {4},
  pages = {1823--1868},
  issn = {1573-7616},
  doi = {10.1007/s10664-018-9669-7},
  url = {https://doi.org/10.1007/s10664-018-9669-7},
  abstract = {Code corpora, as observed in large software systems, are now known to be far more repetitive and predictable than natural language corpora. But why? Does the difference simply arise from the syntactic limitations of programming languages? Or does it arise from the differences in authoring decisions made by the writers of these natural and programming language texts? We conjecture that the differences are not entirely due to syntax, but also from the fact that reading and writing code is un-natural for humans, and requires substantial mental effort; so, people prefer to write code in ways that are familiar to both reader and writer. To support this argument, we present results from two sets of studies: 1) a first set aimed at attenuating the effects of syntax, and 2) a second, aimed at measuring repetitiveness of text written in other settings (e.g. second language, technical/specialized jargon), which are also effortful to write. We find that this repetition in source code is not entirely the result of grammar constraints, and thus some repetition must result from human choice. While the evidence we find of similar repetitive behavior in technical and learner corpora does not conclusively show that such language is used by humans to mitigate difficulty, it is consistent with that theory. This discovery of “non-syntactic” repetitive behaviour is actionable, and can be leveraged for statistically significant improvements on the code suggestion task. We discuss this finding, and other future implications on practice, and for research.},
  keywords = {notion}
}

@inproceedings{casalnuovoTheoryDualChannel2020,
  title = {A {{Theory}} of {{Dual Channel Constraints}}},
  booktitle = {2020 {{IEEE}}/{{ACM}} 42nd {{International Conference}} on {{Software Engineering}}: {{New Ideas}} and {{Emerging Results}} ({{ICSE-NIER}})},
  author = {Casalnuovo, Casey and Barr, Earl T. and Dash, Santanu Kumar and Devanbu, Prem and Morgan, Emily},
  date = {2020},
  pages = {25--28},
  keywords = {notion}
}

@inproceedings{castelluccioAutomaticallyAnalyzingGroups2017,
  title = {Automatically {{Analyzing Groups}} of {{Crashes}} for {{Finding Correlations}}},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Castelluccio, Marco and Sansone, Carlo and Verdoliva, Luisa and Poggi, Giovanni},
  date = {2017},
  series = {{{ESEC}}/{{FSE}} 2017},
  pages = {717--726},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/3106237.3106306},
  url = {http://doi.acm.org/10.1145/3106237.3106306},
  isbn = {978-1-4503-5105-8},
  venue = {Paderborn, Germany},
  keywords = {Crash Analysis,Crash Reports,Crashes,notion}
}

@article{cazorlaProbabilisticWorstCaseTiming2019,
  title = {Probabilistic {{Worst-Case Timing Analysis}}: {{Taxonomy}} and {{Comprehensive Survey}}},
  author = {Cazorla, Francisco J. and Kosmidis, Leonidas and Mezzetti, Enrico and Hernandez, Carles and Abella, Jaume and Vardanega, Tullio},
  date = {2019-02},
  journaltitle = {ACM Comput. Surv.},
  volume = {52},
  number = {1},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {0360-0300},
  doi = {10.1145/3301283},
  url = {https://doi.org/10.1145/3301283},
  abstract = {The unabated increase in the complexity of the hardware and software components of modern embedded real-time systems has given momentum to a host of research in the use of probabilistic and statistical techniques for timing analysis. In the last few years, that front of investigation has yielded a body of scientific literature vast enough to warrant some comprehensive taxonomy of motivations, strategies of application, and directions of research. This survey addresses this very need, singling out the principal techniques in the state of the art of timing analysis that employ probabilistic reasoning at some level, building a taxonomy of them, discussing their relative merit and limitations, and the relations among them. In addition to offering a comprehensive foundation to savvy probabilistic timing analysis, this article also identifies the key challenges to be addressed to consolidate the scientific soundness and industrial viability of this emerging field.},
  keywords = {notion,probabilistic analysis,Worst-case execution time}
}

@inproceedings{ceccatoSOFIAAutomatedSecurity2016,
  title = {{{SOFIA}}: {{An Automated Security Oracle}} for {{Black-box Testing}} of {{SQL-injection Vulnerabilities}}},
  booktitle = {Proceedings of the 31st {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Ceccato, Mariano and Nguyen, Cu D. and Appelt, Dennis and Briand, Lionel C.},
  date = {2016},
  series = {{{ASE}} 2016},
  pages = {167--177},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2970276.2970343},
  url = {http://doi.acm.org/10.1145/2970276.2970343},
  isbn = {978-1-4503-3845-5},
  venue = {Singapore, Singapore},
  keywords = {notion,Security Testing,SOFIA,SQL-injection}
}

@inproceedings{chaAutomaticallyGeneratingSearch2018,
  title = {Automatically {{Generating Search Heuristics}} for {{Concolic Testing}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Software Engineering}}, {{ICSE}}, {{Gothenburg}}, {{Sweden}}},
  author = {Cha, Sooyoung and Hong, Seongjoon and Lee, Junhee and Oh, Hakjoo},
  date = {2018},
  keywords = {notion}
}

@inproceedings{chaimEfficientlyFindingData2021,
  title = {Efficiently {{Finding Data Flow Subsumptions}}},
  booktitle = {2021 14th {{IEEE Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}} ({{ICST}})},
  author = {Chaim, Marcos Lordello and Baral, Kesina and Offutt, Jeff and Concilio, Mario and Araujo, Roberto P. A.},
  date = {2021},
  keywords = {data flow analysis,notion}
}

@inproceedings{chakrabortyNatGenGenerativePretraining2022,
  title = {{{NatGen}}: {{Generative}} Pre-Training by" {{Naturalizing}}" Source Code},
  booktitle = {Proceedings of the 30th {{ACM Joint European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}} ({{ESEC}}/{{FSE}}'22)},
  author = {Chakraborty, Saikat and Ahmed, Toufique and Ding, Yangruibo and Devanbu, Premkumar and Ray, Baishakhi},
  date = {2022},
  keywords = {notion}
}

@inproceedings{chaMakingSymbolicExecution2020,
  title = {Making {{Symbolic Execution Promising}} by {{Learning Aggressive State-Pruning Strategy}}},
  booktitle = {Proceedings of the 28th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Cha, Sooyoung and Oh, Hakjoo},
  date = {2020},
  series = {{{ESEC}}/{{FSE}} 2020},
  pages = {147--158},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3368089.3409755},
  url = {https://doi.org/10.1145/3368089.3409755},
  abstract = {We present HOMI, a new technique to enhance symbolic execution by maintaining only a small number of promising states. In practice, symbolic execution typically maintains as many states as possible in a fear of losing important states. In this paper, however, we show that only a tiny subset of the states plays a significant role in increasing code coverage or reaching bug points. Based on this observation, HOMI aims to minimize the total number of states while keeping “promising” states during symbolic execution. We identify promising states by a learning algorithm that continuously updates the probabilistic pruning strategy based on data accumulated during the testing process. Experimental results show that HOMI greatly increases code coverage and the ability to find bugs of KLEE on open-source C programs.},
  isbn = {978-1-4503-7043-1},
  venue = {Virtual Event, USA},
  keywords = {Dynamic Symbolic Execution,notion,Online Learning}
}

@article{chandraImprovedTailBounds2019,
  title = {Improved {{Tail Bounds}} for {{Missing Mass}} and {{Confidence Intervals}} for {{Good-Turing Estimator}}},
  author = {Chandra, Prafulla and Pradeep, Aditya and Thangaraj, Andrew},
  date = {2019},
  journaltitle = {2019 National Conference on Communications (NCC)},
  pages = {1--6},
  url = {https://api.semanticscholar.org/CorpusID:174819238},
  keywords = {notion}
}

@article{chaoCoveragebasedRarefactionExtrapolation2012,
  title = {Coverage-Based Rarefaction and Extrapolation: Standardizing Samples by Completeness Rather than Size.},
  author = {Chao, Anne and Jost, Lou},
  date = {2012},
  journaltitle = {Ecology},
  volume = {93 12},
  pages = {2533--47},
  keywords = {notion}
}

@article{chaoDecipheringEnigmaUndetected2017,
  title = {Deciphering the Enigma of Undetected Species, Phylogenetic, and Functional Diversity Based on {{Good-Turing}} Theory},
  author = {Chao, Anne and Chiu, Chun-Huo and Colwell, Robert K. and Magnago, Luiz Fernando S. and Chazdon, Robin L. and Gotelli, Nicholas J.},
  date = {2017},
  journaltitle = {Ecology},
  volume = {98},
  number = {11},
  pages = {2914--2929},
  doi = {10.1002/ecy.2000},
  url = {https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/ecy.2000},
  abstract = {Abstract Estimating the species, phylogenetic, and functional diversity of a community is challenging because rare species are often undetected, even with intensive sampling. The Good-Turing frequency formula, originally developed for cryptography, estimates in an ecological context the true frequencies of rare species in a single assemblage based on an incomplete sample of individuals. Until now, this formula has never been used to estimate undetected species, phylogenetic, and functional diversity. Here, we first generalize the Good-Turing formula to incomplete sampling of two assemblages. The original formula and its two-assemblage generalization provide a novel and unified approach to notation, terminology, and estimation of undetected biological diversity. For species richness, the Good-Turing framework offers an intuitive way to derive the non-parametric estimators of the undetected species richness in a single assemblage, and of the undetected species shared between two assemblages. For phylogenetic diversity, the unified approach leads to an estimator of the undetected Faith's phylogenetic diversity (PD, the total length of undetected branches of a phylogenetic tree connecting all species), as well as a new estimator of undetected PD shared between two phylogenetic trees. For functional diversity based on species traits, the unified approach yields a new estimator of undetected Walker et al.'s functional attribute diversity (FAD, the total species-pairwise functional distance) in a single assemblage, as well as a new estimator of undetected FAD shared between two assemblages. Although some of the resulting estimators have been previously published (but derived with traditional mathematical inequalities), all taxonomic, phylogenetic, and functional diversity estimators are now derived under the same framework. All the derived estimators are theoretically lower bounds of the corresponding undetected diversities; our approach reveals the sufficient conditions under which the estimators are nearly unbiased, thus offering new insights. Simulation results are reported to numerically verify the performance of the derived estimators. We illustrate all estimators and assess their sampling uncertainty with an empirical dataset for Brazilian rain forest trees. These estimators should be widely applicable to many current problems in ecology, such as the effects of climate change on spatial and temporal beta diversity and the contribution of trait diversity to ecosystem multi-functionality.},
  keywords = {functional attribute diversity,functional diversity,notion,phylogenetic diversity,shared diversity,species diversity,taxonomic diversity}
}

@article{chaoEntropySpeciesAccumulation2013,
  title = {Entropy and the Species Accumulation Curve: A Novel Entropy Estimator via Discovery Rates of New Species},
  author = {Chao, Anne and Wang, Y. T. and Jost, Lou},
  date = {2013},
  journaltitle = {Methods in Ecology and Evolution},
  volume = {4},
  number = {11},
  pages = {1091--1100},
  doi = {10.1111/2041-210X.12108},
  url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12108},
  abstract = {Summary Estimating Shannon entropy and its exponential from incomplete samples is a central objective of many research fields. However, empirical estimates of Shannon entropy and its exponential depend strongly on sample size and typically exhibit substantial bias. This work uses a novel method to obtain an accurate, low-bias analytic estimator of entropy, based on species frequency counts. Our estimator does not require prior knowledge of the number of species. We show that there is a close relationship between Shannon entropy and the species accumulation curve, which depicts the cumulative number of observed species as a function of sample size. We reformulate entropy in terms of the expected discovery rates of new species with respect to sample size, that is, the successive slopes of the species accumulation curve. Our estimator is obtained by applying slope estimators derived from an improved Good-Turing frequency formula. Our method is also applied to estimate mutual information. Extensive simulations from theoretical models and real surveys show that if sample size is not unreasonably small, the resulting entropy estimator is nearly unbiased. Our estimator generally outperforms previous methods in terms of bias and accuracy (low mean squared error) especially when species richness is large and there is a large fraction of undetected species in samples. We discuss the extension of our approach to estimate Shannon entropy for multiple incidence data. The use of our estimator in constructing an integrated rarefaction and extrapolation curve of entropy (or mutual information) as a function of sample size or sample coverage (an aspect of sample completeness) is also discussed.},
  keywords = {diversity,Good-Turing frequency formula,mutual information,notion,sample coverage,Shannon entropy,species accumulation curve,species discovery rate}
}

@article{chaoEstimatingNumberClasses1992,
  title = {Estimating the {{Number}} of {{Classes}} via {{Sample Coverage}}},
  author = {Chao, Anne and Lee, Shen-Ming},
  date = {1992},
  journaltitle = {Journal of the American Statistical Association},
  volume = {87},
  number = {417},
  pages = {210--217},
  publisher = {Taylor \& Francis},
  doi = {10.1080/01621459.1992.10475194},
  url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1992.10475194},
  keywords = {notion}
}

@article{chaoEstimatingPopulationSize1987,
  title = {Estimating the Population Size for Capture-Recapture Data with Unequal Catchability.},
  author = {Chao, Anne},
  date = {1987},
  journaltitle = {Biometrics},
  volume = {43 4},
  pages = {783--91},
  keywords = {notion}
}

@article{chaoNonparametricEstimationComparison2016,
  title = {Nonparametric Estimation and Comparison of Species Richness},
  author = {Chao, Anne and Chiu, Chun-Huo},
  date = {2016},
  journaltitle = {Els},
  pages = {1--11},
  publisher = {Wiley Online Library},
  keywords = {_species reachness,notion}
}

@article{chaoNonparametricEstimationNumber1984,
  title = {Nonparametric Estimation of the Number of Classes in a Population},
  author = {Chao, Anne},
  date = {1984},
  journaltitle = {Scandinavian Journal of statistics},
  pages = {265--270},
  publisher = {JSTOR},
  keywords = {notion}
}

@article{chaoNonparametricPredictionSpecies2004,
  title = {Nonparametric Prediction in Species Sampling},
  author = {Chao, Anne and Shen, Tsung-Jen},
  date = {2004},
  journaltitle = {Journal of agricultural, biological, and environmental statistics},
  volume = {9},
  pages = {253--269},
  publisher = {Springer},
  keywords = {notion}
}

@article{chaoRarefactionExtrapolationHill2014,
  title = {Rarefaction and Extrapolation with {{Hill}} Numbers: A Framework for Sampling and Estimation in Species Diversity Studies},
  author = {Chao, Anne and Gotelli, Nicholas J and family=Hsieh, given=TC, given-i=TC and Sander, Elizabeth L and family=Ma, given=KH, given-i=KH and Colwell, Robert K and Ellison, Aaron M},
  date = {2014},
  journaltitle = {Ecological monographs},
  volume = {84},
  number = {1},
  pages = {45--67},
  publisher = {Wiley Online Library},
  keywords = {notion}
}

@article{chaoRarefactionExtrapolationPhylogenetic2015,
  title = {Rarefaction and Extrapolation of Phylogenetic Diversity},
  author = {Chao, Anne and Chiu, Chun-Huo and Hsieh, T. C. and Davis, Thomas and Nipperess, David A. and Faith, Daniel P.},
  date = {2015},
  journaltitle = {Methods in Ecology and Evolution},
  volume = {6},
  number = {4},
  pages = {380--388},
  doi = {10.1111/2041-210X.12247},
  url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12247},
  abstract = {Summary Traditional species diversity measures do not make distinctions among species. Faith's phylogenetic diversity (PD), which is defined as the sum of the branch lengths of a phylogenetic tree connecting all species, takes into account phylogenetic differences among species and has found many applications in various research fields. In this paper, we extend Faith's PD to represent the total length of a phylogenetic tree from any fixed point on its main trunk. Like species richness, Faith's PD tends to be an increasing function of sampling effort and thus tends to increase with sample completeness. We develop in this paper the `PD accumulation curve' (an extension of the species accumulation curve) to depict how PD increases with sampling size and sample completeness. To make fair comparisons of Faith's PD among several assemblages based on sampling data from each assemblage, we derive both theoretical formulae and analytic estimators for seamless rarefaction (interpolation) and extrapolation (prediction). We develop a lower bound of the undetected PD for an incomplete sample to guide the extrapolation; the PD estimator for an extrapolated sample is generally reliable up to twice the size of the empirical sample. We propose an integrated curve that smoothly links rarefaction and extrapolation to standardize samples on the basis of sample size or sample completeness. A bootstrap method is used to obtain the unconditional variances of PD estimators and to construct the confidence interval of the expected PD for a fixed sample size or fixed degree of sample completeness. This facilitates comparison of multiple assemblages of both rarefied and extrapolated samples. We illustrate our formulae and estimators using empirical data sets from Australian birds in two sites. We discuss the extension of our approach to the case of multiple incidence data and to incorporate species abundances.},
  keywords = {diversity,extrapolation,notion,phylogenetic diversity,rarefaction,sample coverage,species richness,undetected phylogenetic diversity}
}

@article{chaoSpeciesEstimationApplications2005,
  title = {Species Estimation and Applications},
  author = {Chao, Anne},
  date = {2005},
  journaltitle = {Encyclopedia of statistical sciences},
  volume = {12},
  pages = {7907--7916},
  publisher = {Wiley},
  keywords = {notion}
}

@article{chaoThirtyYearsProgeny2017,
  title = {Thirty Years of Progeny from {{Chao}}'s Inequality: {{Estimating}} and Comparing Richness with Incidence Data and Incomplete Sampling},
  author = {Chao, Anne and Colwell, Robert K},
  date = {2017},
  journaltitle = {SORT-Statistics and Operations Research Transactions},
  pages = {3--54},
  keywords = {notion}
}

@article{chaoUnveilingSpeciesrankAbundance2015,
  title = {Unveiling the Species-Rank Abundance Distribution by Generalizing the {{Good-Turing}} Sample Coverage Theory.},
  author = {Chao, Anne and Hsieh, T. C. and Chazdon, Robin L. and Colwell, Robert K. and Gotelli, Nicholas J.},
  date = {2015},
  journaltitle = {Ecology},
  volume = {96 5},
  pages = {1189--201},
  keywords = {notion}
}

@inproceedings{chaparroDetectingMissingInformation2017,
  title = {Detecting Missing Information in Bug Descriptions},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Chaparro, Oscar and Lu, Jing and Zampetti, Fiorella and Moreno, Laura and Di Penta, Massimiliano and Marcus, Andrian and Bavota, Gabriele and Ng, Vincent},
  date = {2017},
  pages = {396--407},
  publisher = {ACM},
  keywords = {Automated Discourse Identification,Bug Description Discourse,Bug Report,notion}
}

@inproceedings{chaProgramAdaptiveMutationalFuzzing2015,
  title = {Program-{{Adaptive Mutational Fuzzing}}},
  booktitle = {2015 {{IEEE Symposium}} on {{Security}} and {{Privacy}}},
  author = {Cha, Sang Kil and Woo, Maverick and Brumley, David},
  date = {2015},
  pages = {725--741},
  doi = {10.1109/SP.2015.50},
  keywords = {notion}
}

@article{chattopadhyayQuantifyingInformationLeakage2019,
  title = {Quantifying the {{Information Leakage}} in {{Cache Attacks}} via {{Symbolic Execution}}},
  author = {Chattopadhyay, Sudipta and Beck, Moritz and Rezine, Ahmed and Zeller, Andreas},
  date = {2019},
  journaltitle = {ACM Transactions on Embedded Computing Systems (TECS)},
  volume = {18},
  pages = {1--27},
  url = {https://api.semanticscholar.org/CorpusID:57757486},
  keywords = {notion}
}

@inproceedings{chatzikokolakisStatisticalMeasurementInformation2010,
  title = {Statistical {{Measurement}} of {{Information Leakage}}},
  booktitle = {Tools and {{Algorithms}} for the {{Construction}} and {{Analysis}} of {{Systems}}},
  author = {Chatzikokolakis, Konstantinos and Chothia, Tom and Guha, Apratim},
  editor = {Esparza, Javier and Majumdar, Rupak},
  date = {2010},
  pages = {390--404},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  abstract = {Information theory provides a range of useful methods to analyse probability distributions and these techniques have been successfully applied to measure information flow and the loss of anonymity in secure systems. However, previous work has tended to assume that the exact probabilities of every action are known, or that the system is non-deterministic. In this paper, we show that measures of information leakage based on mutual information and capacity can be calculated, automatically, from trial runs of a system alone. We find a confidence interval for this estimate based on the number of possible inputs, observations and samples. We have developed a tool to automatically perform this analysis and we demonstrate our method by analysing a Mixminon anonymous remailer node.},
  isbn = {978-3-642-12002-2},
  keywords = {_information leakage,notion}
}

@article{chaumDiningCryptographersProblem1988,
  title = {The Dining Cryptographers Problem: {{Unconditional}} Sender and Recipient Untraceability},
  author = {Chaum, David},
  date = {1988},
  journaltitle = {Journal of cryptology},
  volume = {1},
  pages = {65--75},
  publisher = {Springer},
  keywords = {notion}
}

@inproceedings{chenAngoraEfficientFuzzing2018,
  title = {Angora: {{Efficient Fuzzing}} by {{Principled Search}}},
  booktitle = {2018 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  author = {Chen, Peng and Chen, Hao},
  date = {2018},
  pages = {711--725},
  doi = {10.1109/SP.2018.00046},
  keywords = {Computer bugs,coverage based fuzzing,Fuzzing,Instruments,Measurement,notion,Shape,Software,Space exploration,taint analysis,vulnrability detection}
}

@inproceedings{chenAutomatedApproachEstimating2018,
  title = {An Automated Approach to Estimating Code Coverage Measures via Execution Logs},
  booktitle = {Proceedings of the 33rd {{ACM}}/{{IEEE International Conference}} on {{Automated Software Engineering}}},
  author = {Chen, Boyuan and Song, Jian and Xu, Peng and Hu, Xing and Jiang, Zhen Ming},
  date = {2018},
  pages = {305--316},
  keywords = {notion}
}

@inproceedings{chenCounterFactualTypingDebugging2014,
  title = {Counter-{{Factual Typing}} for {{Debugging Type Errors}}},
  booktitle = {Proceedings of the 41st {{ACM SIGPLAN-SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Chen, Sheng and Erwig, Martin},
  date = {2014},
  series = {{{POPL}} '14},
  pages = {583--594},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2535838.2535863},
  url = {https://doi.org/10.1145/2535838.2535863},
  isbn = {978-1-4503-2544-8},
  venue = {San Diego, California, USA},
  keywords = {change suggestions,choice types,error localization,notion,type error messages,type inference,type-error debugging}
}

@article{chenFairnessTestingComprehensive2022,
  title = {Fairness {{Testing}}: {{A Comprehensive Survey}} and {{Analysis}} of {{Trends}}},
  author = {Chen, Zhenpeng and Zhang, J. and Hort, Max and Sarro, Federica and Harman, Mark},
  date = {2022},
  journaltitle = {ArXiv},
  volume = {abs/2207.10223},
  keywords = {notion}
}

@inproceedings{chengRevisitingTestCasePrioritization2024,
  title = {Revisiting {{Test-Case Prioritization}} on {{Long-Running Test Suites}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Cheng, Runxiang and Wang, Shuai and Jabbarvand, Reyhaneh and Marinov, Darko},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {615--627},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3680307},
  url = {https://dl.acm.org/doi/10.1145/3650212.3680307},
  urldate = {2024-09-21},
  abstract = {The prolonged continuous integration (CI) runs are affecting timely feedback to software developers.   Test-case prioritization (TCP) aims to expose faults sooner by reordering tests such that the ones more likely to fail are run earlier.   TCP is thus especially important for long-running test suites.   While many studies have explored TCP, they are based on outdated CI builds from over 10 years ago with test suites that last several minutes, or builds from inaccessible, proprietary projects.   In this paper, we present LRTS, the first dataset of long-running test suites, with 21,255 CI builds and 57,437 test-suite runs from 10 large-scale, open-source projects that use Jenkins CI.   LRTS spans from 2020 to 2023, with an average test-suite run duration of 6.5 hours.   On LRTS, we study the effectiveness of 59 leading TCP techniques, the impact of confounding test failures on TCP, and TCP for failing tests with no prior failures.   We revisit prior key findings (9 confirmed, 2 refuted) and establish 3 new findings.   Our results show that prioritizing faster tests that recently failed performs the best, outperforming the sophisticated techniques.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/XQ6C6NL2/Cheng et al. - 2024 - Revisiting Test-Case Prioritization on Long-Running Test Suites.pdf}
}

@unpublished{chenHOPPERInterpretativeFuzzing2023,
  title = {{{HOPPER}}: {{Interpretative Fuzzing}} for {{Libraries}}},
  author = {Chen, Peng and Xie, Yuxuan and Lyu, Yunlong and Wang, Yuxiao and Chen, Hao},
  date = {2023},
  eprint = {2309.03496},
  eprinttype = {arXiv},
  keywords = {notion}
}

@inproceedings{chenIsolationBasedDebuggingNeural2024,
  title = {Isolation-{{Based Debugging}} for {{Neural Networks}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Chen, Jialuo and Wang, Jingyi and Sun, Youcheng and Cheng, Peng and Chen, Jiming},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {338--349},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3652132},
  url = {https://dl.acm.org/doi/10.1145/3650212.3652132},
  urldate = {2024-09-21},
  abstract = {Neural networks (NNs) are known to have diverse defects such as adversarial examples, backdoor and discrimination, raising great concerns about their reliability. While NN testing can effectively expose these defects to a significant degree, understanding their root causes within the network requires further examination. In this work, inspired by the idea of debugging in traditional software for failure isolation, we propose a novel unified neuron-isolation-based framework for debugging neural networks, shortly IDNN. Given a buggy NN that exhibits certain undesired properties (e.g., discrimination), the goal of IDNN is to identify the most critical and minimal set of neurons that are responsible for exhibiting these properties. Notably, such isolation is conducted with the objective that by simply ‘freezing’ these neurons, the model’s undesired properties can be eliminated, resulting in a much more efficient model repair compared to computationally expensive retraining or weight optimization as in existing literature. We conduct extensive experiments to evaluate IDNN across a diverse set of NN structures on five benchmark datasets, for solving three debugging tasks, including backdoor, unfairness, and weak class. As a lightweight framework, IDNN outperforms state-of-the-art baselines by successfully identifying and isolating a very small set of responsible neurons, demonstrating superior generalization performance across all tasks.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/LRMZ9BP2/Chen et al. - 2024 - Isolation-Based Debugging for Neural Networks.pdf}
}

@inproceedings{chenJIGSAWEfficientScalable2022,
  title = {{{JIGSAW}}: {{Efficient}} and {{Scalable Path Constraints Fuzzing}}},
  booktitle = {2022 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  author = {Chen, Ju and Wang, Jinghan and Song, Chengyu and Yin, Heng},
  date = {2022},
  pages = {18--35},
  doi = {10.1109/SP46214.2022.9833796},
  keywords = {notion}
}

@misc{chenLatentExecutionNeural2021,
  title = {Latent {{Execution}} for {{Neural Program Synthesis Beyond Domain-Specific Languages}}},
  author = {Chen, Xinyun and Song, Dawn and Tian, Yuandong},
  date = {2021},
  keywords = {notion}
}

@inproceedings{chenMAATNovelEnsemble2022,
  title = {{{MAAT}}: {{A Novel Ensemble Approach}} to {{Addressing Fairness}} and {{Performance Bugs}} for {{Machine Learning Software}}},
  booktitle = {Proceedings of the 30th {{ACM Joint European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}} ({{ESEC}}/{{FSE}}'22)},
  author = {Chen, Zhenpeng and Zhang, Jie M and Sarro, Federica and Harman, Mark},
  date = {2022},
  publisher = {ACM Press},
  keywords = {fairness,notion}
}

@article{chenProgramRepairRepeated2022,
  title = {Program {{Repair}} with {{Repeated Learning}}},
  author = {Chen, Liushan and Pei, Yu and Pan, Minxue and Zhang, Tian and Wang, Qixin and Furia, Carlo Alberto},
  date = {2022},
  journaltitle = {IEEE Transactions on Software Engineering},
  pages = {1--1},
  doi = {10.1109/TSE.2022.3164662},
  keywords = {notion}
}

@misc{chenRemarkableRoleSimilarity2018,
  title = {The {{Remarkable Role}} of {{Similarity}} in {{Redundancy-based Program Repair}}},
  author = {Chen, Zimin and Monperrus, Martin},
  date = {2018},
  keywords = {notion}
}

@article{chenSamplingBaselineOptimizer2019,
  title = {“{{Sampling}}” as a {{Baseline Optimizer}} for {{Search-Based Software Engineering}}},
  author = {Chen, J. and Nair, V. and Krishna, R. and Menzies, T.},
  date = {2019},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {45},
  number = {6},
  pages = {597--614},
  doi = {10.1109/TSE.2018.2790925},
  keywords = {notion}
}

@article{chenSEQUENCERSequenceSequenceLearning2019,
  title = {{{SEQUENCER}}: {{Sequence-to-Sequence Learning}} for {{End-to-End Program Repair}}},
  author = {Chen, Z. and Kommrusch, S. J. and Tufano, M. and Pouchet, L. and Poshyvanyk, D. and Monperrus, M.},
  date = {2019},
  journaltitle = {IEEE Transactions on Software Engineering},
  pages = {1--1},
  doi = {10.1109/TSE.2019.2940179},
  keywords = {notion}
}

@article{chenSlicingObjectorientedJava2001,
  title = {Slicing {{Object-oriented Java Programs}}},
  author = {Chen, Zhenqiang and Xu, Baowen},
  date = {2001-04},
  journaltitle = {SIGPLAN Not.},
  volume = {36},
  number = {4},
  pages = {33--40},
  publisher = {ACM},
  location = {New York, NY, USA},
  issn = {0362-1340},
  doi = {10.1145/375431.375418},
  url = {http://doi.acm.org/10.1145/375431.375418},
  keywords = {Java,notion,Program Slicing}
}

@article{chenSurveySoftwareLog2021,
  title = {A {{Survey}} of {{Software Log Instrumentation}}},
  author = {Chen, Boyuan and Jiang, Zhen Ming (Jack)},
  date = {2021-05},
  journaltitle = {ACM Comput. Surv.},
  volume = {54},
  number = {4},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {0360-0300},
  doi = {10.1145/3448976},
  url = {https://doi.org/10.1145/3448976},
  abstract = {Log messages have been used widely in many software systems for a variety of purposes during software development and field operation. There are two phases in software logging: log instrumentation and log management. Log instrumentation refers to the practice that developers insert logging code into source code to record runtime information. Log management refers to the practice that operators collect the generated log messages and conduct data analysis techniques to provide valuable insights of runtime behavior. There are many open source and commercial log management tools available. However, their effectiveness highly depends on the quality of the instrumented logging code, as log messages generated by high-quality logging code can greatly ease the process of various log analysis tasks (e.g., monitoring, failure diagnosis, and auditing). Hence, in this article, we conducted a systematic survey on state-of-the-art research on log instrumentation by studying 69 papers between 1997 and 2019. In particular, we have focused on the challenges and proposed solutions used in the three steps of log instrumentation: (1) logging approach; (2) logging utility integration; and (3) logging code composition. This survey will be useful to DevOps practitioners and researchers who are interested in software logging.},
  keywords = {instrumentation,notion,software logging,Systematic survey}
}

@inproceedings{chenTestAdequacyCriterion2010,
  title = {Test Adequacy Criterion Based on Coincidental Correctness Probability},
  booktitle = {Proceedings of the {{Second Asia-Pacific Symposium}} on {{Internetware}}},
  author = {Chen, Jie and Li, Qian and Zhao, Jianhua and Li, Xuandong},
  date = {2010},
  pages = {1--4},
  keywords = {notion}
}

@inproceedings{cherubinFBLEAUFastBlackBox2019,
  title = {F-{{BLEAU}}: {{Fast Black-Box Leakage Estimation}}},
  booktitle = {2019 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  author = {Cherubin, Giovanni and Chatzikokolakis, Konstantinos and Palamidessi, Catuscia},
  date = {2019},
  pages = {835--852},
  doi = {10.1109/SP.2019.00073},
  keywords = {Convergence,estimation,Frequency measurement,leakage,machine-learning,Maximum likelihood estimation,notion,privacy,Privacy,quantitative-information-flow,security-bounds,side-channels,Tools}
}

@article{choBLITZCompositionalBounded2013,
  title = {{{BLITZ}}: {{Compositional}} Bounded Model Checking for Real-World Programs},
  author = {Cho, Chia Yuan and D'Silva, Vijay Victor and Song, Dawn Xiaodong},
  date = {2013},
  journaltitle = {2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  pages = {136--146},
  url = {https://api.semanticscholar.org/CorpusID:158067},
  keywords = {notion}
}

@inproceedings{choFriendshipMobilityUser2011,
  title = {Friendship and Mobility: User Movement in Location-Based Social Networks},
  booktitle = {Proceedings of the 17th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Cho, Eunjoon and Myers, Seth A. and Leskovec, Jure},
  date = {2011},
  series = {{{KDD}} '11},
  pages = {1082--1090},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2020408.2020579},
  url = {https://doi.org/10.1145/2020408.2020579},
  abstract = {Even though human movement and mobility patterns have a high degree of freedom and variation, they also exhibit structural patterns due to geographic and social constraints. Using cell phone location data, as well as data from two online location-based social networks, we aim to understand what basic laws govern human motion and dynamics. We find that humans experience a combination of periodic movement that is geographically limited and seemingly random jumps correlated with their social networks. Short-ranged travel is periodic both spatially and temporally and not effected by the social network structure, while long-distance travel is more influenced by social network ties. We show that social relationships can explain about 10\% to 30\% of all human movement, while periodic behavior explains 50\% to 70\%. Based on our findings, we develop a model of human mobility that combines periodic short range movements with travel due to the social network structure. We show that our model reliably predicts the locations and dynamics of future human movement and gives an order of magnitude better performance than present models of human mobility.},
  isbn = {978-1-4503-0813-7},
  venue = {San Diego, California, USA},
  keywords = {communication networks,human mobility,notion,social networks}
}

@inproceedings{choiDocumentlevelSentimentInference2016,
  title = {Document-Level {{Sentiment Inference}} with {{Social}}, {{Faction}}, and {{Discourse Context}}},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Choi, Eunsol and Rashkin, Hannah and Zettlemoyer, Luke and Choi, Yejin},
  date = {2016-08},
  pages = {333--343},
  publisher = {Association for Computational Linguistics},
  location = {Berlin, Germany},
  doi = {10.18653/v1/P16-1032},
  url = {https://www.aclweb.org/anthology/P16-1032},
  keywords = {notion}
}

@article{choiEndEndPredictionBuffer2017,
  title = {End-to-{{End Prediction}} of {{Buffer Overruns}} from {{Raw Source Code}} via {{Neural Memory Networks}}},
  author = {Choi, Min-je and Jeong, Sehun and Oh, Hakjoo and Choo, Jaegul},
  date = {2017-03},
  url = {https://arxiv.org/abs/1703.02458},
  abstract = {Detecting buffer overruns from a source code is one of the most common and yet challenging tasks in program analysis. Current approaches have mainly relied on rigid rules and handcrafted features devised by a few experts, limiting themselves in terms of flexible applicability and robustness due to diverse bug patterns and characteristics existing in sophisticated real-world software programs. In this paper, we propose a novel, data-driven approach that is completely end-to-end without requiring any hand-crafted features, thus free from any program language-specific structural limitations. In particular, our approach leverages a recently proposed neural network model called memory networks that have shown the state-of-the-art performances mainly in question-answering tasks. Our experimental results using source codes demonstrate that our proposed model is capable of accurately detecting simple buffer overruns. We also present in-depth analyses on how a memory network can learn to understand the semantics in programming languages solely from raw source codes, such as tracing variables of interest, identifying numerical values, and performing their quantitative comparisons.},
  keywords = {Buffer Overrun,Neural Network (NN),notion}
}

@inproceedings{choiGreyBoxConcolicTesting2019,
  title = {Grey-{{Box Concolic Testing}} on {{Binary Code}}},
  booktitle = {2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Choi, Jaeseung and Jang, Joonun and Han, Choongwoo and Cha, Sang Kil},
  date = {2019},
  pages = {736--747},
  doi = {10.1109/ICSE.2019.00082},
  keywords = {notion}
}

@inproceedings{chothiaLeakwatchEstimatingInformation2014,
  title = {Leakwatch: {{Estimating}} Information Leakage from Java Programs},
  booktitle = {Computer {{Security-ESORICS}} 2014: 19th {{European Symposium}} on {{Research}} in {{Computer Security}}, {{Wroclaw}}, {{Poland}}, {{September}} 7-11, 2014. {{Proceedings}}, {{Part II}} 19},
  author = {Chothia, Tom and Kawamoto, Yusuke and Novakovic, Chris},
  date = {2014},
  pages = {219--236},
  publisher = {Springer},
  keywords = {notion}
}

@inproceedings{chothiaProbabilisticPointPointInformation2013,
  title = {Probabilistic {{Point-to-Point Information Leakage}}},
  booktitle = {2013 {{IEEE}} 26th {{Computer Security Foundations Symposium}}},
  author = {Chothia, Tom and Kawamoto, Yusuke and Novakovic, Chris and Parker, David},
  date = {2013},
  pages = {193--205},
  doi = {10.1109/CSF.2013.20},
  keywords = {notion}
}

@inproceedings{chothiaStatisticalTestInformation2011,
  title = {A {{Statistical Test}} for {{Information Leaks Using Continuous Mutual Information}}},
  booktitle = {2011 {{IEEE}} 24th {{Computer Security Foundations Symposium}}},
  author = {Chothia, Tom and Guha, Apratim},
  date = {2011},
  pages = {177--190},
  doi = {10.1109/CSF.2011.19},
  keywords = {Computer security,Continuous Mutual Information,e-Passport,Information leakage,Information Theory,notion,Statistics}
}

@inproceedings{chothiaToolEstimatingInformation2013,
  title = {A Tool for Estimating Information Leakage},
  booktitle = {Computer {{Aided Verification}}: 25th {{International Conference}}, {{CAV}} 2013, {{Saint Petersburg}}, {{Russia}}, {{July}} 13-19, 2013. {{Proceedings}} 25},
  author = {Chothia, Tom and Kawamoto, Yusuke and Novakovic, Chris},
  date = {2013},
  pages = {690--695},
  publisher = {Springer},
  keywords = {notion}
}

@inproceedings{chothiaTraceabilityAttackEPassports2010,
  title = {A {{Traceability Attack}} against E-{{Passports}}},
  booktitle = {Financial {{Cryptography}} and {{Data Security}}},
  author = {Chothia, Tom and Smirnov, Vitaliy},
  editor = {Sion, Radu},
  date = {2010},
  pages = {20--34},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  abstract = {Since 2004, many nations have started issuing “e-passports” containing an RFID tag that, when powered, broadcasts information. It is claimed that these passports are more secure and that our data will be protected from any possible unauthorised attempts to read it. In this paper we show that there is a flaw in one of the passport's protocols that makes it possible to trace the movements of a particular passport, without having to break the passport's cryptographic key. All an attacker has to do is to record one session between the passport and a legitimate reader, then by replaying a particular message, the attacker can distinguish that passport from any other. We have implemented our attack and tested it successfully against passports issued by a range of nations.},
  isbn = {978-3-642-14577-3},
  keywords = {notion}
}

@inproceedings{christakisWhatDevelopersWant2016,
  title = {What {{Developers Want}} and {{Need}} from {{Program Analysis}}: {{An Empirical Study}}},
  booktitle = {Proceedings of the 31st {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Christakis, Maria and Bird, Christian},
  date = {2016},
  series = {{{ASE}} 2016},
  pages = {332--343},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2970276.2970347},
  url = {https://doi.org/10.1145/2970276.2970347},
  abstract = {Program Analysis has been a rich and fruitful field of research for many decades, and countless high quality program analysis tools have been produced by academia. Though there are some well-known examples of tools that have found their way into routine use by practitioners, a common challenge faced by researchers is knowing how to achieve broad and lasting adoption of their tools. In an effort to understand what makes a program analyzer most attractive to developers, we mounted a multi-method investigation at Microsoft. Through interviews and surveys of developers as well as analysis of defect data, we provide insight and answers to four high level research questions that can help researchers design program analyzers meeting the needs of software developers. First, we explore what barriers hinder the adoption of program analyzers, like poorly expressed warning messages. Second, we shed light on what functionality developers want from analyzers, including the types of code issues that developers care about. Next, we answer what non-functional characteristics an analyzer should have to be widely used, how the analyzer should fit into the development process, and how its results should be reported. Finally, we investigate defects in one of Microsoft's flagship software services, to understand what types of code issues are most important to minimize, potentially through program analysis.},
  isbn = {978-1-4503-3845-5},
  venue = {Singapore, Singapore},
  keywords = {code defects,notion,program analysis}
}

@inproceedings{chuaNeuralNetsCan2017,
  title = {Neural {{Nets Can Learn Function Type Signatures From Binaries}}},
  booktitle = {26th {{USENIX Security Symposium}} ({{USENIX Security}} 17)},
  author = {Chua, Zheng Leong and Shen, Shiqi and Saxena, Prateek and Liang, Zhenkai},
  date = {2017},
  pages = {99--116},
  publisher = {USENIX Association},
  location = {Vancouver, BC},
  url = {https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/chua},
  isbn = {978-1-931971-40-9},
  keywords = {notion}
}

@inproceedings{chuGraphNeuralNetworks2024,
  title = {Graph {{Neural Networks}} for {{Vulnerability Detection}}: {{A Counterfactual Explanation}}},
  shorttitle = {Graph {{Neural Networks}} for {{Vulnerability Detection}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Chu, Zhaoyang and Wan, Yao and Li, Qian and Wu, Yang and Zhang, Hongyu and Sui, Yulei and Xu, Guandong and Jin, Hai},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {389--401},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3652136},
  url = {https://dl.acm.org/doi/10.1145/3650212.3652136},
  urldate = {2024-09-21},
  abstract = {Vulnerability detection is crucial for ensuring the security and reliability of software systems. Recently, Graph Neural Networks (GNNs) have emerged as a prominent code embedding approach for vulnerability detection, owing to their ability to capture the underlying semantic structure of source code. However, GNNs face significant challenges in explainability due to their inherently black-box nature. To this end, several factual reasoning-based explainers have been proposed. These explainers provide explanations for the predictions made by GNNs by analyzing the key features that contribute to the outcomes. We argue that these factual reasoning-based explanations cannot answer critical what-if questions: "What would happen to the GNN's decision if we were to alter the code graph into alternative structures?" Inspired by advancements of counterfactual reasoning in artificial intelligence, we propose CFExplainer, a novel counterfactual explainer for GNN-based vulnerability detection. Unlike factual reasoning-based explainers, CFExplainer seeks the minimal perturbation to the input code graph that leads to a change in the prediction, thereby addressing the what-if questions for vulnerability detection. We term this perturbation a counterfactual explanation, which can pinpoint the root causes of the detected vulnerability and furnish valuable insights for developers to undertake appropriate actions for fixing the vulnerability. Extensive experiments on four GNN-based vulnerability detection models demonstrate the effectiveness of CFExplainer over existing state-of-the-art factual reasoning-based explainers.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/V7LVL49R/Chu et al. - 2024 - Graph Neural Networks for Vulnerability Detection A Counterfactual Explanation.pdf}
}

@book{chungElementaryProbabilityTheory2006,
  title = {Elementary Probability Theory: With Stochastic Processes and an Introduction to Mathematical Finance},
  author = {Chung, Kai Lai and AitSahlia, Farid},
  date = {2006},
  publisher = {Springer Science \& Business Media},
  keywords = {notion}
}

@unpublished{chungSliceFinderAutomated2018,
  title = {Slice {{Finder}}: {{Automated Data Sclicing}} for {{Model Validation}}},
  author = {Chung, Yeounoh and Kraska, Tim and Polyzotis, Neoklis and Whang, Steven Euijong},
  date = {2018},
  eprint = {1807.06068},
  eprinttype = {arXiv},
  keywords = {notion}
}

@unpublished{citoCounterfactualExplanationsModels2021,
  title = {Counterfactual {{Explanations}} for {{Models}} of {{Code}}},
  author = {Cito, Jürgen and Dillig, Isil and Murali, Vijayaraghavan and Chandra, Satish},
  date = {2021},
  eprint = {2111.05711},
  eprinttype = {arXiv},
  keywords = {notion}
}

@inproceedings{clarkeCompositionalModelChecking1989,
  title = {Compositional Model Checking},
  booktitle = {[1989] {{Proceedings}}. {{Fourth Annual Symposium}} on {{Logic}} in {{Computer Science}}},
  author = {Clarke, E.M. and Long, D.E. and McMillan, K.L.},
  date = {1989},
  pages = {353--362},
  doi = {10.1109/LICS.1989.39190},
  keywords = {notion}
}

@inproceedings{clarkeStatisticalModelChecking2011,
  title = {Statistical Model Checking for Cyber-Physical Systems},
  booktitle = {Automated {{Technology}} for {{Verification}} and {{Analysis}}: 9th {{International Symposium}}, {{ATVA}} 2011, {{Taipei}}, {{Taiwan}}, {{October}} 11-14, 2011. {{Proceedings}} 9},
  author = {Clarke, Edmund M and Zuliani, Paolo},
  date = {2011},
  pages = {1--12},
  publisher = {Springer},
  keywords = {notion}
}

@inproceedings{clarkInformationTransformationUnderpinning2015,
  title = {Information Transformation: {{An}} Underpinning Theory for Software Engineering},
  booktitle = {2015 {{IEEE}}/{{ACM}} 37th {{IEEE International Conference}} on {{Software Engineering}}},
  author = {Clark, David and Feldt, Robert and Poulding, Simon and Yoo, Shin},
  date = {2015},
  volume = {2},
  pages = {599--602},
  publisher = {IEEE},
  keywords = {notion}
}

@inproceedings{clarkMetamorphicTestingCausal2023,
  title = {Metamorphic {{Testing}} with {{Causal Graphs}}},
  booktitle = {2023 {{IEEE Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}} ({{ICST}})},
  author = {Clark, Andrew G. and Foster, Michael and Walkinshaw, Neil and Hierons, Robert M.},
  date = {2023},
  pages = {153--164},
  doi = {10.1109/ICST57152.2023.00023},
  keywords = {notion}
}

@article{clarkNormalisedSqueezinessFailed2019,
  title = {Normalised {{Squeeziness}} and {{Failed Error Propagation}}},
  author = {Clark, David and Hierons, Robert M. and Patel, Krishna},
  date = {2019},
  journaltitle = {Information Processing Letters},
  volume = {149},
  pages = {6--9},
  issn = {0020-0190},
  doi = {10.1016/j.ipl.2019.04.001},
  url = {http://www.sciencedirect.com/science/article/pii/S0020019019300729},
  abstract = {Failed Error Propagation (FEP) can reduce test effectiveness and recent work proposed an information theoretic measure, Squeeziness, as the theoretical basis for avoiding FEP. This paper demonstrates that Squeeziness is not suitable for comparing programs with different input domains. We then extend Squeeziness to Normalised Squeeziness and demonstrate that this is more generally useful.},
  keywords = {Fault masking,Formal methods,notion,Software engineering,Software testing}
}

@article{clarksonHyperproperties2010,
  title = {Hyperproperties},
  author = {Clarkson, Michael R and Schneider, Fred B},
  date = {2010},
  journaltitle = {Journal of Computer Security},
  volume = {18},
  number = {6},
  pages = {1157--1210},
  publisher = {IOS Press},
  keywords = {notion}
}

@article{clarkSqueezinessInformationTheoretic2012,
  title = {Squeeziness: {{An}} Information Theoretic Measure for Avoiding Fault Masking},
  author = {Clark, David and Hierons, Robert M},
  date = {2012},
  journaltitle = {Information Processing Letters},
  volume = {112},
  number = {8-9},
  pages = {335--340},
  publisher = {Elsevier},
  keywords = {notion}
}

@misc{clarkTestingCausalityScientific2022,
  title = {Testing {{Causality}} in {{Scientific Modelling Software}}},
  author = {Clark, Andrew G. and Foster, Michael and Prifling, Benedikt and Walkinshaw, Neil and Hierons, Robert M. and Schmidt, Volker and Turner, Robert D.},
  date = {2022},
  doi = {10.48550/ARXIV.2209.00357},
  url = {https://arxiv.org/abs/2209.00357},
  organization = {arXiv},
  keywords = {D.2.5,FOS: Computer and information sciences,I.6.4,notion,Software Engineering (cs.SE)}
}

@article{clevelandRobustLocallyWeighted1979,
  title = {Robust Locally Weighted Regression and Smoothing Scatterplots},
  author = {Cleveland, William S},
  date = {1979},
  journaltitle = {Journal of the American statistical association},
  volume = {74},
  number = {368},
  pages = {829--836},
  publisher = {Taylor \& Francis},
  keywords = {notion}
}

@inproceedings{cleveLocatingCausesProgram2005,
  title = {Locating Causes of Program Failures},
  booktitle = {Proceedings. 27th {{International Conference}} on {{Software Engineering}}, 2005. {{ICSE}} 2005.},
  author = {Cleve, Holger and Zeller, Andreas},
  date = {2005},
  pages = {342--351},
  publisher = {IEEE},
  keywords = {notion}
}

@inproceedings{cody-kennySearchImprovedPerformance2017,
  title = {A {{Search}} for {{Improved Performance}} in {{Regular Expressions}}},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {Cody-Kenny, Brendan and Fenton, Michael and Ronayne, Adrian and Considine, Eoghan and McGuire, Thomas and O'Neill, Michael},
  date = {2017},
  series = {{{GECCO}} '17},
  pages = {1280--1287},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/3071178.3071196},
  url = {http://doi.acm.org/10.1145/3071178.3071196},
  isbn = {978-1-4503-4920-8},
  venue = {Berlin, Germany},
  keywords = {genetic programming,notion,performance,regular expressions}
}

@inproceedings{cohenFasterAlgorithmsComputing2016,
  title = {Faster {{Algorithms}} for {{Computing}} the {{Stationary Distribution}}, {{Simulating Random Walks}}, and {{More}}},
  booktitle = {2016 {{IEEE}} 57th {{Annual Symposium}} on {{Foundations}} of {{Computer Science}} ({{FOCS}})},
  author = {Cohen, Michael B. and Kelner, Jonathan and Peebles, John and Peng, Richard and Sidford, Aaron and Vladu, Adrian},
  date = {2016},
  pages = {583--592},
  doi = {10.1109/FOCS.2016.69},
  keywords = {notion}
}

@inproceedings{cohenHaveWeSeen2015,
  title = {Have {{We Seen Enough Traces}}?({{T}})},
  booktitle = {2015 30th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Cohen, Hila and Maoz, Shahar},
  date = {2015},
  pages = {93--103},
  publisher = {IEEE},
  keywords = {notion}
}

@inproceedings{collardSrcMLInfrastructureExploration2013,
  title = {{{srcML}}: {{An Infrastructure}} for the {{Exploration}}, {{Analysis}}, and {{Manipulation}} of {{Source Code}}: {{A Tool Demonstration}}},
  booktitle = {{{IEEE International Conference}} on {{Software Maintenance}}, {{ICSM}}},
  author = {Collard, Michael and Decker, Michael and Maletic, Jonathan},
  date = {2013-09},
  pages = {516--519},
  doi = {10.1109/ICSM.2013.85},
  keywords = {notion}
}

@article{colwellModelsEstimatorsLinking2012,
  title = {Models and Estimators Linking Individual-Based and Sample-Based Rarefaction, Extrapolation and Comparison of Assemblages},
  author = {Colwell, Robert K. and Chao, Anne and Gotelli, Nicholas J. and Lin, Shang-Yi and Mao, Chang Xuan and Chazdon, Robin L. and Longino, John T.},
  date = {2012-03},
  journaltitle = {Journal of Plant Ecology},
  volume = {5},
  number = {1},
  pages = {3--21},
  issn = {1752-9921},
  doi = {10.1093/jpe/rtr044},
  url = {https://doi.org/10.1093/jpe/rtr044},
  abstract = {In ecology and conservation biology, the number of species counted in a biodiversity study is a key metric but is usually a biased underestimate of total species richness because many rare species are not detected. Moreover, comparing species richness among sites or samples is a statistical challenge because the observed number of species is sensitive to the number of individuals counted or the area sampled. For individual-based data, we treat a single, empirical sample of species abundances from an investigator-defined species assemblage or community as a reference point for two estimation objectives under two sampling models: estimating the expected number of species (and its unconditional variance) in a random sample of (i) a smaller number of individuals (multinomial model) or a smaller area sampled (Poisson model) and (ii) a larger number of individuals or a larger area sampled. For sample-based incidence (presence–absence) data, under a Bernoulli product model, we treat a single set of species incidence frequencies as the reference point to estimate richness for smaller and larger numbers of sampling units.The first objective is a problem in interpolation that we address with classical rarefaction (multinomial model) and Coleman rarefaction (Poisson model) for individual-based data and with sample-based rarefaction (Bernoulli product model) for incidence frequencies. The second is a problem in extrapolation that we address with sampling-theoretic predictors for the number of species in a larger sample (multinomial model), a larger area (Poisson model) or a larger number of sampling units (Bernoulli product model), based on an estimate of asymptotic species richness. Although published methods exist for many of these objectives, we bring them together here with some new estimators under a unified statistical and notational framework. This novel integration of mathematically distinct approaches allowed us to link interpolated (rarefaction) curves and extrapolated curves to plot a unified species accumulation curve for empirical examples. We provide new, unconditional variance estimators for classical, individual-based rarefaction and for Coleman rarefaction, long missing from the toolkit of biodiversity measurement. We illustrate these methods with datasets for tropical beetles, tropical trees and tropical ants.Surprisingly, for all datasets we examined, the interpolation (rarefaction) curve and the extrapolation curve meet smoothly at the reference sample, yielding a single curve. Moreover, curves representing 95\% confidence intervals for interpolated and extrapolated richness estimates also meet smoothly, allowing rigorous statistical comparison of samples not only for rarefaction but also for extrapolated richness values. The confidence intervals widen as the extrapolation moves further beyond the reference sample, but the method gives reasonable results for extrapolations up to about double or triple the original abundance or area of the reference sample. We found that the multinomial and Poisson models produced indistinguishable results, in units of estimated species, for all estimators and datasets. For sample-based abundance data, which allows the comparison of all three models, the Bernoulli product model generally yields lower richness estimates for rarefied data than either the multinomial or the Poisson models because of the ubiquity of non-random spatial distributions in nature.},
  keywords = {notion}
}

@inproceedings{corbettBanderaExtractingFinitestate2000,
  title = {Bandera: Extracting Finite-State Models from {{Java}} Source Code},
  booktitle = {Proceedings of the 2000 {{International Conference}} on {{Software Engineering}}. {{ICSE}} 2000 the {{New Millennium}}},
  author = {Corbett, J. C. and Dwyer, M. B. and Hatcliff, J. and Laubach, S. and Pasareanu, C. S. and {Robby} and Zheng, Hongjun},
  date = {2000-06},
  pages = {439--448},
  issn = {0270-5257},
  doi = {10.1145/337180.337625},
  abstract = {Finite-state verification techniques, such as model checking, have shown promise as a cost-effective means for finding defects in hardware designs. To date, the application of these techniques to software has been hindered by several obstacles. Chief among these is the problem of constructing a finite-state model that approximates the executable behavior of the software system of interest. Current best-practice involves hand construction of models which is expensive (prohibitive for all but the smallest systems), prone to errors (which can result in misleading verification results), and difficult to optimize (which is necessary to combat the exponential complexity of verification algorithms). The authors describe an integrated collection of program analysis and transformation components, called Bandera, that enables the automatic extraction of safe, compact finite-state models from program source code. Bandera takes as input Java source code and generates a program model in the input language of one of several existing verification tools; Bandera also maps verifier outputs back to the original source code. We discuss the major components of Bandera and give an overview of how it can be used to model check correctness properties of Java programs.},
  keywords = {Java,notion,Program Slicing}
}

@article{corbiProgramUnderstandingChallenge1989a,
  title = {Program {{Understanding}}: {{Challenge For The}} 1990s},
  author = {Corbi, Thomas A.},
  date = {1989},
  journaltitle = {IBM Systems Journal},
  volume = {28},
  number = {2},
  pages = {294},
  url = {https://www.proquest.com/scholarly-journals/program-understanding-challenge-1990s/docview/222413802/se-2},
  abstract = {Today, large software systems are being changed in order to: 1. eliminate defects, 2. address new requirements, 3. improve design and performance, 4. interface to new programs, 5. adjust to changes in data structures or formats, and 6. exploit new hardware and software features. However, the introduction of changes and enhancements into maturing systems causes the structure of the systems to begin to deteriorate. Clearly, the challenges for tomorrow's programming community will be even more difficult than today's. IBM Corp.'s research division began work in 1986 on the Program Understanding Project, set up to develop tools that would assist programmers in 2 key areas - static analysis (reading the code) and dynamic analysis (running the code). The project focuses on giving programmers a new way of reading programs and of viewing and navigating the dynamic data structures of a program that has been stopped during execution.},
  isbn = {00188670},
  langid = {english},
  keywords = {5240:Software & systems,Computer programming,Computers,Maintenance,Modifications,notion,Programs,Software,Systems development}
}

@misc{cordyFlakiMeLaboratoryControlledTest2019,
  title = {{{FlakiMe}}: {{Laboratory-Controlled Test Flakiness Impact Assessment}}. {{A Case Study}} on {{Mutation Testing}} and {{Program Repair}}},
  author = {Cordy, Maxime and Rwemalika, Renaud and Papadakis, Mike and Harman, Mark},
  date = {2019},
  keywords = {notion}
}

@misc{cornejoCBRControlledBurst2020,
  title = {{{CBR}}: {{Controlled Burst Recording}}},
  author = {Cornejo, Oscar and Briola, Daniela and Micucci, Daniela and Mariani, Leonardo},
  date = {2020},
  keywords = {notion}
}

@inproceedings{cousotModularStaticProgram2002,
  title = {Modular {{Static Program Analysis}}},
  booktitle = {International {{Conference}} on {{Compiler Construction}}},
  author = {Cousot, Patrick and Cousot, Radhia},
  date = {2002},
  url = {https://api.semanticscholar.org/CorpusID:7036189},
  keywords = {notion}
}

@inproceedings{cucu-grosjeanMeasurementBasedProbabilisticTiming2012,
  title = {Measurement-{{Based Probabilistic Timing Analysis}} for {{Multi-path Programs}}},
  booktitle = {2012 24th {{Euromicro Conference}} on {{Real-Time Systems}}},
  author = {Cucu-Grosjean, Liliana and Santinelli, Luca and Houston, Michael and Lo, Code and Vardanega, Tullio and Kosmidis, Leonidas and Abella, Jaume and Mezzetti, Enrico and Quiñones, Eduardo and Cazorla, Francisco J.},
  date = {2012},
  pages = {91--101},
  doi = {10.1109/ECRTS.2012.31},
  keywords = {notion}
}

@inproceedings{cuiTupniAutomaticReverse2008,
  title = {Tupni: {{Automatic Reverse Engineering}} of {{Input Formats}}},
  booktitle = {Proceedings of the 15th {{ACM Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Cui, Weidong and Peinado, Marcus and Chen, Karl and Wang, Helen J. and Irun-Briz, Luis},
  date = {2008},
  series = {{{CCS}} '08},
  pages = {391--402},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1455770.1455820},
  url = {https://doi.org/10.1145/1455770.1455820},
  abstract = {Recent work has established the importance of automatic reverse engineering of protocol or file format specifications. However, the formats reverse engineered by previous tools have missed important information that is critical for security applications. In this paper, we present Tupni, a tool that can reverse engineer an input format with a rich set of information, including record sequences, record types, and input constraints. Tupni can generalize the format specification over multiple inputs. We have implemented a prototype of Tupni and evaluated it on ten different formats: five file formats (WMF, BMP, JPG, PNG and TIF) and five network protocols (DNS, RPC, TFTP, HTTP and FTP). Tupni identified all record sequences in the test inputs. We also show that, by aggregating over multiple WMF files, Tupni can derive a more complete format specification for WMF. Furthermore, we demonstrate the utility of Tupni by using the rich information it provides for zero-day vulnerability signature generation, which was not possible with previous reverse engineering tools.},
  isbn = {978-1-59593-810-7},
  venue = {Alexandria, Virginia, USA},
  keywords = {binary analysis,notion,protocol reverse engineering}
}

@misc{cumminsDeepDataFlow2021,
  title = {Deep {{Data Flow Analysis}}},
  author = {Cummins, Chris and Fisches, Zacharias and Ben-Nun, Tal and Hoefler, Torsten and Leather, Hugh and O'Boyle, Michael},
  date = {2021},
  url = {https://openreview.net/forum?id=SPhswbiXpJQ},
  keywords = {notion}
}

@article{daiDiscriminativeEmbeddingsLatent2016,
  title = {Discriminative {{Embeddings}} of {{Latent Variable Models}} for {{Structured Data}}},
  author = {Dai, Hanjun and Dai, Bo and Song, Le},
  date = {2016},
  journaltitle = {CoRR},
  volume = {abs/1603.05629},
  url = {http://arxiv.org/abs/1603.05629},
  keywords = {notion}
}

@inproceedings{dallmeierExtractionBugLocalization2007,
  title = {Extraction of {{Bug Localization Benchmarks}} from {{History}}},
  booktitle = {Proceedings of the {{Twenty-second IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Dallmeier, Valentin and Zimmermann, Thomas},
  date = {2007},
  series = {{{ASE}} '07},
  pages = {433--436},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/1321631.1321702},
  url = {http://doi.acm.org/10.1145/1321631.1321702},
  isbn = {978-1-59593-882-4},
  venue = {Atlanta, Georgia, USA},
  keywords = {benchmarking,defect localization,notion}
}

@article{danglotCorrectnessAttractionStudy2017,
  title = {Correctness Attraction: A Study of Stability of Software Behavior under Runtime Perturbation},
  author = {Danglot, Benjamin and Preux, Philippe and Baudry, Benoit and Monperrus, Martin},
  date = {2017-12},
  journaltitle = {Empirical Software Engineering},
  volume = {23},
  number = {4},
  pages = {2086--2119},
  publisher = {{Springer Science and Business Media LLC}},
  issn = {1573-7616},
  doi = {10.1007/s10664-017-9571-8},
  url = {http://dx.doi.org/10.1007/s10664-017-9571-8},
  keywords = {notion}
}

@article{dantoniNoFAQSynthesizingCommand2016,
  title = {{{NoFAQ}}: {{Synthesizing Command Repairs}} from {{Examples}}},
  author = {D'Antoni, Loris and Singh, Rishabh and Vaughn, Michael},
  date = {2016},
  journaltitle = {CoRR},
  volume = {abs/1608.08219},
  url = {http://arxiv.org/abs/1608.08219},
  keywords = {notion}
}

@article{davisSurveyProbabilisticTiming2019,
  title = {A {{Survey}} of {{Probabilistic Timing Analysis Techniques}} for {{Real-Time Systems}}},
  author = {Davis, Robert Ian and Cucu-Grosjean, Liliana},
  date = {2019-05},
  journaltitle = {LITES: Leibniz Transactions on Embedded Systems},
  pages = {1--60},
  url = {https://eprints.whiterose.ac.uk/162338/},
  abstract = {This survey covers probabilistic timing analysis techniques for real-time systems. It reviews and critiques the key results in the field from its origins in 2000 to the latest research published up to the end of August 2018. The survey provides a taxonomy of the different methods used, and a classification of existing research. A detailed review is provided covering the main subject areas: static probabilistic timing analysis, measurement-based probabilistic timing analysis, and hybrid methods. In addition, research on supporting mechanisms and techniques, case studies, and evaluations is also reviewed. The survey concludes by identifying open issues, key challenges and possible directions for future research.},
  keywords = {notion,Probabilistic,real-time,timing analysis}
}

@inproceedings{decanImpactSecurityVulnerabilities2018,
  title = {On the Impact of Security Vulnerabilities in the Npm Package Dependency Network},
  booktitle = {Proceedings of the 15th {{International Conference}} on {{Mining Software Repositories}}},
  author = {Decan, Alexandre and Mens, Tom and Constantinou, Eleni},
  date = {2018},
  series = {{{MSR}} '18},
  pages = {181--191},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3196398.3196401},
  url = {https://doi.org/10.1145/3196398.3196401},
  abstract = {Security vulnerabilities are among the most pressing problems in open source software package libraries. It may take a long time to discover and fix vulnerabilities in packages. In addition, vulnerabilities may propagate to dependent packages, making them vulnerable too. This paper presents an empirical study of nearly 400 security reports over a 6-year period in the npm dependency network containing over 610k JavaScript packages. Taking into account the severity of vulnerabilities, we analyse how and when these vulnerabilities are discovered and fixed, and to which extent they affect other packages in the packaging ecosystem in presence of dependency constraints. We report our findings and provide guidelines for package maintainers and tool developers to improve the process of dealing with security issues.},
  isbn = {978-1-4503-5716-6},
  venue = {Gothenburg, Sweden},
  keywords = {dependency network,notion,security vulnerability,semantic versioning,software ecosystem,software repository mining}
}

@inproceedings{defreezPathBasedFunctionEmbedding2018,
  title = {Path-{{Based Function Embedding}} and Its {{Application}} to {{Specification Mining}}},
  booktitle = {{{CoRR}}},
  author = {DeFreez, Daniel and Thakur, Aditya V. and Rubio-González, Cindy},
  date = {2018},
  volume = {abs/1802.07779},
  url = {http://arxiv.org/abs/1802.07779},
  keywords = {notion}
}

@inproceedings{degiovanniUBertMutationTesting2022,
  title = {{{µBert}}: {{Mutation Testing}} Using {{Pre-Trained Language Models}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation Workshops}} ({{ICSTW}})},
  author = {Degiovanni, Renzo and Papadakis, Mike},
  date = {2022},
  pages = {160--169},
  doi = {10.1109/ICSTW55395.2022.00039},
  keywords = {notion}
}

@inproceedings{delplanqueRottenGreenTests2019,
  title = {Rotten {{Green Tests}}},
  booktitle = {Proceedings of the 41st {{International Conference}} on {{Software Engineering}}},
  author = {Delplanque, Julien and Ducasse, Stéphane and Polito, Guillermo and Black, Andrew P. and Etien, Anne},
  date = {2019},
  series = {{{ICSE}} '19},
  pages = {500--511},
  publisher = {IEEE Press},
  location = {Piscataway, NJ, USA},
  doi = {10.1109/ICSE.2019.00062},
  url = {https://doi.org/10.1109/ICSE.2019.00062},
  venue = {Montreal, Quebec, Canada},
  keywords = {notion}
}

@inproceedings{demilloCriticalSlicingSoftware1996,
  title = {Critical {{Slicing}} for {{Software Fault Localization}}},
  booktitle = {Proceedings of the 1996 {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {DeMillo, Richard A. and Pan, Hsin and Spafford, Eugene H.},
  date = {1996},
  series = {{{ISSTA}} '96},
  pages = {121--134},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/229000.226310},
  url = {http://doi.acm.org/10.1145/229000.226310},
  isbn = {0-89791-787-1},
  venue = {San Diego, California, USA},
  keywords = {critical slicing,debugging,dynamic program slicing,failures,fault localization,faults,mutation analysis,notion,static program slicing,testing}
}

@inproceedings{desaiProgramSynthesisUsing2016,
  title = {Program {{Synthesis Using Natural Language}}},
  booktitle = {2016 {{IEEE}}/{{ACM}} 38th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Desai, A. and Gulwani, S. and Hingorani, V. and Jain, N. and Karkare, A. and Marron, M. and R, S. and Roy, S.},
  date = {2016-05},
  pages = {345--356},
  doi = {10.1145/2884781.2884786},
  abstract = {Interacting with computers is a ubiquitous activity for millions of people. Repetitive or specialized tasks often require creation of small, often one-off, programs. End-users struggle with learning and using the myriad of domain-specific languages (DSLs) to effectively accomplish these tasks. We present a general framework for constructing program synthesizers that take natural language (NL) inputs and produce expressions in a target DSL. The framework takes as input a DSL definition and training data consisting of NL/DSL pairs. From these it constructs a synthesizer by learning optimal weights and classifiers (using NLP features) that rank the outputs of a keyword-programming based translation. We applied our framework to three domains: repetitive text editing, an intelligent tutoring system, and flight information queries. On 1200+ English descriptions, the respective synthesizers rank the desired program as the top-1 and top-3 for 80\% and 90\% descriptions respectively.},
  keywords = {Natural Language,notion,Program Synthesis}
}

@inproceedings{desouzaNovelFitnessFunction2018,
  title = {A {{Novel Fitness Function}} for {{Automated Program Repair Based}} on {{Source Code Checkpoints}}},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {family=Souza, given=Eduardo Faria, prefix=de, useprefix=true and Goues, Claire Le and Camilo-Junior, Celso Gonçalves},
  date = {2018},
  series = {{{GECCO}} '18},
  pages = {1443--1450},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/3205455.3205566},
  url = {http://doi.acm.org/10.1145/3205455.3205566},
  isbn = {978-1-4503-5618-3},
  venue = {Kyoto, Japan},
  keywords = {fitness function,genetic programming,notion,program repair,software engineering}
}

@inproceedings{deweyMiMIsSimpleEfficient2020,
  title = {{{MiMIs}}: {{Simple}}, {{Efficient}}, and {{Fast Bounded-Exhaustive Test Case Generators}}},
  booktitle = {2020 {{IEEE}} 13th {{International Conference}} on {{Software Testing}}, {{Validation}} and {{Verification}} ({{ICST}})},
  author = {Dewey, K. and Hairapetian, S. and Gavrilov, M.},
  date = {2020},
  pages = {51--62},
  doi = {10.1109/ICST46399.2020.00016},
  keywords = {notion}
}

@inproceedings{dharCLOTHOSavingPrograms2015,
  title = {{{CLOTHO}}: {{Saving Programs}} from {{Malformed Strings}} and {{Incorrect String-handling}}},
  booktitle = {Proceedings of the 2015 10th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Dhar, Aritra and Purandare, Rahul and Dhawan, Mohan and Rangaswamy, Suresh},
  date = {2015},
  series = {{{ESEC}}/{{FSE}} 2015},
  pages = {555--566},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2786805.2786877},
  url = {http://doi.acm.org/10.1145/2786805.2786877},
  isbn = {978-1-4503-3675-8},
  venue = {Bergamo, Italy},
  keywords = {Automatic Program Repair,Clotho,notion}
}

@inproceedings{diasUntanglingFinegrainedCode2015,
  title = {Untangling Fine-Grained Code Changes},
  booktitle = {2015 {{IEEE}} 22nd {{International Conference}} on {{Software Analysis}}, {{Evolution}}, and {{Reengineering}} ({{SANER}})},
  author = {Dias, M. and Bacchelli, A. and Gousios, G. and Cassou, D. and Ducasse, S.},
  date = {2015-03},
  pages = {341--350},
  issn = {1534-5351},
  doi = {10.1109/SANER.2015.7081844},
  keywords = {Clustering algorithms,EpiceaUntangler,fine-grained code change information,fine-grained code change untangling,Java,Machine learning algorithms,notion,Reliability,Software,software maintenance,Testing,Training,version control system}
}

@inproceedings{dietschCraigVsNewton2017,
  title = {Craig vs. {{Newton}} in {{Software Model Checking}}},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Dietsch, Daniel and Heizmann, Matthias and Musa, Betim and Nutz, Alexander and Podelski, Andreas},
  date = {2017},
  series = {{{ESEC}}/{{FSE}} 2017},
  pages = {487--497},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/3106237.3106307},
  url = {http://doi.acm.org/10.1145/3106237.3106307},
  isbn = {978-1-4503-5105-8},
  venue = {Paderborn, Germany},
  keywords = {Craig Interpolation,Formal Verification,notion,Unsatisfiable Cores}
}

@inproceedings{dinellaHOPPITYLEARNINGGRAPH2020,
  title = {{{HOPPITY}}: {{LEARNING GRAPH TRANSFORMATIONS TO DETECT AND FIX BUGS IN PROGRAMS}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Dinella, Elizabeth and Dai, Hanjun and Li, Ziyang and Naik, Mayur and Song, Le and Wang, Ke},
  date = {2020},
  url = {https://openreview.net/forum?id=SJeqs6EFvB},
  keywords = {notion}
}

@misc{dingEmpiricalStudyOSSFuzz2021,
  title = {An {{Empirical Study}} of {{OSS-Fuzz Bugs}}},
  author = {Ding, Zhen Yu and Goues, Claire Le},
  date = {2021},
  keywords = {notion}
}

@article{ditFeatureLocationSource2013,
  title = {Feature Location in Source Code: A Taxonomy and Survey},
  author = {Dit, Bogdan and Revelle, Meghan and Gethers, Malcom and Poshyvanyk, Denys},
  date = {2013},
  journaltitle = {Journal of software: Evolution and Process},
  volume = {25},
  number = {1},
  pages = {53--95},
  publisher = {Wiley Online Library},
  keywords = {notion}
}

@inproceedings{domingosMiningHighspeedData2000,
  title = {Mining High-Speed Data Streams},
  booktitle = {Proceedings of the Sixth {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Domingos, Pedro and Hulten, Geoff},
  date = {2000},
  pages = {71--80},
  keywords = {notion}
}

@inproceedings{dongTimetravelTestingAndroid2020,
  title = {Time-Travel Testing of Android Apps},
  booktitle = {Proceedings of the {{ACM}}/{{IEEE}} 42nd {{International Conference}} on {{Software Engineering}}},
  author = {Dong, Zhen and Böhme, Marcel and Cojocaru, Lucia and Roychoudhury, Abhik},
  date = {2020},
  pages = {481--492},
  keywords = {notion}
}

@article{dormuthLogicalSegmentationSource2019,
  title = {Logical {{Segmentation}} of {{Source Code}}},
  author = {Dormuth, Jacob and Gelman, Ben and Moore, Jessica and Slater, David},
  date = {2019},
  journaltitle = {CoRR},
  volume = {abs/1907.08615},
  url = {http://arxiv.org/abs/1907.08615},
  keywords = {notion}
}

@inproceedings{dorrePracticalDetectionEntropy2016,
  title = {Practical {{Detection}} of {{Entropy Loss}} in {{Pseudo-Random Number Generators}}},
  booktitle = {Proceedings of the 2016 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Dörre, Felix and Klebanov, Vladimir},
  date = {2016},
  series = {{{CCS}} '16},
  pages = {678--689},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2976749.2978369},
  url = {https://doi.org/10.1145/2976749.2978369},
  abstract = {Pseudo-random number generators (PRNGs) are a critical infrastructure for cryptography and security of many computer applications. At the same time, PRNGs are surprisingly difficult to design, implement, and debug. This paper presents the first static analysis technique specifically for quality assurance of cryptographic PRNG implementations.The analysis targets a particular kind of implementation defect, the entropy loss. Entropy loss occurs when the entropy contained in the PRNG seed is not utilized to the full extent for generating the pseudo-random output stream. The Debian OpenSSL disaster, probably the most prominent PRNG-related security incident, was one but not the only manifestation of such a defect.Together with the static analysis technique, we present its implementation, a tool named Entroposcope. The tool offers a high degree of automation and practicality. We have applied the tool to five real-world PRNGs of different designs and show that it effectively detects both known and previously unknown instances of entropy loss.},
  isbn = {978-1-4503-4139-4},
  venue = {Vienna, Austria},
  keywords = {bounded model checking,entropy loss,information flow,notion,OpenSSL,PRNG,pseudo-random number generator,static analysis}
}

@article{doSupportingControlledExperimentation2005,
  title = {Supporting {{Controlled Experimentation}} with {{Testing Techniques}}: {{An Infrastructure}} and {{Its Potential Impact}}},
  author = {Do, Hyunsook and Elbaum, Sebastian and Rothermel, Gregg},
  date = {2005-10},
  journaltitle = {Empirical Softw. Engg.},
  volume = {10},
  number = {4},
  pages = {405--435},
  publisher = {Kluwer Academic Publishers},
  location = {USA},
  issn = {1382-3256},
  doi = {10.1007/s10664-005-3861-2},
  url = {https://doi.org/10.1007/s10664-005-3861-2},
  abstract = {Where the creation, understanding, and assessment of software testing and regression testing techniques are concerned, controlled experimentation is an indispensable research methodology. Obtaining the infrastructure necessary to support such experimentation, however, is difficult and expensive. As a result, progress in experimentation with testing techniques has been slow, and empirical data on the costs and effectiveness of techniques remains relatively scarce. To help address this problem, we have been designing and constructing infrastructure to support controlled experimentation with testing and regression testing techniques. This paper reports on the challenges faced by researchers experimenting with testing techniques, including those that inform the design of our infrastructure. The paper then describes the infrastructure that we are creating in response to these challenges, and that we are now making available to other researchers, and discusses the impact that this infrastructure has had and can be expected to have.},
  keywords = {controlled experimentation,experiment infrastructure,notion,regression testing,siemens suite,Software testing}
}

@article{drukhConcentrationBoundsUnigram2005,
  title = {Concentration {{Bounds}} for {{Unigram Language Models}}},
  author = {Drukh, Evgeny and Mansour, Yishay},
  date = {2005},
  journaltitle = {Journal of Machine Learning Research},
  volume = {6},
  number = {8},
  keywords = {notion}
}

@inproceedings{duDeepLogAnomalyDetection2017,
  title = {{{DeepLog}}: {{Anomaly Detection}} and {{Diagnosis}} from {{System Logs}} through {{Deep Learning}}},
  booktitle = {Proceedings of the 2017 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Du, Min and Li, Feifei and Zheng, Guineng and Srikumar, Vivek},
  date = {2017},
  series = {{{CCS}} '17},
  pages = {1285--1298},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3133956.3134015},
  url = {https://doi.org/10.1145/3133956.3134015},
  abstract = {Anomaly detection is a critical step towards building a secure and trustworthy system. The primary purpose of a system log is to record system states and significant events at various critical points to help debug system failures and perform root cause analysis. Such log data is universally available in nearly all computer systems. Log data is an important and valuable resource for understanding system status and performance issues; therefore, the various system logs are naturally excellent source of information for online monitoring and anomaly detection. We propose DeepLog, a deep neural network model utilizing Long Short-Term Memory (LSTM), to model a system log as a natural language sequence. This allows DeepLog to automatically learn log patterns from normal execution, and detect anomalies when log patterns deviate from the model trained from log data under normal execution. In addition, we demonstrate how to incrementally update the DeepLog model in an online fashion so that it can adapt to new log patterns over time. Furthermore, DeepLog constructs workflows from the underlying system log so that once an anomaly is detected, users can diagnose the detected anomaly and perform root cause analysis effectively. Extensive experimental evaluations over large log data have shown that DeepLog has outperformed other existing log-based anomaly detection methods based on traditional data mining methodologies.},
  isbn = {978-1-4503-4946-8},
  venue = {Dallas, Texas, USA},
  keywords = {anomaly detection,deep learning,log data analysis,notion}
}

@article{dullienWeirdMachinesExploitability2020,
  title = {Weird {{Machines}}, {{Exploitability}}, and {{Provable Unexploitability}}},
  author = {Dullien, Thomas},
  date = {2020-04},
  journaltitle = {IEEE Transactions on Emerging Topics in Computing},
  volume = {8},
  number = {2},
  pages = {391--403},
  issn = {2168-6750},
  doi = {10.1109/TETC.2017.2785299},
  url = {https://ieeexplore.ieee.org/abstract/document/8226852},
  urldate = {2024-10-21},
  abstract = {The concept of exploit is central to computer security, particularly in the context of memory corruptions. Yet, in spite of the centrality of the concept and voluminous descriptions of various exploitation techniques or countermeasures, a good theoretical framework for describing and reasoning about exploitation has not yet been put forward. A body of concepts and folk theorems exists in the community of exploitation practitioners; unfortunately, these concepts are rarely written down or made sufficiently precise for people outside of this community to benefit from them. This paper clarifies a number of these concepts, provides a clear definition of exploit, a clear definition of the concept of a weird machine, and how programming of a weird machine leads to exploitation. The papers also shows, somewhat counterintuitively, that it is feasible to design some software in a way that even powerful attackers-with the ability to corrupt memory once-cannot gain an advantage. The approach in this paper is focused on memory corruptions. While it can be applied to many security vulnerabilities introduced by other programming mistakes, it does not address side channel attacks, protocol weaknesses, or security problems that are present by design.},
  eventtitle = {{{IEEE Transactions}} on {{Emerging Topics}} in {{Computing}}},
  keywords = {Complexity theory,computation theory,computer hacking,Computer security,Concrete,Cryptography,information security,language-theoretic security,notion,Programming,Software,Transducers},
  file = {/Users/bohrok/Zotero/storage/K7E3A9AF/Dullien - 2020 - Weird Machines, Exploitability, and Provable Unexploitability.pdf;/Users/bohrok/Zotero/storage/MPGDXCRQ/8226852.html}
}

@article{dumMonteCarloSimulation1992,
  title = {Monte {{Carlo}} Simulation of the Atomic Master Equation for Spontaneous Emission},
  author = {Dum, R. and Zoller, P. and Ritsch, H.},
  date = {1992-04},
  journaltitle = {Phys. Rev. A},
  volume = {45},
  number = {7},
  pages = {4879--4887},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevA.45.4879},
  url = {https://link.aps.org/doi/10.1103/PhysRevA.45.4879},
  keywords = {notion}
}

@inproceedings{durieuxDynamicPatchGeneration2017,
  title = {Dynamic Patch Generation for Null Pointer Exceptions Using Metaprogramming},
  booktitle = {2017 {{IEEE}} 24th {{International Conference}} on {{Software Analysis}}, {{Evolution}} and {{Reengineering}} ({{SANER}})},
  author = {Durieux, T. and Cornu, B. and Seinturier, L. and Monperrus, M.},
  date = {2017-02},
  pages = {349--358},
  doi = {10.1109/SANER.2017.7884635},
  abstract = {Null pointer exceptions (NPE) are the number one cause of uncaught crashing exceptions in production. In this paper, we aim at exploring the search space of possible patches for null pointer exceptions with metaprogramming. Our idea is to transform the program under repair with automated code transformation, so as to obtain a metaprogram. This metaprogram contains automatically injected hooks, that can be activated to emulate a null pointer exception patch. This enables us to perform a fine-grain analysis of the runtime context of null pointer exceptions. We set up an experiment with 16 real null pointer exceptions that have happened in the field. We compare the effectiveness of our metaprogramming approach against simple templates for repairing null pointer exceptions.},
  keywords = {automated code transformation,Computer crashes,Context,dynamic patch generation,fine-grain analysis,Java,Maintenance engineering,metaprogramming,notion,NPE,null pointer exceptions,Production,program repair,search space,software maintenance,Space exploration,Taxonomy}
}

@inproceedings{duRipplesMutationEmpirical2024,
  title = {Ripples of a {{Mutation}} — {{An Empirical Study}} of {{Propagation Effects}} in {{Mutation Testing}}},
  booktitle = {Proceedings of the {{IEEE}}/{{ACM}} 46th {{International Conference}} on {{Software Engineering}}},
  author = {Du, Hang and Palepu, Vijay Krishna and Jones, James A.},
  date = {2024},
  series = {{{ICSE}} '24},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3597503.3639179},
  url = {https://doi.org/10.1145/3597503.3639179},
  abstract = {The mechanics of how a fault reveals itself as a test failure is of keen interest to software researchers and practitioners alike. An improved understanding of how faults translate to failures can guide improvements in broad facets of software testing, ranging from test suite design to automated program repair, which are premised on the understanding that the presence of faults would alter some test executions.In this work, we study such effects by mutations, as applicable in mutation testing. Mutation testing enables the generation of a large corpus of faults; thereby harvesting a large pool of mutated test runs for analysis. Specifically, we analyze more than 1.1 million mutated test runs to study if and how the underlying mutations induce infections that propagate their way to observable failures.We adopt a broad-spectrum approach to analyze such a large pool of mutated runs. For every mutated test run, we are able to determine: (a) if the mutation induced a state infection; (b) if the infection propagated through the end of the test run; and (c) if the test failed in the presence of a propagated infection.By examining such infection-, propagation- and revealability-effects for more than 43,000 mutations executed across 1.1 million test runs we are able to arrive at some surprising findings. Our results find that once state infection is observed, propagation is frequently detected; however, a propagated infection does not always reveal itself as a test failure. We also find that a significant portion of survived mutants in our study could have been killed by observing propagated state infections that were left undetected. Finally, we also find that different mutation operators can demonstrate substantial differences in their specific impacts on the execution-to-failure ripples of the resulting mutations.},
  isbn = {9798400702174},
  venue = {{$<$}conf-loc{$>$}, {$<$}city{$>$}Lisbon{$<$}/city{$>$}, {$<$}country{$>$}Portugal{$<$}/country{$>$}, {$<$}/conf-loc{$>$}},
  keywords = {dynamic analysis,empirical study,error propagation,mutation testing,notion,software fault infection}
}

@inproceedings{durschmidROSInferStaticallyInferring2024,
  title = {{{ROSInfer}}: {{Statically Inferring Behavioral Component Models}} for {{ROS-based Robotics Systems}}},
  shorttitle = {{{ROSInfer}}},
  booktitle = {Proceedings of the {{IEEE}}/{{ACM}} 46th {{International Conference}} on {{Software Engineering}}},
  author = {Dürschmid, Tobias and Timperley, Christopher Steven and Garlan, David and Le Goues, Claire},
  date = {2024-04-12},
  series = {{{ICSE}} '24},
  pages = {1--13},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3597503.3639206},
  url = {https://dl.acm.org/doi/10.1145/3597503.3639206},
  urldate = {2024-09-19},
  abstract = {Robotics systems are complex, safety-critical systems that can consist of hundreds of software components that interact with each other dynamically during run time. Software components of robotics systems often exhibit reactive, periodic, and state-dependent behavior. Incorrect component composition can lead to unexpected behavior, such as components passively waiting for initiation messages that never arrive. Model-based software analysis is a common technique to identify incorrect behavioral composition by checking desired properties of given behavioral models that are based on component state machines. However, writing state machine models for hundreds of software components manually is a labor-intensive process. This motivates work on automated model inference. In this paper, we present an approach to infer behavioral models for systems based on the Robot Operating System (ROS) using static analysis by exploiting assumptions about the usage of the ROS API and ecosystem. Our approach is based on searching for common behavioral patterns that ROS developers use for implementing reactive, periodic, and state-dependent behavior using the ROS framework API. We evaluate our approach and our tool ROSInfer on five complex real-world ROS systems with a total of 534 components. For this purpose we manually created 155 models of components from the source code to be used as a ground truth and available data set for other researchers. ROSInfer can infer causal triggers for 87\% of component architectural behaviors in the 534 components.},
  isbn = {9798400702174},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/EER8RZVA/Dürschmid et al. - 2024 - ROSInfer Statically Inferring Behavioral Component Models for ROS-based Robotics Systems.pdf}
}

@article{dusingPersistingReusingResults,
  title = {Persisting and {{Reusing Results}} of {{Static Program Analyses}} on a {{Large Scale}}},
  author = {Düsing, Johannes and Hermann, Ben},
  keywords = {_reuse,notion}
}

@unpublished{dutraFormatFuzzerEffectiveFuzzing2021,
  title = {{{FormatFuzzer}}: {{Effective Fuzzing}} of {{Binary File Formats}}},
  author = {Dutra, Rafael and Gopinath, Rahul and Zeller, Andreas},
  date = {2021},
  eprint = {2109.11277},
  eprinttype = {arXiv},
  keywords = {notion}
}

@article{duttaHierarchicallyLocalizingSoftware2019,
  title = {Hierarchically {{Localizing Software Faults Using DNN}}},
  author = {Dutta, A. and Manral, R. and Mitra, P. and Mall, R.},
  date = {2019},
  journaltitle = {IEEE Transactions on Reliability},
  pages = {1--26},
  issn = {1558-1721},
  doi = {10.1109/TR.2019.2956120},
  keywords = {Deep neural network (DNN),fault localization,notion,program debugging}
}

@inproceedings{duttaStormProgramReduction2019,
  title = {Storm: {{Program Reduction}} for {{Testing}} and {{Debugging Probabilistic Programming Systems}}},
  booktitle = {Proceedings of the 2019 27th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Dutta, Saikat and Zhang, Wenxian and Huang, Zixin and Misailovic, Sasa},
  date = {2019},
  series = {{{ESEC}}/{{FSE}} 2019},
  pages = {729--739},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/3338906.3338972},
  url = {http://doi.acm.org/10.1145/3338906.3338972},
  isbn = {978-1-4503-5572-8},
  venue = {Tallinn, Estonia},
  keywords = {notion,Probabilistic Programming Languages,Software Testing}
}

@inproceedings{dwyerProbabilisticProgramAnalysis2017,
  title = {Probabilistic {{Program Analysis}}},
  booktitle = {Grand {{Timely Topics}} in {{Software Engineering}}},
  author = {Dwyer, Matthew B. and Filieri, Antonio and Geldenhuys, Jaco and Gerrard, Mitchell and Păsăreanu, Corina S. and Visser, Willem},
  editor = {Cunha, Jácome and Fernandes, João P. and Lämmel, Ralf and Saraiva, João and Zaytsev, Vadim},
  date = {2017},
  pages = {1--25},
  publisher = {Springer International Publishing},
  location = {Cham},
  abstract = {This paper provides a survey of recent work on adapting techniques for program analysis to compute probabilistic characterizations of program behavior. We survey how the frameworks of data flow analysis and symbolic execution have incorporated information about input probability distributions to quantify the likelihood of properties of program states. We identify themes that relate and distinguish a variety of techniques that have been developed over the past 15 years in this area. In doing so, we point out opportunities for future research that builds on the strengths of different techniques.},
  isbn = {978-3-319-60074-1},
  keywords = {notion}
}

@article{efronEstimatingNumberUnsen1976,
  title = {Estimating the {{Number}} of {{Unsen Species}}: {{How Many Words Did Shakespeare Know}}?},
  author = {Efron, Bradley and Thisted, Ronald},
  date = {1976},
  journaltitle = {Biometrika},
  volume = {63},
  number = {3},
  eprint = {2335721},
  eprinttype = {jstor},
  pages = {435--447},
  publisher = {[Oxford University Press, Biometrika Trust]},
  issn = {00063444},
  url = {http://www.jstor.org/stable/2335721},
  urldate = {2023-01-26},
  abstract = {Shakespeare wrote 31 534 different words, of which 14 376 appear only once, 4343 twice, etc. The question considered is how many words he knew but did not use. A parametric empirical Bayes model due to Fisher and a nonparametric model due to Good \& Toulmin are examined. The latter theory is augmented using linear programming methods. We conclude that the models are equivalent to supposing that Shakespeare knew at least 35 000 more words.},
  keywords = {notion}
}

@inproceedings{ellisEssentialGuideEffect2010,
  title = {The Essential Guide to Effect Sizes : Statistical Power, Meta-Analysis, and the Interpretation of Research Results},
  author = {Ellis, Paul D.},
  date = {2010},
  keywords = {notion}
}

@inproceedings{eomFuzzingJavaScriptInterpreters2024,
  title = {Fuzzing {{JavaScript Interpreters}} with {{Coverage-Guided Reinforcement Learning}} for {{LLM-Based Mutation}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Eom, Jueon and Jeong, Seyeon and Kwon, Taekyoung},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {1656--1668},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3680389},
  url = {https://dl.acm.org/doi/10.1145/3650212.3680389},
  urldate = {2024-09-21},
  abstract = {JavaScript interpreters, crucial for modern web browsers, require an effective fuzzing method to identify security-related bugs. However, the strict grammatical requirements for input present significant challenges. Recent efforts to integrate language models for context- aware mutation in fuzzing are promising but lack the necessary coverage guidance to be fully effective. This paper presents a novel technique called CovRL (Coverage-guided Reinforcement Learning) that combines Large Language Models (LLMs) with Reinforcement Learning (RL) from coverage feedback. Our fuzzer, CovRL-Fuzz, integrates coverage feedback directly into the LLM by leveraging the Term Frequency-Inverse Document Frequency (TF-IDF) method to construct a weighted coverage map. This map is key in calculating the fuzzing reward, which is then applied to the LLM-based mutator through reinforcement learning. CovRL-Fuzz, through this approach, enables the generation of test cases that are more likely to discover new coverage areas, thus improving bug detection while minimizing syntax and semantic errors, all without needing extra post-processing. Our evaluation results show that CovRL-Fuzz outperforms the state-of-the-art fuzzers in enhancing code coverage and identifying bugs in JavaScript interpreters: CovRL-Fuzz identified 58 real-world security-related bugs in the latest JavaScript interpreters, including 50 previously unknown bugs and 15 CVEs.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/2GAULBYU/Eom et al. - 2024 - Fuzzing JavaScript Interpreters with Coverage-Guided Reinforcement Learning for LLM-Based Mutation.pdf}
}

@inproceedings{epitropakisEmpiricalEvaluationPareto2015,
  title = {Empirical {{Evaluation}} of {{Pareto Efficient Multi-objective Regression Test Case Prioritisation}}},
  booktitle = {Proceedings of the 2015 {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Epitropakis, Michael G. and Yoo, Shin and Harman, Mark and Burke, Edmund K.},
  date = {2015},
  series = {{{ISSTA}} 2015},
  pages = {234--245},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2771783.2771788},
  url = {http://doi.acm.org/10.1145/2771783.2771788},
  isbn = {978-1-4503-3620-8},
  venue = {Baltimore, MD, USA},
  keywords = {Additional Greedy Algorithm,Coverage Compaction,Multi-objective Evolutionary Algorithm,notion,Test Case Prioritization}
}

@article{ernstDynamicallyDiscoveringLikely2001,
  title = {Dynamically Discovering Likely Program Invariants to Support Program Evolution},
  author = {Ernst, M. D. and Cockrell, J. and Griswold, W. G. and Notkin, D.},
  date = {2001-02},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {27},
  number = {2},
  pages = {99--123},
  issn = {0098-5589},
  doi = {10.1109/32.908957},
  abstract = {Explicitly stated program invariants can help programmers by identifying program properties that must be preserved when modifying code. In practice, however, these invariants are usually implicit. An alternative to expecting programmers to fully annotate code with invariants is to automatically infer likely invariants from the program itself. This research focuses on dynamic techniques for discovering invariants from execution traces. This article reports three results. First, it describes techniques for dynamically discovering invariants, along with an implementation, named Daikon, that embodies these techniques. Second, it reports on the application of Daikon to two sets of target programs. In programs from Gries's work (1981) on program derivation, the system rediscovered predefined invariants. In a C program lacking explicit invariants, the system discovered invariants that assisted a software evolution task. These experiments demonstrate that, at least for small programs, invariant inference is both accurate and useful. Third, it analyzes scalability issues, such as invariant detection runtime and accuracy, as functions of test suites and program points instrumented},
  keywords = {Daikon,notion,Program Invariant}
}

@inproceedings{ettingerUntanglingSliceExtraction2004,
  title = {Untangling: A Slice Extraction Refactoring.},
  author = {Ettinger, Ran and Verbaere, Mathieu},
  date = {2004-01},
  pages = {93--101},
  doi = {10.1145/976270.976283},
  keywords = {notion}
}

@article{faginComparingPartialRankings2006,
  title = {Comparing Partial Rankings},
  author = {Fagin, Ronald and Kumar, Ravi and Mahdian, Mohammad and Sivakumar, D and Vee, Erik},
  date = {2006},
  journaltitle = {SIAM Journal on Discrete Mathematics},
  volume = {20},
  number = {3},
  pages = {628--648},
  publisher = {SIAM},
  keywords = {notion}
}

@inproceedings{falkLawsSmallNumbers1994,
  title = {Laws of {{Small Numbers}}: {{Extremes}} and {{Rare Events}}},
  author = {Falk, Michael and Hüsler, Jürg and Reiss, Rolf-Dieter},
  date = {1994},
  keywords = {notion}
}

@inproceedings{falleriAutomaticExtractionWordNetIdentifier2010,
  title = {Automatic {{Extraction}} of a {{WordNet-Like Identifier Network}} from {{Software}}},
  booktitle = {2010 {{IEEE}} 18th {{International Conference}} on {{Program Comprehension}}},
  author = {Falleri, J.- and Huchard, M. and Lafourcade, M. and Nebut, C. and Prince, V. and Dao, M.},
  date = {2010-06},
  pages = {4--13},
  issn = {1092-8138},
  doi = {10.1109/ICPC.2010.12},
  keywords = {automatic extraction,Data mining,Documentation,Europe,feature extraction,Identifiers,Information resources,natural language processing,Natural language processing,Natural Language Processing,natural language processing technique,Navigation,notion,Open source software,program comprehension,Software architecture,Software Comprehension,software engineering,software maintenance,Software maintenance,Software systems,word processing,WordNet like identifier network}
}

@inproceedings{fangDDGFDynamicDirected2024,
  title = {{{DDGF}}: {{Dynamic Directed Greybox Fuzzing}} with {{Path Profiling}}},
  shorttitle = {{{DDGF}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Fang, Haoran and Zhang, Kaikai and Yu, Donghui and Zhang, Yuanyuan},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {832--843},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3680324},
  url = {https://dl.acm.org/doi/10.1145/3650212.3680324},
  urldate = {2024-09-21},
  abstract = {Coverage-Guided Fuzzing (CGF) has become the most popular and effective method for vulnerability detection. It is usually designed as an automated “black-box” tool. Security auditors start it and then just wait for the results. However, after a period of testing, CGF struggles to find new coverage gradually, thus making it inefficient. It is difficult for users to explain reasons that prevent fuzzing from making further progress and to determine whether the existing coverage is sufficient. In addition, there is no way to interact and direct the fuzzing process.                   In this paper, we design the dynamic directed greybox fuzzing (DDGF) to facilitate collaboration between the user and fuzzer. By leveraging Ball-Larus path profiling algorithm, we propose two new techniques: dynamic introspection and dynamic direction. Dynamic introspection reveals the significant imbalance in the distribution of path frequency through encoding and decoding. Based on the insight from introspection, users can dynamically direct the fuzzer to focus testing on the selected paths in real time. We implement DDGF based on AFL++. Experiments on Magma show that DDGF is effective in helping the fuzzer to reproduce vulnerabilities faster, with up to 100x speedup and only 13\% performance overhead. DDGF shows the great potential of human-in-the-loop for fuzzing.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/DE3YF7H5/Fang et al. - 2024 - DDGF Dynamic Directed Greybox Fuzzing with Path Profiling.pdf}
}

@inproceedings{fanOracleGuidedProgramSelection2024,
  title = {Oracle-{{Guided Program Selection}} from {{Large Language Models}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Fan, Zhiyu and Ruan, Haifeng and Mechtaev, Sergey and Roychoudhury, Abhik},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {628--640},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3680308},
  url = {https://dl.acm.org/doi/10.1145/3650212.3680308},
  urldate = {2024-09-21},
  abstract = {While large language models (LLMs) have shown significant advancements in code generation, their susceptibility to producing incorrect code poses a significant challenge to the adoption of LLM-generated programs. This issue largely stems from the reliance on natural language descriptions as informal oracles in code generation. Current strategies to mitigate this involve selecting the best program from multiple LLM-generated alternatives, judged by criteria like the consistency of their execution results on an LLM-generated test suite. However, this approach has crucial limitations: (1) LLMs often generate redundant tests or tests that cannot distinguish between correct and incorrect solutions, (2) the used consistency criteria, such as the majority vote, fail to foster developer trust due to the absence of transparent rationale behind the made choices. In this work, we propose a new perspective on increasing the quality of LLM-generated code via program selection using the LLM as a test oracle. Our method is based on our experimentally confirmed observation that LLMs serve more effectively as oracles when tasked with selecting the correct output from multiple choices. Leveraging this insight, we first generate distinguishing inputs that capture semantic discrepancies of programs sampled from an LLM, and record outputs produced by the programs on these inputs. An LLM then selects the most likely to be correct output from these, guided by the natural language problem description. We implemented this idea in a tool LLMCodeChoice and evaluated its accuracy in generating and selecting standalone programs. Our experiments demonstrated its effectiveness in improving pass@1 by 3.6-7\% on HumanEval and MBPP benchmarks compared to the state-of-art CodeT. Most interestingly, the selected input-output specifications helped us to uncover incompleteness and ambiguities in task descriptions and also identify incorrect ground-truth implementations in the benchmarks.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/VN73NVUW/Fan et al. - 2024 - Oracle-Guided Program Selection from Large Language Models.pdf}
}

@inproceedings{fanSMOKEScalablePathSensitive2019,
  title = {{{SMOKE}}: {{Scalable Path-Sensitive Memory Leak Detection}} for {{Millions}} of {{Lines}} of {{Code}}},
  author = {Fan, Gang and {wu}, Rongxin and Shi, Qingkai and Xiao, Xiao and Zhou, Jinguo and Zhang, Charles},
  date = {2019-05},
  keywords = {notion}
}

@article{fatimaFlakifyBlackBoxLanguage2021,
  title = {Flakify: {{A Black-Box}}, {{Language Model-based Predictor}} for {{Flaky Tests}}},
  author = {Fatima, Sakina and Ghaleb, Taher Ahmed and Briand, Lionel Claude},
  date = {2021},
  journaltitle = {ArXiv},
  volume = {abs/2112.12331},
  keywords = {notion}
}

@article{favaroRediscoveryGoodTuring2016,
  title = {Rediscovery of {{Good}}–{{Turing}} Estimators via {{Bayesian}} Nonparametrics},
  author = {Favaro, Stefano and Nipoti, Bernardo and Teh, Yee Whye},
  date = {2016},
  journaltitle = {Biometrics},
  volume = {72},
  keywords = {notion}
}

@unpublished{feinmanDetectingAdversarialSamples2017,
  title = {Detecting Adversarial Samples from Artifacts},
  author = {Feinman, Reuben and Curtin, Ryan R and Shintre, Saurabh and Gardner, Andrew B},
  date = {2017},
  eprint = {1703.00410},
  eprinttype = {arXiv},
  keywords = {notion}
}

@misc{feitelsonConsiderationsPitfallsControlled2021,
  title = {Considerations and {{Pitfalls}} in {{Controlled Experiments}} on {{Code Comprehension}}},
  author = {Feitelson, Dror G.},
  date = {2021},
  keywords = {notion}
}

@inproceedings{feldtTestSetDiameter2016,
  title = {Test {{Set Diameter}}: {{Quantifying}} the {{Diversity}} of {{Sets}} of {{Test Cases}}},
  booktitle = {2016 {{IEEE International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}} ({{ICST}})},
  author = {Feldt, R. and Poulding, S. and Clark, D. and Yoo, S.},
  date = {2016-04},
  pages = {223--233},
  doi = {10.1109/ICST.2016.33},
  abstract = {A common and natural intuition among software testers is that test cases need to differ if a software system is to be tested properly and its quality ensured. Consequently, much research has gone into formulating distance measures for how test cases, their inputs and/or their outputs differ. However, common to these proposals is that they are data type specific and/or calculate the diversity only between pairs of test inputs, traces or outputs. We propose a new metric to measure the diversity of sets of tests: the test set diameter (TSDm). It extends our earlier, pairwise test diversity metrics based on recent advances in information theory regarding the calculation of the normalized compression distance (NCD) for multisets. A key advantage is that TSDm is a universal measure of diversity and so can be applied to any test set regardless of data type of the test inputs (and, moreover, to other test-related data such as execution traces). But this universality comes at the cost of greater computational effort compared to competing approaches. Our experiments on four different systems show that the test set diameter can help select test sets with higher structural and fault coverage than random selection even when only applied to test inputs. This can enable early test design and selection, prior to even having a software system to test, and complement other types of test automation and analysis. We argue that this quantification of test set diversity creates a number of opportunities to better understand software quality and provides practical ways to increase it.},
  keywords = {notion,Test Case,Test Set Diameter(TSDm)}
}

@inproceedings{fengLearningUniversalProbabilistic2010,
  title = {Learning {{Universal Probabilistic Models}} for {{Fault Localization}}},
  booktitle = {Proceedings of the 9th {{ACM SIGPLAN-SIGSOFT Workshop}} on {{Program Analysis}} for {{Software Tools}} and {{Engineering}}},
  author = {Feng, Min and Gupta, Rajiv},
  date = {2010},
  series = {{{PASTE}} '10},
  pages = {81--88},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/1806672.1806688},
  url = {http://doi.acm.org/10.1145/1806672.1806688},
  isbn = {978-1-4503-0082-7},
  venue = {Toronto, Ontario, Canada},
  keywords = {bayesian network,dynamic dependence graph,fault localization,notion,probabilistic inference}
}

@article{feyziCGTFLUsingCooperative2020,
  title = {{{CGT-FL}}: Using Cooperative Game Theory to Effective Fault Localization in Presence of Coincidental Correctness},
  author = {Feyzi, Farid},
  date = {2020},
  journaltitle = {Empirical Software Engineering},
  volume = {25},
  number = {5},
  pages = {3873--3927},
  publisher = {Springer},
  keywords = {notion}
}

@article{feyziFPAFLIncorporatingStatic2018,
  title = {{{FPA-FL}}: {{Incorporating}} Static Fault-Proneness Analysis into Statistical Fault Localization},
  author = {Feyzi, Farid and Parsa, Saeed},
  date = {2018},
  journaltitle = {Journal of Systems and Software},
  volume = {136},
  pages = {39--58},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2017.11.002},
  url = {https://www.sciencedirect.com/science/article/pii/S0164121217302583},
  abstract = {Despite the proven applicability of the statistical methods in automatic fault localization, these approaches are biased by data collected from different executions of the program. This biasness could result in unstable statistical models which may vary dependent on test data provided for trial executions of the program. To resolve the difficulty, in this article a new `fault-proneness'-aware statistical approach based on Elastic-Net regression, namely FPA-FL is proposed. The main idea behind FPA-FL is to consider the static structure and the fault-proneness of the program statements in addition to their dynamic correlations with the program termination state. The grouping effect of FPA-FL is helpful for finding multiple faults and supporting scalability. To provide the context of failure, cause-effect chains of program faults are discovered. FPA-FL is evaluated from different viewpoints on well-known test suites. The results reveal high fault localization performance of our approach, compared with similar techniques in the literature.},
  keywords = {Backward dynamic slice,Coincidental correctness,Elastic-net regression,Fault localization,Fault-proneness,notion,Statistical debugging}
}

@article{feyziInforenceEffectiveFault2019,
  title = {Inforence: Effective Fault Localization Based on Information-Theoretic Analysis and Statistical Causal Inference},
  author = {Feyzi, Farid and Parsa, Saeed},
  date = {2019},
  journaltitle = {Frontiers of Computer Science},
  volume = {13},
  number = {4},
  pages = {735--759},
  doi = {10.1007/s11704-017-6512-z},
  url = {https://doi.org/10.1007/s11704-017-6512-z},
  abstract = {In this paper, a novel approach, Inforence, is proposed to isolate the suspicious codes that likely contain faults. Inforence employs a feature selection method, based on mutual information, to identify those bug-related statements that may cause the program to fail. Because the majority of a program faults may be revealed as undesired joint effect of the program statements on each other and on program termination state, unlike the state-of-the-art methods, Inforence tries to identify and select groups of interdependent statements which altogether may affect the program failure. The interdependence amongst the statements is measured according to their mutual effect on each other and on the program termination state. To provide the context of failure, the selected bug-related statements are chained to each other, considering the program static structure. Eventually, the resultant cause-effect chains are ranked according to their combined causal effect on program failure. To validate Inforence, the results of our experiments with seven sets of programs include Siemens suite, gzip, grep, sed, space, make and bash are presented. The experimental results are then compared with those provided by different fault localization techniques for the both single-fault and multi-fault programs. The experimental results prove the outperformance of the proposed method compared to the state-of-the-art techniques.},
  isbn = {2095-2236},
  keywords = {notion}
}

@article{feyziKernelbasedDetectionCoincidentally2018,
  title = {Kernel-Based Detection of Coincidentally Correct Test Cases to Improve Fault Localisation Effectiveness},
  author = {Feyzi, Farid and Parsa, Saeed},
  date = {2018},
  journaltitle = {International Journal of Applied Pattern Recognition},
  volume = {5},
  number = {2},
  pages = {119--136},
  publisher = {Inderscience Publishers (IEL)},
  keywords = {notion}
}

@article{feyziProgramSlicingBasedMethod2018,
  title = {A {{Program Slicing-Based Method}} for {{Effective Detection}} of {{Coincidentally Correct Test Cases}}},
  author = {Feyzi, Farid and Parsa, Saeed},
  date = {2018-09},
  journaltitle = {Computing},
  volume = {100},
  number = {9},
  pages = {927--969},
  publisher = {Springer-Verlag},
  location = {Berlin, Heidelberg},
  issn = {0010-485X},
  doi = {10.1007/s00607-018-0591-z},
  url = {https://doi.org/10.1007/s00607-018-0591-z},
  abstract = {Despite the proven applicability of the spectrum-based fault localization (SBFL) methods, their effectiveness may be degraded due to the presence of coincidental correctness, which occurs when faults fail to propagate, i.e., their execution does not result in failures. This article aims at improving SBFL effectiveness by mitigating the effect of coincidentally correct test cases. In this regard, given a test suite in which each test has been classified as failing or passing and each faulty program has a single-bug, we present a program slicing-based technique to identify a set of program entities that directly affect the program output when executed with failing test cases, called failure candidate causes (FCC). We then use FCC set to identify test cases that can be marked as being coincidentally correct. These tests are identified based on two heuristics: the average suspiciousness score of the statements that directly affect the program output and the coverage ratio of those statements. To evaluate our approach, we used several evaluation metrics and conducted extensive experiments on programs containing single and multiple bugs, including both real and seeded faults. The empirical results demonstrate that the proposed heuristics can alleviate the coincidental correctness problem and improve the accuracy of SBFL techniques.},
  keywords = {68N30,68Nxx,Coincidental correctness,Coverage based faults localization,EBDS,notion,Program slicing,Software testing}
}

@inproceedings{filieriQuantificationSoftwareChanges2015,
  title = {Quantification of {{Software Changes}} through {{Probabilistic Symbolic Execution}} ({{N}})},
  booktitle = {2015 30th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Filieri, Antonio and Pasareanu, Corina S. and Yang, Guowei},
  date = {2015},
  pages = {703--708},
  doi = {10.1109/ASE.2015.78},
  keywords = {notion}
}

@inproceedings{filieriReliabilityAnalysisSymbolic2013,
  title = {Reliability Analysis in {{Symbolic PathFinder}}},
  booktitle = {2013 35th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Filieri, Antonio and Păsăreanu, Corina S. and Visser, Willem},
  date = {2013-05},
  pages = {622--631},
  issn = {1558-1225},
  doi = {10.1109/ICSE.2013.6606608},
  url = {https://ieeexplore.ieee.org/abstract/document/6606608},
  urldate = {2024-10-29},
  abstract = {Software reliability analysis tackles the problem of predicting the failure probability of software. Most of the current approaches base reliability analysis on architectural abstractions useful at early stages of design, but not directly applicable to source code. In this paper we propose a general methodology that exploit symbolic execution of source code for extracting failure and success paths to be used for probabilistic reliability assessment against relevant usage scenarios. Under the assumption of finite and countable input domains, we provide an efficient implementation based on Symbolic PathFinder that supports the analysis of sequential and parallel programs, even with structured data types, at the desired level of confidence. The tool has been validated on both NASA prototypes and other test cases showing a promising applicability scope.},
  eventtitle = {2013 35th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  keywords = {Actuators,Instruction sets,Java,notion,Schedules,Software reliability},
  file = {/Users/bohrok/Zotero/storage/N7D4M2A4/Filieri et al. - 2013 - Reliability analysis in Symbolic PathFinder.pdf;/Users/bohrok/Zotero/storage/N2EKRBFU/6606608.html}
}

@inproceedings{filieriStatisticalSymbolicExecution2014,
  title = {Statistical {{Symbolic Execution}} with {{Informed Sampling}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGSOFT International Symposium}} on {{Foundations}} of {{Software Engineering}}},
  author = {Filieri, Antonio and Păsăreanu, Corina S. and Visser, Willem and Geldenhuys, Jaco},
  date = {2014},
  series = {{{FSE}} 2014},
  pages = {437--448},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2635868.2635899},
  url = {https://doi.org/10.1145/2635868.2635899},
  abstract = {Symbolic execution techniques have been proposed recently for the probabilistic analysis of programs. These techniques seek to quantify the likelihood of reaching program events of interest, e.g., assert violations. They have many promising applications but have scalability issues due to high computational demand. To address this challenge, we propose a statistical symbolic execution technique that performs Monte Carlo sampling of the symbolic program paths and uses the obtained information for Bayesian estimation and hypothesis testing with respect to the probability of reaching the target events. To speed up the convergence of the statistical analysis, we propose Informed Sampling, an iterative symbolic execution that first explores the paths that have high statistical significance, prunes them from the state space and guides the execution towards less likely paths. The technique combines Bayesian estimation with a partial exact analysis for the pruned paths leading to provably improved convergence of the statistical analysis. We have implemented statistical symbolic execution with informed sampling in the Symbolic PathFinder tool. We show experimentally that the informed sampling obtains more precise results and converges faster than a purely statistical analysis and may also be more efficient than an exact symbolic analysis. When the latter does not terminate symbolic execution with informed sampling can give meaningful results under the same time and memory limits.},
  isbn = {978-1-4503-3056-5},
  venue = {Hong Kong, China},
  keywords = {notion,Statistical Symbolic Execution}
}

@inproceedings{finkbeinerModelCheckingQuantitative2018,
  title = {Model Checking Quantitative Hyperproperties},
  booktitle = {Computer {{Aided Verification}}: 30th {{International Conference}}, {{CAV}} 2018, {{Held}} as {{Part}} of the {{Federated Logic Conference}}, {{FloC}} 2018, {{Oxford}}, {{UK}}, {{July}} 14-17, 2018, {{Proceedings}}, {{Part I}}},
  author = {Finkbeiner, Bernd and Hahn, Christopher and Torfah, Hazem},
  date = {2018},
  pages = {144--163},
  publisher = {Springer},
  keywords = {notion}
}

@inproceedings{fioraldiAFLCombiningIncremental2020,
  title = {{{AFL}}++: {{Combining}} Incremental Steps of Fuzzing Research},
  booktitle = {14th \{\vphantom\}{{USENIX}}\vphantom\{\} {{Workshop}} on {{Offensive Technologies}} (\{\vphantom\}{{WOOT}}\vphantom\{\} 20)},
  author = {Fioraldi, Andrea and Maier, Dominik and Eißfeldt, Heiko and Heuse, Marc},
  date = {2020},
  keywords = {notion}
}

@inproceedings{fioraldiUseLikelyInvariants2021,
  title = {The {{Use}} of {{Likely Invariants}} as {{Feedback}} for {{Fuzzers}}},
  booktitle = {30th {{USENIX Security Symposium}} ({{USENIX Security}} 21)},
  author = {Fioraldi, Andrea and D’Elia, Daniele Cono and Balzarotti, Davide},
  date = {2021-08},
  pages = {2829--2846},
  publisher = {USENIX Association},
  url = {https://www.usenix.org/conference/usenixsecurity21/presentation/fioraldi},
  isbn = {978-1-939133-24-3},
  keywords = {notion}
}

@inproceedings{fioraldiWEIZZAutomaticGreyBox2020,
  title = {{{WEIZZ}}: {{Automatic Grey-Box Fuzzing}} for {{Structured Binary Formats}}},
  booktitle = {Proceedings of the 29th {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Fioraldi, Andrea and D'Elia, Daniele Cono and Coppa, Emilio},
  date = {2020},
  series = {{{ISSTA}} 2020},
  pages = {1--13},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3395363.3397372},
  url = {https://doi.org/10.1145/3395363.3397372},
  abstract = {Fuzzing technologies have evolved at a fast pace in recent years, revealing bugs in programs with ever increasing depth and speed. Applications working with complex formats are however more difficult to take on, as inputs need to meet certain format-specific characteristics to get through the initial parsing stage and reach deeper behaviors of the program. Unlike prior proposals based on manually written format specifications, we propose a technique to automatically generate and mutate inputs for unknown chunk-based binary formats. We identify dependencies between input bytes and comparison instructions, and use them to assign tags that characterize the processing logic of the program. Tags become the building block for structure-aware mutations involving chunks and fields of the input. Our technique can perform comparably to structure-aware fuzzing proposals that require human assistance. Our prototype implementation WEIZZ revealed 16 unknown bugs in widely used programs.},
  isbn = {978-1-4503-8008-9},
  venue = {Virtual Event, USA},
  keywords = {binary testing,chunk-based formats,Fuzzing,notion,structural mutations}
}

@article{fisherRelationNumberSpecies1943,
  title = {The {{Relation Between}} the {{Number}} of {{Species}} and the {{Number}} of {{Individuals}} in a {{Random Sample}} of an {{Animal Population}}},
  author = {Fisher, R. A. and Corbet, A. Steven and Williams, C. B.},
  date = {1943},
  journaltitle = {Journal of Animal Ecology},
  volume = {12},
  number = {1},
  eprint = {1411},
  eprinttype = {jstor},
  pages = {42--58},
  publisher = {[Wiley, British Ecological Society]},
  issn = {00218790, 13652656},
  url = {http://www.jstor.org/stable/1411},
  urldate = {2023-05-05},
  abstract = {Part 1. It is shown that in a large collection of Lepidoptera captured in Malaya the frequency of the number of species represented by different numbers of individuals fitted somewhat closely to a hyperbola type of curve, so long as only the rarer species were considered. The data for the commoner species was not so strictly `randomized', but the whole series could be closely fitted by a series of the logarithmic type as described by Fisher in Part 3. Other data for random collections of insects in the field were also shown to fit fairly well to this series. Part 2. Extensive data on the capture of about 1500 Macrolepidoptera of about 240 species in a light-trap at Harpenden is analysed in relation to Fisher's mathematical theory and is shown to fit extremely closely to the calculations. The calculations are applied first to the frequency of occurrence of species represented by different numbers of individuals–and secondly to the number of species in samples of different sizes from the same population. The parameter ` alpha ', which it is suggested should be called the `index of diversity', is shown to have a regular seasonal change in the case of the Macrolepidoptera in the trap. In addition, samples from two traps which overlooked somewhat different vegetation are shown to have ` alpha ' values which are significantly different. It is shown that, provided the samples are not small, ` alpha ' is the increase in the number of species obtained by increasing the size of a sample by e (2.718). A diagram is given (Fig. 8) from which any one of the values, total number of species, total number of individuals and index of diversity (alpha), can be obtained approximately if the other two are known. The standard error of alpha is also indicated on the same diagram. Part 3. A theoretical distribution is developed which appears to be suitable for the frequencies with which different species occur in a random collection, in the common case in which many species are so rare that their chance of inclusion is small. The relationships of the new distribution with the negative binomial and the Poisson series are established. Numerical processes are exhibited for fitting the series to observations containing given numbers of species and individuals, and for estimating the parameter alpha representing the richness in species of the material sampled; secondly, for calculating the standard error of alpha, and thirdly, for testing whether the series exhibits a significant deviation from the limiting form used. Special tables are presented for facilitating these calculations.},
  keywords = {notion}
}

@article{fjelstadApplicationProgramMaintenance1979,
  title = {Application Program Maintenance Study: Report to Our Respondents},
  author = {family=Fjelstad, given=RK, given-i=RK and family=Hamlen, given=WT, given-i=WT},
  date = {1979},
  journaltitle = {Proceedings of GUIDE},
  volume = {48},
  keywords = {notion}
}

@inproceedings{florezAutomatedFinegrainedRequirementscode2019,
  title = {Automated {{Fine-grained Requirements-to-code Traceability Link Recovery}}},
  booktitle = {Proceedings of the 41st {{International Conference}} on {{Software Engineering}}: {{Companion Proceedings}}},
  author = {Florez, Juan Manuel},
  date = {2019},
  series = {{{ICSE}} '19},
  pages = {222--225},
  publisher = {IEEE Press},
  location = {Piscataway, NJ, USA},
  doi = {10.1109/ICSE-Companion.2019.00087},
  url = {https://doi.org/10.1109/ICSE-Companion.2019.00087},
  venue = {Montreal, Quebec, Canada},
  keywords = {discourse analysis,notion,qualitative analysis,static analysis,traceability}
}

@inproceedings{floydDecodingRepresentationCode2017,
  title = {Decoding the {{Representation}} of {{Code}} in the {{Brain}}: {{An fMRI Study}} of {{Code Review}} and {{Expertise}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Software Engineering}}},
  author = {Floyd, Benjamin and Santander, Tyler and Weimer, Westley},
  date = {2017},
  series = {{{ICSE}} '17},
  pages = {175--186},
  publisher = {IEEE Press},
  location = {Piscataway, NJ, USA},
  doi = {10.1109/ICSE.2017.24},
  url = {https://doi.org/10.1109/ICSE.2017.24},
  isbn = {978-1-5386-3868-2},
  venue = {Buenos Aires, Argentina},
  keywords = {Code Comprehension,Medical Imaging,notion,Prose Review}
}

@inproceedings{forreCausalCalculusPresence2020,
  title = {Causal Calculus in the Presence of Cycles, Latent Confounders and Selection Bias},
  booktitle = {Uncertainty in {{Artificial Intelligence}}},
  author = {Forré, Patrick and Mooij, Joris M},
  date = {2020},
  pages = {71--80},
  publisher = {PMLR},
  keywords = {notion}
}

@misc{forreConstraintbasedCausalDiscovery2018,
  title = {Constraint-Based {{Causal Discovery}} for {{Non-Linear Structural Causal Models}} with {{Cycles}} and {{Latent Confounders}}},
  author = {Forré, Patrick and Mooij, Joris M.},
  date = {2018},
  keywords = {causal discovery,notion}
}

@misc{forreMarkovPropertiesGraphical2017,
  title = {Markov {{Properties}} for {{Graphical Models}} with {{Cycles}} and {{Latent Variables}}},
  author = {Forré, Patrick and Mooij, Joris M.},
  date = {2017},
  keywords = {causal discovery,notion}
}

@misc{fosterCausalTestAdequacy2023,
  title = {Causal Test Adequacy},
  author = {Foster, M. and Wild, C. and Hierons, R. and Walkinshaw, N.},
  date = {2023-12},
  journaltitle = {17th IEEE International Conference on Software Testing, Verification and Validation (ICST) 2024},
  url = {https://eprints.whiterose.ac.uk/208652/},
  abstract = {Causal reasoning is becoming an increasingly popular technique for testing software. In this setting, the tester starts from a simple directed graph that captures their underlying understanding of causal relationships between relevant variables in the program, and this knowledge is then used to reason about causal input-output relationships that are observed during testing. One question that has not yet been addressed in this context is how to measure test adequacy: How do we know whether a causal relationship (or set of relationships) has been properly established by a test set? In this paper we present a metric inspired by Weyuker?s notion of inference adequacy. For a given causal relationship, we estimate the causal effect from the test data. The basis of our adequacy metric is then an estimate of the convergence of this estimate, which we calculate using statistical bootstrapping. We evaluate our metric on tests for three diverse computational models. The results show a statistically significant correlation between our metric and a test suite?s ability to detect mutants, and also that it is a good indicator of whether a sufficient number of system executions have been observed to trust the outcome of the test.},
  organization = {Institute of Electrical and Electronics Engineers (IEEE)},
  keywords = {causal inference,notion,software testing,test adequacy}
}

@inproceedings{francelRelationshipSlicingDebugging1999,
  title = {The Relationship of Slicing and Debugging to Program Understanding},
  booktitle = {Proceedings {{Seventh International Workshop}} on {{Program Comprehension}}},
  author = {Francel, M. A. and Rugaber, S.},
  date = {1999-05},
  pages = {106--113},
  issn = {1092-8138},
  doi = {10.1109/WPC.1999.777749},
  keywords = {code understanding,Computer science,debugging,Debugging,Educational institutions,Fault diagnosis,IEEE standard glossaries,Mathematics,minimal subprogram,notion,program debugging,program faults,program slicing,program understanding,reverse engineering,Terminology}
}

@article{francelValueSlicingDebugging2001,
  title = {The Value of Slicing While Debugging},
  author = {Francel, Margaret and Rugaber, Spencer},
  date = {2001-07},
  journaltitle = {Sci. Comput. Program.},
  volume = {40},
  pages = {151--169},
  doi = {10.1016/S0167-6423(01)00013-2},
  keywords = {notion}
}

@article{fraserWholeTestSuite2013,
  title = {Whole {{Test Suite Generation}}},
  author = {Fraser, G. and Arcuri, A.},
  date = {2013-02},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {39},
  number = {2},
  pages = {276--291},
  issn = {0098-5589},
  doi = {10.1109/TSE.2012.14},
  abstract = {Not all bugs lead to program crashes, and not always is there a formal specification to check the correctness of a software test's outcome. A common scenario in software testing is therefore that test data are generated, and a tester manually adds test oracles. As this is a difficult task, it is important to produce small yet representative test sets, and this representativeness is typically measured using code coverage. There is, however, a fundamental problem with the common approach of targeting one coverage goal at a time: Coverage goals are not independent, not equally difficult, and sometimes infeasible-the result of test generation is therefore dependent on the order of coverage goals and how many of them are feasible. To overcome this problem, we propose a novel paradigm in which whole test suites are evolved with the aim of covering all coverage goals at the same time while keeping the total size as small as possible. This approach has several advantages, as for example, its effectiveness is not affected by the number of infeasible targets in the code. We have implemented this novel approach in the EvoSuite tool, and compared it to the common approach of addressing one goal at a time. Evaluated on open source libraries and an industrial case study for a total of 1,741 classes, we show that EvoSuite achieved up to 188 times the branch coverage of a traditional approach targeting single branches, with up to 62 percent smaller test suites.},
  keywords = {Branch Coverage,EvoSuite,Genetic Algorithm (GA),notion,SBSE,Test Generation}
}

@inproceedings{FuzzBench,
  title = {{{FuzzBench}}: {{An}} Open Fuzzer Benchmarking Platform and Service},
  booktitle = {Proceedings of the 29th {{ACM}} Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  author = {Metzman, Jonathan and Szekeres, László and Maurice Romain Simon, Laurent and Trevelin Sprabery, Read and Arya, Abhishek},
  date = {2021},
  series = {Esec/Fse 2021},
  pages = {1393--1403},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3468264.3473932},
  url = {https://doi.org/10.1145/3468264.3473932},
  isbn = {978-1-4503-8562-6},
  pagetotal = {11},
  keywords = {notion}
}

@online{FuzzFactoryDomainspecificFuzzing,
  title = {{{FuzzFactory}}: Domain-Specific Fuzzing with Waypoints},
  url = {https://dl.acm.org/doi/10.1145/3360600},
  urldate = {2024-09-19},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/EF5AQKIH/3360600.html}
}

@inproceedings{galeGoodTuringSmoothingTears2001,
  title = {Good-{{Turing Smoothing Without Tears}}},
  author = {Gale, William A.},
  date = {2001},
  keywords = {notion}
}

@inproceedings{galhotraFairnessTestingTesting2017,
  title = {Fairness {{Testing}}: {{Testing Software}} for {{Discrimination}}},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Galhotra, Sainyam and Brun, Yuriy and Meliou, Alexandra},
  date = {2017},
  series = {{{ESEC}}/{{FSE}} 2017},
  pages = {498--510},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/3106237.3106277},
  url = {http://doi.acm.org/10.1145/3106237.3106277},
  isbn = {978-1-4503-5105-8},
  venue = {Paderborn, Germany},
  keywords = {Discrimination Testing,Fairness Testing,notion,Software Testing}
}

@article{gallagherUsingProgramSlicing1991,
  title = {Using Program Slicing in Software Maintenance},
  author = {Gallagher, K. B. and Lyle, J. R.},
  date = {1991-08},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {17},
  number = {8},
  pages = {751--761},
  issn = {2326-3881},
  doi = {10.1109/32.83912},
  keywords = {Computer science,Couplings,Lattices,line number,linear time,notion,program slice,program slicing,program testing,Programming profession,semantically consistent changes,set inclusion,single variable decomposition slices,slice-based decomposition,software maintenance,Software maintenance,software maintenance problem,Software testing,Software tools,unmodified components}
}

@inproceedings{gambiPracticalTestDependency2018,
  title = {Practical {{Test Dependency Detection}}},
  booktitle = {2018 {{IEEE}} 11th {{International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}} ({{ICST}})},
  author = {Gambi, A. and Bell, J. and Zeller, A.},
  date = {2018-04},
  pages = {1--11},
  doi = {10.1109/ICST.2018.00011},
  keywords = {Computer bugs,data-flow,detection algorithm,empirical study,flaky tests,Minimization,notion,Out of order,Pollution,practical test dependency detection,PRADET,problematic test dependencies,program testing,regression analysis,regression test selection,regression tests,Test dependence,Testing,Tools}
}

@inproceedings{ganCollaflPathSensitive2018,
  title = {Collafl: {{Path}} Sensitive Fuzzing},
  booktitle = {2018 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  author = {Gan, Shuitao and Zhang, Chao and Qin, Xiaojun and Tu, Xuwen and Li, Kang and Pei, Zhongyu and Chen, Zuoning},
  date = {2018},
  pages = {679--696},
  publisher = {IEEE},
  keywords = {notion}
}

@inproceedings{ganGREYONEDataFlow2020,
  title = {\{\vphantom\}{{GREYONE}}\vphantom\{\}: {{Data Flow Sensitive Fuzzing}}},
  booktitle = {29th \{\vphantom\}{{USENIX}}\vphantom\{\} {{Security Symposium}} (\{\vphantom\}{{USENIX}}\vphantom\{\} {{Security}} 20)},
  author = {Gan, Shuitao and Zhang, Chao and Chen, Peng and Zhao, Bodong and Qin, Xiaojun and Wu, Dong and Chen, Zuoning},
  date = {2020},
  pages = {2577--2594},
  keywords = {notion}
}

@inproceedings{gaoCoveragePlateauComprehensive2023,
  title = {Beyond the {{Coverage Plateau}}: {{A Comprehensive Study}} of {{Fuzz Blockers}} ({{Registered Report}})},
  booktitle = {Proceedings of the 2nd {{International Fuzzing Workshop}}},
  author = {Gao, Wentao and Pham, Van-Thuan and Liu, Dongge and Chang, Oliver and Murray, Toby and Rubinstein, Benjamin I.P.},
  date = {2023},
  series = {{{FUZZING}} 2023},
  pages = {47--55},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3605157.3605177},
  url = {https://doi.org/10.1145/3605157.3605177},
  abstract = {Fuzzing and particularly code coverage-guided greybox fuzzing is highly successful in automated vulnerability discovery, as evidenced by the multitude of vulnerabilities uncovered in real-world software systems. However, results on large benchmarks such as FuzzBench indicate that the state-of-the-art fuzzers often reach a plateau after a certain period, typically around 12 hours. With the aid of the newly introduced FuzzIntrospector platform, this study aims to analyze and categorize the fuzz blockers that impede the progress of fuzzers. Such insights can shed light on future fuzzing research, suggesting areas that require further attention. Our preliminary findings reveal that the majority of top fuzz blockers are not directly related to the program input, emphasizing the need for enhanced techniques in automated fuzz driver generation and modification.},
  isbn = {9798400702471},
  venue = {Seattle, WA, USA},
  keywords = {fuzzing,notion,software security,vulnerability detection}
}

@inproceedings{geethalHumanLoopOracleLearning2022,
  title = {Human-in-the-{{Loop Oracle Learning}} for {{Semantic Bugs}} in {{String Processing Programs}}},
  booktitle = {Symposium on {{Software Testing}} and {{Analysis}} ({{ISSTA}}'22)},
  author = {Geethal, Charaka and Pham, Van-Thuan and Aleti, Aldeida and Böhme, Marcel},
  date = {2022},
  keywords = {notion}
}

@inproceedings{geldenhuysProbabilisticSymbolicExecution2012,
  title = {Probabilistic {{Symbolic Execution}}},
  booktitle = {Proceedings of the 2012 {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Geldenhuys, Jaco and Dwyer, Matthew B. and Visser, Willem},
  date = {2012},
  series = {{{ISSTA}} 2012},
  pages = {166--176},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2338965.2336773},
  url = {https://doi.org/10.1145/2338965.2336773},
  abstract = {The continued development of efficient automated decision procedures has spurred the resurgence of research on symbolic execution over the past decade. Researchers have applied symbolic execution to a wide range of software analysis problems including: checking programs against contract specifications, inferring bounds on worst-case execution performance, and generating path-adequate test suites for widely used library code. In this paper, we explore the adaptation of symbolic execution to perform a more quantitative type of reasoning — the calculation of estimates of the probability of executing portions of a program. We present an extension of the widely used Symbolic PathFinder symbolic execution system that calculates path probabilities. We exploit state-of-the-art computational algebra techniques to count the number of solutions to path conditions, yielding exact results for path probabilities. To mitigate the cost of using these techniques, we present two optimizations, PC slicing and count memoization, that significantly reduce the cost of probabilistic symbolic execution. Finally, we present the results of an empirical evaluation applying our technique to challenging library container implementations and illustrate the benefits that adding probabilities to program analyses may offer.},
  isbn = {978-1-4503-1454-1},
  venue = {Minneapolis, MN, USA},
  keywords = {notion}
}

@article{gelmanWhatAreMost2021,
  title = {What Are the Most Important Statistical Ideas of the Past 50 Years?},
  author = {Gelman, Andrew and Vehtari, Aki},
  date = {2021},
  journaltitle = {Journal of the American Statistical Association},
  volume = {116},
  number = {536},
  pages = {2087--2097},
  publisher = {Taylor \& Francis},
  keywords = {notion}
}

@article{gerrardConditionalQuantitativeProgram2022,
  title = {Conditional {{Quantitative Program Analysis}}},
  author = {Gerrard, Mitchell and Borges, Mateus and Dwyer, Matthew B. and Filieri, Antonio},
  date = {2022},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {48},
  number = {4},
  pages = {1212--1227},
  doi = {10.1109/TSE.2020.3016778},
  keywords = {_failure,_probabilistic program analysis,_program analysis,_quantitative program analysis,_static analysis,notion}
}

@inproceedings{gertenTracebackFaultLocalization2024,
  title = {Traceback: {{A Fault Localization Technique}} for {{Molecular Programs}}},
  shorttitle = {Traceback},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Gerten, Michael C. and Lathrop, James I. and Cohen, Myra B.},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {415--427},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3652138},
  url = {https://dl.acm.org/doi/10.1145/3650212.3652138},
  urldate = {2024-09-21},
  abstract = {Fault localization is essential to software maintenance tasks such   as testing and automated program repair. Many fault localization   techniques have been developed, the most common of which are   spectrum-based. Most techniques have been designed for traditional programming paradigms that map passing and failing test   cases to lines or branches of code, hence specialized programming   paradigms which utilize different code abstractions may fail to localize well. In this paper, we study fault localization in the context   of a class of programs, molecular programs. Recent research has   designed automated testing and repair frameworks for these pro-   grams but has ignored the importance of fault localization. As we   demonstrate, using existing spectrum-based approaches may not   provide much information. Instead we propose a novel approach,   Traceback, that leverages temporal trace data. In an empirical study   on a set of 89 faulty program variants, we demonstrate that Trace-   back provides between a 32-90\% improvement in localization over   reaction-based mapping, a direct translation of spectrum-based   localization. We see little difference in parameter tuning of Trace-   back when all tests, or only code-based (invariant) tests are used,   however the best depth and weight parameters vary when using   specification based tests, which can be either functional or meta-   morphic. Overall, invariant-based tests provide the best localization   results (either alone or in combination with others), followed by   metamorphic and then functional tests.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/UMS4F93P/Gerten et al. - 2024 - Traceback A Fault Localization Technique for Molecular Programs.pdf}
}

@inproceedings{ghanbariDecompositionDeepNeural2024,
  title = {Decomposition of {{Deep Neural Networks}} into {{Modules}} via {{Mutation Analysis}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Ghanbari, Ali},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {1669--1681},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3680390},
  url = {https://dl.acm.org/doi/10.1145/3650212.3680390},
  urldate = {2024-09-21},
  abstract = {Recently, several approaches have been proposed for decomposing deep neural network (DNN) classifiers into binary classifier modules to facilitate modular development and repair of such models. These approaches concern only the problem of decomposing classifier models, and some of them rely on the activation patterns of the neurons, thereby limiting their applicability.     In this paper, we propose a DNN decomposition technique, named Incite, that uses neuron mutation to quantify the contributions of the neurons to a given output of a model. Then, for each model output, a subgraph induced by the nodes with highest contribution scores for that output are selected and extracted as a module. Incite is agnostic to the type of the model and the activation functions used in its construction, and is applicable to not just classifiers, but to regression models as well. Furthermore, the costs of mutation analysis in Incite has been reduced by heuristic clustering of neurons, enabling its application to models with millions of parameters. Lastly, Incite prunes away the neurons that do not contribute to the outcome of the modules, producing compressed, efficient modules.     We have evaluated Incite using 16 DNN models for well-known classification and regression problems and report its effectiveness along combined accuracy (and MAE) of the modules, the overlap in model elements between the modules, and the compression ratio. We observed that, for classification models, Incite, on average, incurs 3.44\% loss in accuracy, and the average overlap between the modules is 71.76\%, while the average compression ratio is 1.89X. Meanwhile, for regression models, Incite, on average, incurs 18.56\% gain in MAE, and the overlap between modules is 80.14\%, while the average compression ratio is 1.83X.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/DCJ7I33U/Ghanbari - 2024 - Decomposition of Deep Neural Networks into Modules via Mutation Analysis.pdf}
}

@inproceedings{ghanbariPatchCorrectnessAssessment2022,
  title = {Patch {{Correctness Assessment}} in {{Automated Program Repair Based}} on the {{Impact}} of {{Patches}} on {{Production}} and {{Test Code}}},
  booktitle = {Proceedings of the 31st {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}} ({{ISSTA}} '22)},
  author = {Ghanbari, Ali and Marcus, Andrian},
  date = {2022},
  pages = {12},
  publisher = {ACM},
  keywords = {notion}
}

@inproceedings{gholamianNaturalnessLocalnessSoftware2021,
  title = {On the {{Naturalness}} and {{Localness}} of {{Software Logs}}},
  booktitle = {2021 {{IEEE}}/{{ACM}} 18th {{International Conference}} on {{Mining Software Repositories}} ({{MSR}})},
  author = {Gholamian, Sina and Ward, Paul A. S.},
  date = {2021},
  pages = {155--166},
  doi = {10.1109/MSR52588.2021.00028},
  keywords = {notion}
}

@inproceedings{gligoricMutationTestingMeets2017,
  title = {Mutation {{Testing Meets Approximate Computing}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Software Engineering}}: {{New Ideas}} and {{Emerging Results Track}}},
  author = {Gligoric, Milos and Khurshid, Sarfraz and Misailovic, Sasa and Shi, August},
  date = {2017},
  series = {{{ICSE-NIER}} '17},
  pages = {3--6},
  publisher = {IEEE Press},
  location = {Piscataway, NJ, USA},
  doi = {10.1109/ICSE-NIER.2017.15},
  url = {https://doi.org/10.1109/ICSE-NIER.2017.15},
  isbn = {978-1-5386-2675-7},
  venue = {Buenos Aires, Argentina},
  keywords = {notion}
}

@inproceedings{godefroidDARTDirectedAutomated2005,
  title = {{{DART}}: {{Directed Automated Random Testing}}},
  booktitle = {Proceedings of the 2005 {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Godefroid, Patrice and Klarlund, Nils and Sen, Koushik},
  date = {2005},
  series = {{{PLDI}} '05},
  pages = {213--223},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1065010.1065036},
  url = {https://doi.org/10.1145/1065010.1065036},
  abstract = {We present a new tool, named DART, for automatically testing software that combines three main techniques: (1) automated extraction of the interface of a program with its external environment using static source-code parsing; (2) automatic generation of a test driver for this interface that performs random testing to simulate the most general environment the program can operate in; and (3) dynamic analysis of how the program behaves under random testing and automatic generation of new test inputs to direct systematically the execution along alternative program paths. Together, these three techniques constitute Directed Automated Random Testing, or DART for short. The main strength of DART is thus that testing can be performed completely automatically on any program that compiles – there is no need to write any test driver or harness code. During testing, DART detects standard errors such as program crashes, assertion violations, and non-termination. Preliminary experiments to unit test several examples of C programs are very encouraging.},
  isbn = {1-59593-056-6},
  venue = {Chicago, IL, USA},
  keywords = {automated test generation,interfaces,notion,program verification,random testing,software testing}
}

@article{godefroidDARTDirectedAutomated2005a,
  title = {{{DART}}: {{Directed Automated Random Testing}}},
  author = {Godefroid, Patrice and Klarlund, Nils and Sen, Koushik},
  date = {2005-06},
  journaltitle = {SIGPLAN Not.},
  volume = {40},
  number = {6},
  pages = {213--223},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {0362-1340},
  doi = {10.1145/1064978.1065036},
  url = {https://doi.org/10.1145/1064978.1065036},
  abstract = {We present a new tool, named DART, for automatically testing software that combines three main techniques: (1) automated extraction of the interface of a program with its external environment using static source-code parsing; (2) automatic generation of a test driver for this interface that performs random testing to simulate the most general environment the program can operate in; and (3) dynamic analysis of how the program behaves under random testing and automatic generation of new test inputs to direct systematically the execution along alternative program paths. Together, these three techniques constitute Directed Automated Random Testing, or DART for short. The main strength of DART is thus that testing can be performed completely automatically on any program that compiles – there is no need to write any test driver or harness code. During testing, DART detects standard errors such as program crashes, assertion violations, and non-termination. Preliminary experiments to unit test several examples of C programs are very encouraging.},
  keywords = {automated test generation,interfaces,notion,program verification,random testing,software testing}
}

@inproceedings{godefroidLearnfuzzMachineLearning2017,
  title = {Learn\&fuzz: {{Machine}} Learning for Input Fuzzing},
  booktitle = {2017 32nd {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Godefroid, Patrice and Peleg, Hila and Singh, Rishabh},
  date = {2017},
  pages = {50--59},
  publisher = {IEEE},
  keywords = {notion}
}

@inproceedings{golaghaCanWePredict2020,
  title = {Can {{We Predict}} the {{Quality}} of {{Spectrum-based Fault Localization}}?},
  booktitle = {{{IEEE International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}} ({{ICST}})},
  author = {Golagha, Mojdeh and Pretschner, Alexander and Briand, Lionel},
  date = {2020},
  keywords = {notion}
}

@inproceedings{goldAllowingOverlappingBoundaries2006,
  title = {Allowing {{Overlapping Boundaries}} in {{Source Code}} Using a {{Search Based Approach}} to {{Concept Binding}}},
  booktitle = {2006 22nd {{IEEE International Conference}} on {{Software Maintenance}}},
  author = {Gold, Nicolas and Harman, Mark and Li, Zheng and Mahdavi, Kiarash},
  date = {2006},
  pages = {310--319},
  doi = {10.1109/ICSM.2006.10},
  keywords = {notion}
}

@inproceedings{goldGeneralizedObservationalSlicing2017,
  title = {Generalized {{Observational Slicing}} for {{Tree-represented Modelling Languages}}},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Gold, Nicolas E. and Binkley, David and Harman, Mark and Islam, Syed and Krinke, Jens and Yoo, Shin},
  date = {2017},
  series = {{{ESEC}}/{{FSE}} 2017},
  pages = {547--558},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/3106237.3106304},
  url = {http://doi.acm.org/10.1145/3106237.3106304},
  isbn = {978-1-4503-5105-8},
  venue = {Paderborn, Germany},
  keywords = {MATLAB,notion,Observational Slicing,ORBS,Program Slicing,Simulink}
}

@inproceedings{goldHypothesisbasedConceptAssignment2001,
  title = {Hypothesis-Based Concept Assignment to Support Software Maintenance},
  booktitle = {Proceedings {{IEEE International Conference}} on {{Software Maintenance}}. {{ICSM}} 2001},
  author = {Gold, Nicolas},
  date = {2001},
  pages = {545--548},
  publisher = {IEEE},
  keywords = {notion}
}

@article{goldHypothesisbasedConceptAssignment2002,
  title = {Hypothesis-Based Concept Assignment in Software Maintenance},
  author = {Gold, Nicolas and Bennett, Keith},
  date = {2002},
  journaltitle = {IEE Proceedings-Software},
  volume = {149},
  number = {4},
  pages = {103--110},
  publisher = {IET},
  keywords = {notion}
}

@article{goldSearchBasedApproach2009,
  title = {A {{Search Based Approach}} for {{Overlapping Concept Boundaries}}},
  author = {Gold, Nicolas and Harman, Mark and Li, Zheng and Mahdavi, Kiarash},
  date = {2009-08},
  keywords = {notion}
}

@article{goldUnifyingProgramSlicing2005,
  title = {Unifying Program Slicing and Concept Assignment for Higher-Level Executable Source Code Extraction},
  author = {Gold, N. E. and Harman, M. and Binkley, D. and Hierons, R. M.},
  date = {2005},
  journaltitle = {Software: Practice and Experience},
  volume = {35},
  number = {10},
  pages = {977--1006},
  doi = {10.1002/spe.664},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.664},
  abstract = {Abstract Program slicing and concept assignment have both been proposed as source code extraction techniques. Unfortunately, each has a weakness that prevents wider application. For slicing, the extraction criterion is expressed at a very low level; constructing a slicing criterion requires detailed code knowledge which is often unavailable. The concept assignment extraction criterion is expressed at the domain level. However, unlike a slice, the extracted code is not executable as a separate subprogram in its own right. This paper introduces a unification of slicing and concept assignment which exploits their combined advantages, while overcoming these two individual weaknesses. Our `concept slices' are executable programs extracted using high-level criteria. The paper introduces four techniques that combine slicing and concept assignment and algorithms for each. These algorithms were implemented in two separate tools used to illustrate the application of the concept slicing algorithms in two very different case studies. The first is a commercially-written COBOL module from a large financial organization, the second is an open source utility program written in C. Copyright \textbackslash copyright 2005 John Wiley \& Sons, Ltd.},
  keywords = {concept assignment,notion,program slicing,reverse engineering}
}

@incollection{gomesModelCounting2021,
  title = {Model Counting},
  booktitle = {Handbook of Satisfiability},
  author = {Gomes, Carla P and Sabharwal, Ashish and Selman, Bart},
  date = {2021},
  pages = {993--1014},
  publisher = {IOS press},
  doi = {10.3233/FAIA201009},
  keywords = {notion}
}

@article{goncalvesComparativeStudyConcept2014,
  title = {A Comparative Study on Concept Drift Detectors},
  author = {Gonçalves, Paulo M. and Santos, Silas G. T. de Carvalho and Barros, Roberto S. M. and Vieira, Davi C. L.},
  date = {2014},
  journaltitle = {Expert Systems with Applications},
  volume = {41},
  number = {18},
  pages = {8144--8156},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2014.07.019},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417414004175},
  abstract = {In data stream environments, drift detection methods are used to identify when the context has changed. This paper evaluates eight different concept drift detectors (ddm, eddm, pht, stepd, dof, adwin, Paired Learners, and ecdd) and performs tests using artificial datasets affected by abrupt and gradual concept drifts, with several rates of drift, with and without noise and irrelevant attributes, and also using real-world datasets. In addition, a 2k factorial design was used to indicate the parameters that most influence performance which is a novelty in the area. Also, a variation of the Friedman non-parametric statistical test was used to identify the best methods. Experiments compared accuracy, evaluation time, as well as false alarm and miss detection rates. Additionally, we used the Mahalanobis distance to measure how similar the methods are when compared to the best possible detection output. This work can, to some extent, also be seen as a research survey of existing drift detection methods.},
  keywords = {Comparison,Concept drift detectors,Data streams,notion,Time-changing data}
}

@inproceedings{gongEffectsClassImbalance2012,
  title = {Effects of {{Class Imbalance}} in {{Test Suites}}: {{An Empirical Study}} of {{Spectrum-Based Fault Localization}}},
  booktitle = {2012 {{IEEE}} 36th {{Annual Computer Software}} and {{Applications Conference Workshops}}},
  author = {Gong, C. and Zheng, Z. and Li, W. and Hao, P.},
  date = {2012},
  pages = {470--475},
  doi = {10.1109/COMPSACW.2012.89},
  keywords = {notion}
}

@article{gongStateDependencyProbabilistic2015,
  title = {State Dependency Probabilistic Model for Fault Localization},
  author = {Gong, Dandan and Su, Xiaohong and Wang, Tiantian and Ma, Peijun and Yu, Wang},
  date = {2015},
  journaltitle = {Information and Software Technology},
  volume = {57},
  pages = {430--445},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2014.05.022},
  url = {http://www.sciencedirect.com/science/article/pii/S0950584914001396},
  abstract = {Context Fault localization is an important and expensive activity in software debugging. Previous studies indicated that statistically-based fault-localization techniques are effective in prioritizing the possible faulty statements with relatively low computational complexity, but prior works on statistical analysis have not fully investigated the behavior state information of each program element. Objective The objective of this paper is to propose an effective fault-localization approach based on the analysis of state dependence information between program elements. Method In this paper, state dependency is proposed to describe the control flow dependence between statements with particular states. A state dependency probabilistic model uses path profiles to analyze the state dependency information. Then, a fault-localization approach is proposed to locate faults by differentiating the state dependencies in passed and failed test cases. Results We evaluated the fault-localization effectiveness of our approach based on the experiments on Siemens programs and four UNIX programs. Furthermore, we compared our approach with current state-of-art fault-localization methods such as SOBER, Tarantula, and CP. The experimental results show that, our approach can locate more faults than the other methods in every range on Siemens programs, and the overall efficiency of our approach in the range of 10–30\% of analyzed source code is higher than the other methods on UNIX programs. Conclusion Our studies show that our approach consistently outperforms the other evaluated techniques in terms of effectiveness in fault localization on Siemens programs. Moreover, our approach is highly effective in fault localization even when very few test cases are available.},
  keywords = {Control flow graph,Fault localization,notion,Statistical analysis}
}

@article{goodNumberNewSpecies1956,
  title = {The {{Number}} of {{New Species}}, and the {{Increase}} in {{Population Coverage}}, When a {{Sample}} Is {{Increased}}},
  author = {Good, I. J. and Toulmin, G. H.},
  date = {1956},
  journaltitle = {Biometrika},
  volume = {43},
  number = {1/2},
  eprint = {2333577},
  eprinttype = {jstor},
  pages = {45--63},
  publisher = {[Oxford University Press, Biometrika Trust]},
  issn = {00063444},
  url = {http://www.jstor.org/stable/2333577},
  urldate = {2023-01-26},
  keywords = {notion}
}

@article{goodPopulationFrequenciesSpecies1953,
  title = {The {{Population Frequencies}} of {{Species}} and the {{Estimation}} of {{Population Parameters}}},
  author = {Good, I. J.},
  date = {1953},
  journaltitle = {Biometrika},
  volume = {40},
  number = {3/4},
  eprint = {2333344},
  eprinttype = {jstor},
  pages = {237--264},
  publisher = {[Oxford University Press, Biometrika Trust]},
  issn = {00063444},
  url = {http://www.jstor.org/stable/2333344},
  urldate = {2022-09-21},
  abstract = {A random sample is drawn from a population of animals of various species. (The theory may also be applied to studies of literary vocabulary, for example.) If a particular species is represented r times in the sample of size N, then r/N is not a good estimate of the population frequency, p, when r is small. Methods are given for estimating p, assuming virtually nothing about the underlying population. The estimates are expressed in terms of smoothed values of the numbers nr (r = 1, 2, 3...), where nr is the number of distinct species that are each represented r times in the sample. (nr may be described as `the frequency of the frequency r'.) Turing is acknowledged for the most interesting formula in this part of the work. An estimate of the proportion of the population represented by the species occurring in the sample is an immediate corollary. Estimates are made of measures of heterogeneity of the population, including Yule's characteristic' and Shannon's entropy'. Methods are then discussed that do depend on assumptions about the underlying population. It is here that most work has been done by other writers. It is pointed out that a hypothesis can give a good fit to the numbers nr but can give quite the wrong value for Yule's characteristic. An example of this is Fisher's fit to some data of Williams's on Macrolepidoptera.},
  keywords = {notion}
}

@online{GoogleWorkloadTraces2024,
  title = {Google {{Workload Traces}}},
  shorttitle = {Google {{Workload Traces}}},
  date = {2024-10-25},
  url = {https://dynamorio.org/google_workload_traces.html},
  urldate = {2024-10-27},
  organization = {Google Workload Traces}
}

@inproceedings{gopinathInputAlgebras2021,
  title = {Input {{Algebras}}},
  booktitle = {2021 {{IEEE}}/{{ACM}} 43rd {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Gopinath, Rahul and Nemati, Hamed and Zeller, Andreas},
  date = {2021},
  pages = {699--710},
  doi = {10.1109/ICSE43902.2021.00070},
  keywords = {notion}
}

@inproceedings{gopinathMiningInputGrammars2020,
  title = {Mining {{Input Grammars}} from {{Dynamic Control Flow}}},
  booktitle = {Proceedings of the 28th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Gopinath, Rahul and Mathis, Björn and Zeller, Andreas},
  date = {2020},
  series = {{{ESEC}}/{{FSE}} 2020},
  pages = {172--183},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3368089.3409679},
  url = {https://doi.org/10.1145/3368089.3409679},
  abstract = {One of the key properties of a program is its input specification. Having a formal input specification can be critical in fields such as vulnerability analysis, reverse engineering, software testing, clone detection, or refactoring. Unfortunately, accurate input specifications for typical programs are often unavailable or out of date. In this paper, we present a general algorithm that takes a program and a small set of sample inputs and automatically infers a readable context-free grammar capturing the input language of the program. We infer the syntactic input structure only by observing access of input characters at different locations of the input parser. This works on all stack based recursive descent input parsers, including parser combinators, and works entirely without program specific heuristics. Our Mimid prototype produced accurate and readable grammars for a variety of evaluation subjects, including complex languages such as JSON, TinyC, and JavaScript.},
  isbn = {978-1-4503-7043-1},
  venue = {Virtual Event, USA},
  keywords = {context-free grammar,control-flow,dataflow,dynamic analysis,fuzzing,notion}
}

@unpublished{gopinathSamplefreeLearningInput2018,
  title = {Sample-Free Learning of Input Grammars for Comprehensive Software Fuzzing},
  author = {Gopinath, Rahul and Mathis, Björn and Höschele, Mathias and Kampmann, Alexander and Zeller, Andreas},
  date = {2018},
  eprint = {1810.08289},
  eprinttype = {arXiv},
  keywords = {notion}
}

@inproceedings{gopinathTheoryCompositeFaults2017,
  title = {The {{Theory}} of {{Composite Faults}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}} ({{ICST}})},
  author = {Gopinath, R. and Jensen, C. and Groce, A.},
  date = {2017-03},
  pages = {47--57},
  doi = {10.1109/ICST.2017.12},
  abstract = {Fault masking happens when the effect of one fault serves to mask that of another fault for particular test inputs. The coupling effect is relied upon by testing practitioners to ensure that fault masking is rare. It states that complex faults are coupled to simple faults in such a way that a test data set that detects all simple faults in a program will detect a high percentage of the complex faults. While this effect has been empirically evaluated, our theoretical understanding of the coupling effect is as yet incomplete. Wah proposed a theory of the coupling effect on finite bijective (or near bijective) functions with the same domain and co-domain and assuming a uniform distribution for candidate functions. This model, however, was criticized as being too simple to model real systems, as it did not account for differing domain and co-domain in real programs, or for the syntactic neighborhood. We propose a new theory of fault coupling for general functions (with certain constraints). We show that there are two kinds of fault interactions, of which only the weak interaction can be modeled by the theory of the coupling effect. The strong interaction can produce faults that are semantically different from the original faults. These faults should hence be considered as independent atomic faults. Our analysis shows that the theory holds even when the effect of the syntactic neighborhood of the program is considered. We analyze numerous real-world programs with real faults to validate our hypothesis.},
  keywords = {Composite Faults,notion}
}

@article{goreCausalProgramSlicing2009,
  title = {Causal {{Program Slicing}}},
  author = {Gore, Ross and Reynolds, Paul F.},
  date = {2009},
  journaltitle = {2009 ACM/IEEE/SCS 23rd Workshop on Principles of Advanced and Distributed Simulation},
  pages = {19--26},
  keywords = {notion}
}

@inproceedings{goreReducingConfoundingBias2012,
  title = {Reducing {{Confounding Bias}} in {{Predicate-Level Statistical Debugging Metrics}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Software Engineering}}},
  author = {Gore, Ross and Reynolds, Jr., Paul F.},
  date = {2012},
  series = {{{ICSE}} '12},
  pages = {463--473},
  publisher = {IEEE Press},
  location = {Zurich, Switzerland},
  isbn = {978-1-4673-1067-3},
  keywords = {notion}
}

@inproceedings{goreStatisticalDebuggingElastic2011,
  title = {Statistical Debugging with Elastic Predicates},
  booktitle = {2011 26th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}} 2011)},
  author = {Gore, Ross and Reynolds, Paul F. and Kamensky, David},
  date = {2011},
  pages = {492--495},
  doi = {10.1109/ASE.2011.6100107},
  keywords = {notion}
}

@inproceedings{gottschlichThreePillarsMachine2018,
  title = {The {{Three Pillars}} of {{Machine Programming}}},
  booktitle = {Proceedings of the 2nd {{ACM SIGPLAN International Workshop}} on {{Machine Learning}} and {{Programming Languages}}},
  author = {Gottschlich, Justin and Solar-Lezama, Armando and Tatbul, Nesime and Carbin, Michael and Rinard, Martin and Barzilay, Regina and Amarasinghe, Saman and Tenenbaum, Joshua B. and Mattson, Tim},
  date = {2018},
  series = {{{MAPL}} 2018},
  pages = {69--80},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3211346.3211355},
  url = {https://doi.org/10.1145/3211346.3211355},
  abstract = {In this position paper, we describe our vision of the future of machine programming through a categorical examination of three pillars of research. Those pillars are: (i) intention, (ii) invention, and (iii) adaptation. Intention emphasizes advancements in the human-to-computer and computer-to-machine-learning interfaces. Invention emphasizes the creation or refinement of algorithms or core hardware and software building blocks through machine learning (ML). Adaptation emphasizes advances in the use of ML-based constructs to autonomously evolve software.},
  isbn = {978-1-4503-5834-7},
  venue = {Philadelphia, PA, USA},
  keywords = {adaptation,intention,invention,machine programming,notion,program synthesis,software development,software maintenance}
}

@article{gouesGenProgGenericMethod2012,
  title = {{{GenProg}}: {{A Generic Method}} for {{Automatic Software Repair}}},
  author = {Goues, C. Le and Nguyen, T. and Forrest, S. and Weimer, W.},
  date = {2012-01},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {38},
  number = {1},
  pages = {54--72},
  issn = {0098-5589},
  doi = {10.1109/TSE.2011.104},
  abstract = {This paper describes GenProg, an automated method for repairing defects in off-the-shelf, legacy programs without formal specifications, program annotations, or special coding practices. GenProg uses an extended form of genetic programming to evolve a program variant that retains required functionality but is not susceptible to a given defect, using existing test suites to encode both the defect and required functionality. Structural differencing algorithms and delta debugging reduce the difference between this variant and the original program to a minimal repair. We describe the algorithm and report experimental results of its success on 16 programs totaling 1.25 M lines of C code and 120K lines of module code, spanning eight classes of defects, in 357 seconds, on average. We analyze the generated repairs qualitatively and quantitatively to demonstrate that the process efficiently produces evolved programs that repair the defect, are not fragile input memorizations, and do not lead to serious degradation in functionality.},
  keywords = {Automated Program Repair,Genetic Algorithm (GA),GenProg,notion}
}

@inproceedings{gouesSystematicStudyAutomated2012,
  title = {A Systematic Study of Automated Program Repair: {{Fixing}} 55 out of 105 Bugs for \$8 Each},
  booktitle = {2012 34th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Goues, C. Le and Dewey-Vogt, M. and Forrest, S. and Weimer, W.},
  date = {2012-06},
  pages = {3--13},
  issn = {0270-5257},
  doi = {10.1109/ICSE.2012.6227211},
  abstract = {There are more bugs in real-world programs than human programmers can realistically address. This paper evaluates two research questions: “What fraction of bugs can be repaired automatically?” and “How much does it cost to repair a bug automatically?” In previous work, we presented GenProg, which uses genetic programming to repair defects in off-the-shelf C programs. To answer these questions, we: (1) propose novel algorithmic improvements to GenProg that allow it to scale to large programs and find repairs 68\% more often, (2) exploit GenProg's inherent parallelism using cloud computing resources to provide grounded, human-competitive cost measurements, and (3) generate a large, indicative benchmark set to use for systematic evaluations. We evaluate GenProg on 105 defects from 8 open-source programs totaling 5.1 million lines of code and involving 10,193 test cases. GenProg automatically repairs 55 of those 105 defects. To our knowledge, this evaluation is the largest available of its kind, and is often two orders of magnitude larger than previous work in terms of code or test suite size or defect count. Public cloud computing prices allow our 105 runs to be reproduced for 403; a successful repair completes in 96 minutes and costs 7.32, on average.},
  keywords = {Automated Program Repair,Genetic Algorithm (GA),GenProg,notion}
}

@misc{grammatechinc.CodeSurferSlicingSystem2002,
  title = {The {{CodeSurfer Slicing System}}},
  author = {{Grammatech Inc.}},
  date = {2002},
  url = {www.grammatech.com},
  keywords = {notion}
}

@unpublished{grassbergerEntropyEstimatesInsufficient2003,
  title = {Entropy Estimates from Insufficient Samplings},
  author = {Grassberger, Peter},
  date = {2003},
  eprint = {physics/0307138},
  eprinttype = {arXiv},
  keywords = {notion}
}

@article{gravesHybridComputingUsing2016,
  title = {Hybrid Computing Using a Neural Network with Dynamic External Memory},
  author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-BarwiÅ„ska, Agnieszka and Colmenarejo, Sergio GÃ³mez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, AdriÃPuigdomÃ¨nech and Hermann, Karl Moritz and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
  year = {2016/10/27/print},
  journaltitle = {Nature},
  volume = {538},
  number = {7626},
  pages = {471--476},
  publisher = {Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  url = {http://dx.doi.org/10.1038/nature20101},
  abstract = {Artificial neural networks are remarkably adept at sensory processing, sequence learning and reinforcement learning, but are limited in their ability to represent variables and data structures and to store data over long timescales, owing to the lack of an external memory. Here we introduce a machine learning model called a differentiable neural computer (DNC), which consists of a neural network that can read from and write to an external memory matrix, analogous to the random-access memory in a conventional computer. Like a conventional computer, it can use its memory to represent and manipulate complex data structures, but, like a neural network, it can learn to do so from data. When trained with supervised learning, we demonstrate that a DNC can successfully answer synthetic questions designed to emulate reasoning and inference problems in natural language. We show that it can learn tasks such as finding the shortest path between specified points and inferring the missing links in randomly generated graphs, and then generalize these tasks to specific graphs such as transport networks and family trees. When trained with reinforcement learning, a DNC can complete a moving blocks puzzle in which changing goals are specified by sequences of symbols. Taken together, our results demonstrate that DNCs have the capacity to solve complex, structured tasks that are inaccessible to neural networks without external readâ€“write memory.},
  isbn = {0028-0836},
  keywords = {Differentiable Neural Computer (DNC),Neural Network (NN),notion}
}

@misc{graziotinBehavioralSoftwareEngineering2020,
  title = {Behavioral {{Software Engineering}}: {{Methodological Introduction}} to {{Psychometrics}}},
  author = {Graziotin, Daniel and Lenberg, Per and Feldt, Robert and Wagner, Stefan},
  date = {2020},
  keywords = {notion}
}

@inproceedings{greenGraphFuzzLibraryAPI2022,
  title = {{{GraphFuzz}}: {{Library API Fuzzing}} with {{Lifetime-Aware Dataflow Graphs}}},
  booktitle = {Proceedings of the 44th {{International Conference}} on {{Software Engineering}}},
  author = {Green, Harrison and Avgerinos, Thanassis},
  date = {2022},
  series = {{{ICSE}} '22},
  pages = {1070--1081},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3510003.3510228},
  url = {https://doi.org/10.1145/3510003.3510228},
  abstract = {We present the design and implementation of GraphFuzz, a new structure-, coverage- and object lifetime-aware fuzzer capable of automatically testing low-level Library APIs. Unlike other fuzzers, GraphFuzz models sequences of executed functions as a dataflow graph, thus enabling it to perform graph-based mutations both at the data and at the execution trace level. GraphFuzz comes with an automated specification generator to minimize the developer integration effort.We use GraphFuzz to analyze Skia—the rigorously tested Google Chrome graphics library—and benchmark GraphFuzz-generated fuzzing harnesses against hand-optimized, painstakingly written libFuzzer harnesses. We find that GraphFuzz generates test cases that achieve 2–3x more code coverage on average with minimal development effort, and also uncovered previous unknown defects in the process. We demonstrate GraphFuzz's applicability on low-level APIs by analyzing four additional open-source libraries and finding dozens of previously unknown defects. All security relevant findings have already been reported and fixed by the developers.Last, we open-source GraphFuzz under a permissive license and provide code to reproduce all results in this paper.},
  isbn = {978-1-4503-9221-1},
  venue = {Pittsburgh, Pennsylvania},
  keywords = {notion}
}

@article{griffithsFindingScientificTopics2004,
  title = {Finding Scientific Topics},
  author = {Griffiths, Thomas L. and Steyvers, Mark},
  date = {2004},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {101},
  pages = {5228--5235},
  doi = {10.1073/pnas.0307752101},
  url = {http://www.pnas.org/content/101/suppl_1/5228.abstract},
  abstract = {A first step in identifying the content of a document is determining which topics that document addresses. We describe a generative model for documents, introduced by Blei, Ng, and Jordan [Blei, D. M., Ng, A. Y. \&amp; Jordan, M. I. (2003) J. Machine Learn. Res. 3, 993-1022], in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. We then present a Markov chain Monte Carlo algorithm for inference in this model. We use this algorithm to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics. We show that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying “hot topics” by examining temporal dynamics and tagging abstracts to illustrate semantic content.},
  issue = {suppl 1},
  keywords = {Latent Semantic Indexing (LSI),notion}
}

@inproceedings{grozActiveInferenceExtended2023,
  title = {Active {{Inference}} of {{Extended Finite State Models}} of {{Software Systems}}},
  booktitle = {Proceedings of 16th Edition of the {{International Conference}} on {{Grammatical Inference}}},
  author = {Groz, Roland and Oriat, Catherine and Vega, Germán and Simao, Adenilso and Foster, Michael and Walkinshaw, Neil},
  editor = {Coste, François and Ouardi, Faissal and Rabusseau, Guillaume},
  date = {2023-07-10},
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {217},
  pages = {265--269},
  publisher = {PMLR},
  url = {https://proceedings.mlr.press/v217/groz23a.html},
  abstract = {Extended finite state machines (EFSMs) model stateful systems with internal data variables, and have many software engineering applications. It is possible to infer such models by observing system behaviour. Still, existing approaches are either limited to classical FSM models with no internal data state, or implicitly require the ability to reset the system under inference, which may not always be possible. We present an extension to the hW-inference algorithm that can infer EFSM models, with input and output parameters as well as guards and internal registers and their data update functions, from systems without a reliable reset. For the problem to be tractable, we require some assumptions on the observability and determinism of the system. The main restriction is that the control flow of the system must be finite, although data types could be infinite.},
  keywords = {notion}
}

@article{grundCodeShovelConstructingMethodLevel2021,
  title = {{{CodeShovel}}: {{Constructing Method-Level Source Code Histories}}},
  author = {Grund, Felix and Chowdhury, Shaiful Alam and Bradley, Nick C. and Hall, Braxton and Holmes, Reid},
  date = {2021},
  journaltitle = {2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},
  pages = {1510--1522},
  keywords = {notion}
}

@inproceedings{guanDepthStudyRuntime2024,
  title = {An {{In-Depth Study}} of {{Runtime Verification Overheads}} during {{Software Testing}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Guan, Kevin and Legunsen, Owolabi},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {1798--1810},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3680400},
  url = {https://dl.acm.org/doi/10.1145/3650212.3680400},
  urldate = {2024-09-21},
  abstract = {Runtime verification (RV) monitors program executions against formal specifications (specs). Researchers showed that RV during software testing amplifies the bug-finding ability of tests, and found hundreds of new bugs by using RV to monitor passing tests in open-source projects. But, RV’s runtime overhead is widely seen as a hindrance to its broad adoption, especially during continuous integration. Yet, there is no in-depth study of the prevalence, usefulness for bug finding, and components of these overheads during testing, so that researchers can better understand how to speed up RV.       We study RV overhead during testing, monitoring developer-written unit tests in 1,544 open-source projects against 160 specs of correct JDK API usage. We make four main findings. (1) RV overhead is below 12.48 seconds, which others considered acceptable, in 40.9\% of projects, but up to 5,002.9x (or, 28.7 hours) in the other projects. (2) 99.87\% of monitors that RV generates to dynamically check program traces are wasted; they can only find bugs that the other 0.13\% find. (3) Contrary to conventional wisdom, RV overhead in most projects is dominated by instrumentation, not monitoring. (4) 36.74\% of monitoring time is spent in test code or libraries.       As evidence that our study provides a new basis that future work can exploit, we perform two more experiments. First, we show that offline instrumentation (when possible) greatly reduces RV runtime overhead for single versions of many projects. Second, we show that simply amortizing high instrumentation costs across multiple program versions can outperform, by up to 4.53x, a recent evolution-aware RV technique that uses complex program analysis.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/VYKZZ354/Guan and Legunsen - 2024 - An In-Depth Study of Runtime Verification Overheads during Software Testing.pdf}
}

@inproceedings{guimaraesOptimizingMutationTesting2020,
  title = {Optimizing {{Mutation Testing}} by {{Discovering Dynamic Mutant Subsumption Relations}}},
  booktitle = {2020 {{IEEE}} 13th {{International Conference}} on {{Software Testing}}, {{Validation}} and {{Verification}} ({{ICST}})},
  author = {Guimarães, M. A. and Fernandes, L. and Ribeiro, M. and family=Amorim, given=M., prefix=d', useprefix=true and Gheyi, R.},
  date = {2020},
  pages = {198--208},
  doi = {10.1109/ICST46399.2020.00029},
  keywords = {notion}
}

@article{guoSurveyLearningCausality2020,
  title = {A {{Survey}} of {{Learning Causality}} with {{Data}}: {{Problems}} and {{Methods}}},
  author = {Guo, Ruocheng and Cheng, Lu and Li, Jundong and Hahn, P. Richard and Liu, Huan},
  date = {2020-07},
  journaltitle = {ACM Comput. Surv.},
  volume = {53},
  number = {4},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {0360-0300},
  doi = {10.1145/3397269},
  url = {https://doi.org/10.1145/3397269},
  abstract = {This work considers the question of how convenient access to copious data impacts our ability to learn causal effects and relations. In what ways is learning causality in the era of big data different from—or the same as—the traditional one? To answer this question, this survey provides a comprehensive and structured review of both traditional and frontier methods in learning causality and relations along with the connections between causality and machine learning. This work points out on a case-by-case basis how big data facilitates, complicates, or motivates each approach.},
  keywords = {causal discovery,causal inference,Causal machine learning,notion}
}

@inproceedings{guPracticalGUITesting2019,
  title = {Practical {{GUI Testing}} of {{Android Applications}} via {{Model Abstraction}} and {{Refinement}}},
  booktitle = {Proceedings of the 41st {{International Conference}} on {{Software Engineering}}},
  author = {Gu, Tianxiao and Sun, Chengnian and Ma, Xiaoxing and Cao, Chun and Xu, Chang and Yao, Yuan and Zhang, Qirun and Lu, Jian and Su, Zhendong},
  date = {2019},
  series = {{{ICSE}} '19},
  pages = {269--280},
  publisher = {IEEE Press},
  location = {Montreal, Quebec, Canada},
  doi = {10.1109/ICSE.2019.00042},
  url = {https://doi.org/10.1109/ICSE.2019.00042},
  abstract = {This paper introduces a new, fully automated model-based approach for effective testing of Android apps. Different from existing model-based approaches that guide testing with a static GUI model (i.e., the model does not evolve its abstraction during testing, and is thus often imprecise), our approach dynamically optimizes the model by leveraging the runtime information during testing. This capability of model evolution significantly improves model precision, and thus dramatically enhances the testing effectiveness compared to existing approaches, which our evaluation confirms. We have realized our technique in a practical tool, Ape. On 15 large, widely-used apps from the Google Play Store, Ape outperforms the state-of-the-art Android GUI testing tools in terms of both testing coverage and the number of detected unique crashes. To further demonstrate Ape's effectiveness and usability, we conduct another evaluation of Ape on 1,316 popular apps, where it found 537 unique crashes. Out of the 38 reported crashes, 13 have been fixed and 5 have been confirmed.},
  keywords = {CEGAR,GUI testing,mobile app testing,notion}
}

@inproceedings{guptaDeepFixFixingCommon2017,
  title = {{{DeepFix}}: {{Fixing Common C Language Errors}} by {{Deep Learning}}},
  booktitle = {{{AAAI}}},
  author = {Gupta, Rahul and Pal, Soham and Kanade, Aditya and Shevade, Shirish K.},
  date = {2017},
  keywords = {notion}
}

@inproceedings{gustafssonMalardalenWCETBenchmarks2010,
  title = {The {{Mälardalen WCET}} Benchmarks: {{Past}}, Present and Future},
  booktitle = {10th {{International Workshop}} on {{Worst-Case Execution Time Analysis}} ({{WCET}} 2010)},
  author = {Gustafsson, Jan and Betts, Adam and Ermedahl, Andreas and Lisper, Björn},
  date = {2010},
  publisher = {Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik},
  keywords = {notion}
}

@inproceedings{habchiWhatMadeThis2022,
  title = {What {{Made This Test Flake}}? {{Pinpointing Classes Responsible}} for {{Test Flakiness}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Software Maintenance}} and {{Evolution}} ({{ICSME}})},
  author = {Habchi, Sarra and Haben, Guillaume and Sohn, Jeongju and Franci, Adriano and Papadakis, Mike and Cordy, Maxime and Traon, Yves Le},
  date = {2022},
  pages = {352--363},
  doi = {10.1109/ICSME55016.2022.00039},
  keywords = {Codes,Fault diagnosis,fault localisation,Flaky tests,Java,Measurement,notion,root cause analysis,Root cause analysis,Soft sensors,Software maintenance}
}

@online{haddoucheOnlinePACBayesLearning2022,
  title = {Online {{PAC-Bayes Learning}}},
  author = {Haddouche, Maxime and Guedj, Benjamin},
  date = {2022-10-13},
  eprint = {2206.00024},
  eprinttype = {arXiv},
  eprintclass = {cs, math, stat},
  doi = {10.48550/arXiv.2206.00024},
  url = {http://arxiv.org/abs/2206.00024},
  urldate = {2024-09-20},
  abstract = {Most PAC-Bayesian bounds hold in the batch learning setting where data is collected at once, prior to inference or prediction. This somewhat departs from many contemporary learning problems where data streams are collected and the algorithms must dynamically adjust. We prove new PAC-Bayesian bounds in this online learning framework, leveraging an updated definition of regret, and we revisit classical PAC-Bayesian results with a batch-to-online conversion, extending their remit to the case of dependent data. Our results hold for bounded losses, potentially \textbackslash emph\{non-convex\}, paving the way to promising developments in online learning.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,notion,Statistics - Machine Learning},
  file = {/Users/bohrok/Zotero/storage/ZTRMANKC/Haddouche and Guedj - 2022 - Online PAC-Bayes Learning.pdf;/Users/bohrok/Zotero/storage/HCQN97PF/2206.html}
}

@article{hajnalDemanddrivenApproachSlicing2012,
  title = {A Demand-Driven Approach to Slicing Legacy {{COBOL}} Systems},
  author = {Hajnal, Ákos and Forgács, István},
  date = {2012-01},
  journaltitle = {Journal of Software Maintenance},
  volume = {24},
  pages = {67--82},
  doi = {10.1002/smr.533},
  keywords = {notion}
}

@misc{hanComparisonCodeEmbeddings2021,
  title = {A {{Comparison}} of {{Code Embeddings}} and {{Beyond}}},
  author = {Han, Siqi and Wang, DongXia and Li, Wanting and Lu, Xuesong},
  date = {2021},
  keywords = {notion}
}

@article{hanleyIfNothingGoes1983,
  title = {If Nothing Goes Wrong, Is Everything All Right? {{Interpreting}} Zero Numerators.},
  author = {Hanley, James A. and Lippman‐Hand, Abby},
  date = {1983},
  journaltitle = {JAMA},
  volume = {249 13},
  pages = {1743--5},
  keywords = {notion}
}

@article{haoEliminatingHarmfulRedundancy2005,
  title = {Eliminating Harmful Redundancy for Testing-Based Fault Localization Using Test Suite Reduction: An Experimental Study},
  author = {Hao, Dan and Zhang, Lu and Zhong, Hao and Mei, Hong and Sun, Jiasu},
  date = {2005},
  journaltitle = {21st IEEE International Conference on Software Maintenance (ICSM'05)},
  pages = {683--686},
  keywords = {notion}
}

@inproceedings{haoOptimalPredictionNumber2020,
  title = {Optimal {{Prediction}} of the {{Number}} of {{Unseen Species}} with {{Multiplicity}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hao, Yi and Li, Ping},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  date = {2020},
  volume = {33},
  pages = {8553--8564},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2020/file/618790ae971abb5610b16c826fb72d01-Paper.pdf},
  keywords = {notion}
}

@article{haoTestInputReduction2009,
  title = {Test Input Reduction for Result Inspection to Facilitate Fault Localization},
  author = {Hao, Dan and Xie, Tao and Zhang, Lu and Wang, Xiaoyin and Sun, Jiasu and Mei, Hong},
  date = {2009},
  journaltitle = {Automated Software Engineering},
  volume = {17},
  pages = {5--31},
  keywords = {notion}
}

@article{hardekopfFlowsensitivePointerAnalysis2011,
  title = {Flow-Sensitive Pointer Analysis for Millions of Lines of Code},
  author = {Hardekopf, Ben and Lin, Calvin},
  date = {2011},
  journaltitle = {International Symposium on Code Generation and Optimization (CGO 2011)},
  pages = {289--298},
  keywords = {notion}
}

@article{harmanAmorphousProgramSlicing2003,
  title = {Amorphous Program Slicing},
  author = {Harman, Mark and Binkley, David and Danicic, Sebastian},
  date = {2003},
  journaltitle = {Journal of Systems and Software},
  volume = {68},
  number = {1},
  pages = {45--64},
  issn = {0164-1212},
  doi = {10.1016/S0164-1212(02)00135-8},
  url = {http://www.sciencedirect.com/science/article/pii/S0164121202001358},
  abstract = {Traditional, syntax-preserving program slicing simplifies a program by deleting components (e.g., statements and predicates) that do not affect a computation of interest. Amorphous slicing removes the limitation to component deletion as the only means of simplification, while retaining the semantic property that a slice preserves the selected behaviour of interest from the original program. This leads to slices which are often considerably smaller than their syntax-preserving counterparts. A formal framework is introduced to define and compare amorphous and traditional program slicing. After this definition, an algorithm for computing amorphous slices, based on the system dependence graph, is presented. An implementation of this algorithm is used to demonstrate the utility of amorphous slicing with respect to code-level analysis of array access safety. The resulting empirical study indicates that programmers' comprehension of array safety is improved by amorphous slicing.},
  keywords = {notion,Program comprehension,Program dependence graph,Program slicing,Program transformation}
}

@article{harmanCloudEngineeringSearch2013,
  title = {Cloud Engineering Is {{Search Based Software Engineering}} Too},
  author = {Harman, Mark and Lakhotia, Kiran and Singer, Jeremy and White, David R. and Yoo, Shin},
  date = {2013},
  journaltitle = {Journal of Systems and Software},
  volume = {86},
  number = {9},
  pages = {2225--2241},
  keywords = {Cloud Computing,notion,SBSE}
}

@inproceedings{harmanCodeExtractionAlgorithms2002,
  title = {Code Extraction Algorithms Which Unify Slicing and Concept Assignment},
  booktitle = {Ninth {{Working Conference}} on {{Reverse Engineering}}, 2002. {{Proceedings}}.},
  author = {Harman, M. and Gold, N. and Hierons, R. and Binkley, D.},
  date = {2002},
  pages = {11--20},
  doi = {10.1109/WCRE.2002.1173060},
  keywords = {notion}
}

@inproceedings{harmanCurrentStateFuture2007,
  title = {The {{Current State}} and {{Future}} of {{Search Based Software Engineering}}},
  booktitle = {2007 {{Future}} of {{Software Engineering}}},
  author = {Harman, Mark},
  date = {2007},
  series = {{{FOSE}} '07},
  pages = {342--357},
  publisher = {IEEE Computer Society},
  location = {Washington, DC, USA},
  doi = {10.1109/FOSE.2007.29},
  url = {http://dx.doi.org/10.1109/FOSE.2007.29},
  isbn = {0-7695-2829-5},
  keywords = {notion,SBSE}
}

@article{harmanExactScalableSensitivity2014,
  title = {Exact {{Scalable Sensitivity Analysis}} for the {{Next Release Problem}}},
  author = {Harman, Mark and Krinke, Jens and Medina-Bulo, Inmaculada and Palomo-Lozano, Francisco and Ren, Jian and Yoo, Shin},
  date = {2014},
  journaltitle = {ACM Transactions on Software Engineering and Methodology},
  volume = {23},
  number = {2},
  pages = {19:1--19:31},
  keywords = {Next Release Problem,notion}
}

@inproceedings{harmanGeneticProgrammingReverse2013,
  title = {Genetic Programming for {{Reverse Engineering}}},
  booktitle = {2013 20th {{Working Conference}} on {{Reverse Engineering}} ({{WCRE}})},
  author = {Harman, Mark and Langdon, William B. and Weimer, Westley},
  date = {2013},
  pages = {1--10},
  doi = {10.1109/WCRE.2013.6671274},
  keywords = {notion}
}

@article{harmanOverviewProgramSlicing2001,
  title = {An Overview of Program Slicing},
  author = {Harman, Mark and Hierons, Robert},
  date = {2001},
  journaltitle = {Software Focus},
  volume = {2},
  number = {3},
  pages = {85--92},
  doi = {10.1002/swf.41},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/swf.41},
  abstract = {Abstract MARK HARMAN and ROBERT HIERONS review three semantic paradigms for slicing — static, dynamic and conditioned; and two syntactic paradigms — syntax-preserving and amorphous. Slicing has been applied to many software development problems including testing, reuse, maintenance and evolution. This paper describes the main forms of program slice and some of the applications to which slicing has been put. Copyright \textbackslash copyright 2001 John Wiley \& Sons, Ltd.},
  keywords = {notion,program slicing,software evolution,software maintenance,software reuse,software testing}
}

@article{harmanSearchbasedSoftwareEngineering2012,
  title = {Search-Based {{Software Engineering}}: {{Trends}}, {{Techniques}} and {{Applications}}},
  author = {Harman, Mark and Mansouri, S. Afshin and Zhang, Yuanyuan},
  date = {2012-12},
  journaltitle = {ACM Comput. Surv.},
  volume = {45},
  number = {1},
  pages = {11:1--11:61},
  publisher = {ACM},
  location = {New York, NY, USA},
  issn = {0360-0300},
  doi = {10.1145/2379776.2379787},
  url = {http://doi.acm.org/10.1145/2379776.2379787},
  keywords = {notion,SBSE}
}

@inproceedings{harmanStartupsScaleupsOpportunities2018,
  title = {From {{Start-ups}} to {{Scale-ups}}: {{Opportunities}} and {{Open Problems}} for {{Static}} and {{Dynamic Program Analysis}}},
  booktitle = {2018 {{IEEE}} 18th {{International Working Conference}} on {{Source Code Analysis}} and {{Manipulation}} ({{SCAM}})},
  author = {Harman, M. and O'Hearn, P.},
  date = {2018-09},
  pages = {1--23},
  doi = {10.1109/SCAM.2018.00009},
  keywords = {authors experience,compositional reasoning,dynamic analysis,dynamic program analysis,Facebook,flaky test,Industries,Infer,notion,open problems,opportunities,program diagnostics,Prototypes,Sapienz,SBSE,scale-ups,scientific community,Separation Logic,significant attention,social networking (online),Software,start-ups,Static analysis,static program analysis,testing,Testing,Tools,verification}
}

@article{harmanTestabilityTransformation2004,
  title = {Testability Transformation},
  author = {Harman, Mark and Hu, Lin and Hierons, Rob and Wegener, Joachim and Sthamer, Harmen and Baresel, André and Roper, Marc},
  date = {2004},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {30},
  number = {1},
  pages = {3--16},
  publisher = {IEEE},
  keywords = {notion}
}

@inproceedings{harmanTheoreticalEmpiricalAnalysis2007,
  title = {A {{Theoretical}} \& {{Empirical Analysis}} of {{Evolutionary Testing}} and {{Hill Climbing}} for {{Structural Test Data Generation}}},
  booktitle = {Proceedings of the 2007 {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Harman, Mark and McMinn, Phil},
  date = {2007},
  series = {{{ISSTA}} '07},
  pages = {73--83},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/1273463.1273475},
  url = {http://doi.acm.org/10.1145/1273463.1273475},
  isbn = {978-1-59593-734-6},
  venue = {London, United Kingdom},
  keywords = {Alternating Variable Method (AVM),Genetic Algorithm (GA),notion,SBST}
}

@article{hazimehMagmaGroundtruthFuzzing2020,
  title = {Magma: {{A}} Ground-Truth Fuzzing Benchmark},
  author = {Hazimeh, Ahmad and Herrera, Adrian and Payer, Mathias},
  date = {2020},
  journaltitle = {Proceedings of the ACM on Measurement and Analysis of Computing Systems},
  volume = {4},
  number = {3},
  pages = {1--29},
  publisher = {ACM New York, NY, USA},
  keywords = {notion}
}

@inproceedings{headManagingMessesComputational2019,
  title = {Managing {{Messes}} in {{Computational Notebooks}}},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Head, Andrew and Hohman, Fred and Barik, Titus and Drucker, Steven M. and DeLine, Robert},
  date = {2019},
  series = {{{CHI}} '19},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3290605.3300500},
  url = {https://doi.org/10.1145/3290605.3300500},
  isbn = {978-1-4503-5970-2},
  venue = {Glasgow, Scotland Uk},
  keywords = {clutter,code history,computational notebooks,exploratory programming,inconsistency,messes,notion,program slicing}
}

@article{heckerTimeSensitiveControlDependencies2021,
  title = {On {{Time-Sensitive Control Dependencies}}},
  author = {Hecker, Martin and Bischof, Simon and Snelting, Gregor},
  date = {2021-12},
  journaltitle = {ACM Trans. Program. Lang. Syst.},
  volume = {44},
  number = {1},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {0164-0925},
  doi = {10.1145/3486003},
  url = {https://doi.org/10.1145/3486003},
  abstract = {We present efficient algorithms for time-sensitive control dependencies (CDs). If statement y is time-sensitively control dependent on statement x, then x decides not only whether y is executed but also how many timesteps after x. If y is not standard control dependent on x, but time-sensitively control dependent, then y will always be executed after x, but the execution time between x and y varies. This allows us to discover, e.g., timing leaks in security-critical software.We systematically develop properties and algorithms for time-sensitive CDs, as well as for nontermination-sensitive CDs. These work not only for standard control flow graphs (CFGs) but also for CFGs lacking a unique exit node (e.g., reactive systems). We show that Cytron's efficient algorithm for dominance frontiers [10] can be generalized to allow efficient computation not just of classical CDs but also of time-sensitive and nontermination-sensitive CDs. We then use time-sensitive CDs and time-sensitive slicing to discover cache timing leaks in an AES implementation. Performance measurements demonstrate scalability of the approach.},
  keywords = {Control dependency,notion,program slicing,timing dependency,timing leak}
}

@inproceedings{heDebinPredictingDebug2018,
  title = {Debin: {{Predicting Debug Information}} in {{Stripped Binaries}}},
  booktitle = {Proceedings of the 2018 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {He, Jingxuan and Ivanov, Pesho and Tsankov, Petar and Raychev, Veselin and Vechev, Martin},
  date = {2018},
  series = {{{CCS}} '18},
  pages = {1667--1680},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/3243734.3243866},
  url = {http://doi.acm.org/10.1145/3243734.3243866},
  isbn = {978-1-4503-5693-0},
  venue = {Toronto, Canada},
  keywords = {binary code,debug information,machine learning,notion,security}
}

@article{heinze-demlCausalStructureLearning2018,
  title = {Causal {{Structure Learning}}},
  author = {Heinze-Deml, Christina and Maathuis, Marloes H. and Meinshausen, Nicolai},
  date = {2018},
  journaltitle = {Annual Review of Statistics and Its Application},
  volume = {5},
  number = {1},
  pages = {371--391},
  doi = {10.1146/annurev-statistics-031017-100630},
  url = {https://doi.org/10.1146/annurev-statistics-031017-100630},
  abstract = {Graphical models can represent a multivariate distribution in a convenient and accessible form as a graph. Causal models can be viewed as a special class of graphical models that represent not only the distribution of the observed system but also the distributions under external interventions. They hence enable predictions under hypothetical interventions, which is important for decision making. The challenging task of learning causal models from data always relies on some underlying assumptions. We discuss several recently proposed structure learning algorithms and their assumptions, and we compare their empirical performance under various scenarios.},
  keywords = {notion}
}

@inproceedings{hellendoornAreDeepNeural2017,
  title = {Are {{Deep Neural Networks}} the {{Best Choice}} for {{Modeling Source Code}}?},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Hellendoorn, Vincent J. and Devanbu, Premkumar},
  date = {2017},
  series = {{{ESEC}}/{{FSE}} 2017},
  pages = {763--773},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/3106237.3106290},
  url = {http://doi.acm.org/10.1145/3106237.3106290},
  isbn = {978-1-4503-5105-8},
  venue = {Paderborn, Germany},
  keywords = {language models,naturalness,notion,software tools}
}

@inproceedings{hellendoornGlobalRelationalModels2020,
  title = {Global {{Relational Models}} of {{Source Code}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Hellendoorn, Vincent J. and Sutton, Charles and Singh, Rishabh and Maniatis, Petros and Bieber, David},
  date = {2020},
  url = {https://openreview.net/forum?id=B1lnbRNtwr},
  keywords = {notion}
}

@misc{hellersteinGoogleClusterData2010,
  title = {Google Cluster Data},
  author = {Hellerstein, Joseph L.},
  date = {2010-01},
  keywords = {notion},
  annotation = {Published: Google research blog}
}

@inproceedings{helmTotalRecallHow2024,
  title = {Total {{Recall}}? {{How Good Are Static Call Graphs Really}}?},
  shorttitle = {Total {{Recall}}?},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Helm, Dominik and Keidel, Sven and Kampkötter, Anemone and Düsing, Johannes and Roth, Tobias and Hermann, Ben and Mezini, Mira},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {112--123},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3652114},
  url = {https://dl.acm.org/doi/10.1145/3650212.3652114},
  urldate = {2024-09-21},
  abstract = {Static call graphs are a fundamental building block of program analysis.    However, differences in call-graph construction and the use of specific language features can yield unsoundness and imprecision.    Call-graph analyses are evaluated using measures of precision and recall, but this is hard when a ground truth for real-world programs is generally unobtainable.        In this work, we propose to use carefully constructed dynamic baselines based on fixed entry points and input corpora.    The creation of this dynamic baseline is posed as an approximation of the ground truth---an optimization problem.    We use manual extension and coverage-guided fuzzing for creating suitable input corpora.        With these dynamic baselines, we study call-graph quality of multiple algorithms and implementations using four real-world Java programs.    We find that our methodology provides valuable insights into call-graph quality and how to measure it.    With this work, we provide a novel methodology to advance the field of static program analysis as we assess the computation of one of its core data structures---the call graph.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/MCZ6BVE6/Helm et al. - 2024 - Total Recall How Good Are Static Call Graphs Really.pdf}
}

@unpublished{heLoghubLargeCollection2020,
  title = {Loghub: {{A}} Large Collection of System Log Datasets towards Automated Log Analytics},
  author = {He, Shilin and Zhu, Jieming and He, Pinjia and Lyu, Michael R},
  date = {2020},
  eprint = {2008.06448},
  eprinttype = {arXiv},
  keywords = {notion}
}

@inproceedings{hendersonBehavioralFaultLocalization2018,
  title = {Behavioral {{Fault Localization}} by {{Sampling Suspicious Dynamic Control Flow Subgraphs}}},
  booktitle = {2018 {{IEEE}} 11th {{International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}} ({{ICST}})},
  author = {Henderson, T. A. D. and Podgurski, A.},
  date = {2018-04},
  pages = {93--104},
  issn = {null},
  doi = {10.1109/ICST.2018.00019},
  keywords = {Automatic Fault Localization,basic-block level dynamic control flow graphs,behavioral fault localization,data mining,discriminative subgraph mining,Discriminative Subgraph Mining,Dynamic Analysis,flow graphs,Heuristic algorithms,Instruments,Libraries,Measurement,notion,previous fault localization techniques,Probability,Profiling,program debugging,program testing,random processes,sampling methods,Score Weighted Random Walks,Software,Software algorithms,software fault tolerance,statistical analysis,Statistical Fault Localization,statistical fault localization metrics,subgraph-suspiciousness measures,suspicious dynamic control flow subgraphs sampling,SWRW}
}

@article{hendersonEvaluatingAutomaticFault2019,
  title = {Evaluating {{Automatic Fault Localization Using Markov Processes}}},
  author = {Henderson, Tim A. D. and Podgurski, Andy and Küçük, Yigit},
  date = {2019},
  journaltitle = {2019 19th International Working Conference on Source Code Analysis and Manipulation (SCAM)},
  pages = {115--126},
  keywords = {notion}
}

@inproceedings{heoEffectiveProgramDebloating2018,
  title = {Effective {{Program Debloating}} via {{Reinforcement Learning}}},
  booktitle = {Proceedings of the 2018 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Heo, Kihong and Lee, Woosuk and Pashakhanloo, Pardis and Naik, Mayur},
  date = {2018},
  series = {{{CCS}} '18},
  pages = {380--394},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/3243734.3243838},
  url = {http://doi.acm.org/10.1145/3243734.3243838},
  isbn = {978-1-4503-5693-0},
  venue = {Toronto, Canada},
  keywords = {notion,program debloating,reinforcement learning}
}

@inproceedings{heoResourceawareProgramAnalysis2019,
  title = {Resource-Aware {{Program Analysis}} via {{Online Abstraction Coarsening}}},
  booktitle = {Proceedings of the 41st {{International Conference}} on {{Software Engineering}}},
  author = {Heo, Kihong and Oh, Hakjoo and Yang, Hongseok},
  date = {2019},
  series = {{{ICSE}} '19},
  pages = {94--104},
  publisher = {IEEE Press},
  location = {Piscataway, NJ, USA},
  doi = {10.1109/ICSE.2019.00027},
  url = {https://doi.org/10.1109/ICSE.2019.00027},
  venue = {Montreal, Quebec, Canada},
  keywords = {learning,notion,resource constraint,static analysis}
}

@inproceedings{heraultApproximateProbabilisticModel2004,
  title = {Approximate {{Probabilistic Model Checking}}},
  booktitle = {International {{Conference}} on {{Verification}}, {{Model Checking}} and {{Abstract Interpretation}}},
  author = {Hérault, Thomas and Lassaigne, Richard and Magniette, Frédéric and Peyronnet, Sylvain},
  date = {2004},
  keywords = {notion}
}

@article{herboldFinegrainedDataSet2021,
  title = {A {{Fine-grained Data Set}} and {{Analysis}} of {{Tangling}} in {{Bug Fixing Commits}}},
  author = {Herbold, Steffen and Trautsch, Alexander and Ledel, Benjamin and Aghamohammadi, Alireza and Ghaleb, Taher Ahmed and Chahal, Kuljit Kaur and Bossenmaier, Tim and Nagaria, Bhaveet and Makedonski, Philip and Ahmadabadi, Matin Nili and others},
  date = {2021},
  journaltitle = {Empirical Software Engineering},
  publisher = {Springer},
  keywords = {notion}
}

@article{herboldLargeScaleManualValidation2020,
  title = {Large-{{Scale Manual Validation}} of {{Bug Fixing Commits}}: {{A Fine-grained Analysis}} of {{Tangling}}},
  author = {Herbold, Steffen and Trautsch, Alexander and Ledel, Benjamin and Aghamohammadi, Alireza and Ghaleb, Taher Ahmed and Chahal, Kuljit Kaur and Bossenmaier, Tim and Nagaria, Bhaveet and Makedonski, Philip and Ahmadabadi, Matin Nili and Szabados, Kristóf and Spieker, Helge and Madeja, Matej and Hoy, Nathaniel and Lenarduzzi, Valentina and Wang, Shangwen and Rodríguez-Pérez, Gema and Palacios, Ricardo Colomo and Verdecchia, Roberto and Singh, Paramvir and Qin, Yihao and Chakroborti, Debasish and Davis, Willard and Walunj, Vijay and Wu, Hongjun and Marcilio, Diego and Alam, Omar and Aldaeej, Abdullah and Amit, Idan and Turhan, Burak and Eismann, Simon and Wickert, Anna-Katharina and Malavolta, Ivano and Sulír, Matús and Fard, Fatemeh and Henley, Austin Z. and Kourtzanidis, Stratos and Tuzun, Eray and Treude, Christoph and Shamasbi, Simin Maleki and Pashchenko, Ivan and Wyrich, Marvin and Davis, James and Serebrenik, Alexander and Albrecht, Ella and Aktas, Ethem Utku and Strüber, Daniel and Erbel, Johannes},
  date = {2020},
  journaltitle = {CoRR},
  volume = {abs/2011.06244},
  eprint = {2011.06244},
  eprinttype = {arXiv},
  url = {https://arxiv.org/abs/2011.06244},
  keywords = {notion}
}

@inproceedings{herreraSeedSelectionSuccessful2021,
  title = {Seed Selection for Successful Fuzzing},
  booktitle = {Proceedings of the 30th {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Herrera, Adrian and Gunadi, Hendra and Magrath, Shane and Norrish, Michael and Payer, Mathias and Hosking, Antony L},
  date = {2021},
  pages = {230--243},
  keywords = {notion}
}

@inproceedings{herzigImpactTangledCode2013,
  title = {The {{Impact}} of {{Tangled Code Changes}}},
  booktitle = {Proceedings of the 10th {{Working Conference}} on {{Mining Software Repositories}}},
  author = {Herzig, Kim and Zeller, Andreas},
  date = {2013},
  series = {{{MSR}} '13},
  pages = {121--130},
  publisher = {IEEE Press},
  location = {Piscataway, NJ, USA},
  url = {http://dl.acm.org/citation.cfm?id=2487085.2487113},
  isbn = {978-1-4673-2936-1},
  venue = {San Francisco, CA, USA},
  keywords = {notion}
}

@article{herzigImpactTangledCode2016,
  title = {The {{Impact}} of {{Tangled Code Changes}} on {{Defect Prediction Models}}},
  author = {Herzig, Kim and Just, Sascha and Zeller, Andreas},
  date = {2016-04},
  journaltitle = {Empirical Softw. Engg.},
  volume = {21},
  number = {2},
  pages = {303--336},
  publisher = {Kluwer Academic Publishers},
  location = {USA},
  issn = {1382-3256},
  doi = {10.1007/s10664-015-9376-6},
  url = {https://doi.org/10.1007/s10664-015-9376-6},
  abstract = {When interacting with source control management system, developers often commit unrelated or loosely related code changes in a single transaction. When analyzing version histories, such tangled changes will make all changes to all modules appear related, possibly compromising the resulting analyses through noise and bias. In an investigation of five open-source Java projects, we found between 7 \% and 20 \% of all bug fixes to consist of multiple tangled changes. Using a multi-predictor approach to untangle changes, we show that on average at least 16.6 \% of all source files are incorrectly associated with bug reports. These incorrect bug file associations seem to not significantly impact models classifying source files to have at least one bug or no bugs. But our experiments show that untangling tangled code changes can result in more accurate regression bug prediction models when compared to models trained and tested on tangled bug datasets–in our experiments, the statistically significant accuracy improvements lies between 5 \% and 200 \%. We recommend better change organization to limit the impact of tangled changes.},
  keywords = {Data noise,Defect prediction,notion,Untangling}
}

@article{herzigUntanglingChanges2011,
  title = {Untangling Changes},
  author = {Herzig, Kim and Zeller, Andreas},
  date = {2011},
  journaltitle = {Unpublished manuscript, September},
  volume = {37},
  pages = {38--40},
  keywords = {notion}
}

@article{heSurveyAutomatedLog2021,
  title = {A {{Survey}} on {{Automated Log Analysis}} for {{Reliability Engineering}}},
  author = {He, Shilin and He, Pinjia and Chen, Zhuangbin and Yang, Tianyi and Su, Yuxin and Lyu, Michael R.},
  date = {2021-07},
  journaltitle = {ACM Comput. Surv.},
  volume = {54},
  number = {6},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {0360-0300},
  doi = {10.1145/3460345},
  url = {https://doi.org/10.1145/3460345},
  abstract = {Logs are semi-structured text generated by logging statements in software source code. In recent decades, software logs have become imperative in the reliability assurance mechanism of many software systems, because they are often the only data available that record software runtime information. As modern software is evolving into a large scale, the volume of logs has increased rapidly. To enable effective and efficient usage of modern software logs in reliability engineering, a number of studies have been conducted on automated log analysis. This survey presents a detailed overview of automated log analysis research, including how to automate and assist the writing of logging statements, how to compress logs, how to parse logs into structured event templates, and how to employ logs to detect anomalies, predict failures, and facilitate diagnosis. Additionally, we survey work that releases open-source toolkits and datasets. Based on the discussion of the recent advances, we present several promising future directions toward real-world and next-generation automated log analysis.},
  keywords = {Log,log analysis,log compression,log mining,log parsing,logging,notion}
}

@inproceedings{heuleMimicComputingModels2015,
  title = {Mimic: {{Computing Models}} for {{Opaque Code}}},
  booktitle = {Proceedings of the 2015 10th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Heule, Stefan and Sridharan, Manu and Chandra, Satish},
  date = {2015},
  series = {{{ESEC}}/{{FSE}} 2015},
  pages = {710--720},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2786805.2786875},
  url = {http://doi.acm.org/10.1145/2786805.2786875},
  isbn = {978-1-4503-3675-8},
  venue = {Bergamo, Italy},
  keywords = {JavaScript,Markov Chain Monte Carlo (MCMC),notion,Opaque Code,Program Synthesis}
}

@inproceedings{heusserQuantifyingInformationLeaks2010,
  title = {Quantifying {{Information Leaks}} in {{Software}}},
  booktitle = {Proceedings of the 26th {{Annual Computer Security Applications Conference}}},
  author = {Heusser, Jonathan and Malacaria, Pasquale},
  date = {2010},
  series = {{{ACSAC}} '10},
  pages = {261--269},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1920261.1920300},
  url = {https://doi.org/10.1145/1920261.1920300},
  abstract = {Leakage of confidential information represents a serious security risk. Despite a number of novel, theoretical advances, it has been unclear if and how quantitative approaches to measuring leakage of confidential information could be applied to substantial, real-world programs. This is mostly due to the high complexity of computing precise leakage quantities. In this paper, we introduce a technique which makes it possible to decide if a program conforms to a quantitative policy which scales to large state-spaces with the help of bounded model checking.Our technique is applied to a number of officially reported information leak vulnerabilities in the Linux Kernel. Additionally, we also analysed authentication routines in the Secure Remote Password suite and of a Internet Message Support Protocol implementation. Our technique shows when there is unacceptable leakage; the same technique is also used to verify, for the first time, that the applied software patches indeed plug the information leaks.This is the first demonstration of quantitative information flow addressing security concerns of real-world industrial programs.},
  isbn = {978-1-4503-0133-6},
  venue = {Austin, Texas, USA},
  keywords = {information leakage,Linux kernel,notion,quantitative information flow}
}

@article{hieronsAvoidingCoincidentalCorrectness2006,
  title = {Avoiding Coincidental Correctness in Boundary Value Analysis},
  author = {Hierons, Robert M},
  date = {2006},
  journaltitle = {ACM Transactions on Software Engineering and Methodology (TOSEM)},
  volume = {15},
  number = {3},
  pages = {227--241},
  publisher = {ACM New York, NY, USA},
  keywords = {notion}
}

@inproceedings{hindleNaturalnessSoftware2012,
  title = {On the Naturalness of Software},
  booktitle = {2012 34th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Hindle, A. and Barr, E. T. and Su, Z. and Gabel, M. and Devanbu, P.},
  date = {2012-06},
  pages = {837--847},
  issn = {0270-5257},
  doi = {10.1109/ICSE.2012.6227135},
  abstract = {Natural languages like English are rich, complex, and powerful. The highly creative and graceful use of languages like English and Tamil, by masters like Shakespeare and Avvaiyar, can certainly delight and inspire. But in practice, given cognitive constraints and the exigencies of daily life, most human utterances are far simpler and much more repetitive and predictable. In fact, these utterances can be very usefully modeled using modern statistical methods. This fact has led to the phenomenal success of statistical approaches to speech recognition, natural language translation, question-answering, and text mining and comprehension. We begin with the conjecture that most software is also natural, in the sense that it is created by humans at work, with all the attendant constraints and limitations - and thus, like natural language, it is also likely to be repetitive and predictable. We then proceed to ask whether a) code can be usefully modeled by statistical language models and b) such models can be leveraged to support software engineers. Using the widely adopted n-gram model, we provide empirical evidence supportive of a positive answer to both these questions. We show that code is also very repetitive, and in fact even more so than natural languages. As an example use of the model, we have developed a simple code completion engine for Java that, despite its simplicity, already improves Eclipse's built-in completion capability. We conclude the paper by laying out a vision for future research in this area.},
  keywords = {Code Completion,Language Model,Natural Language,notion}
}

@misc{hintonForwardForwardAlgorithmPreliminary2022,
  title = {The {{Forward-Forward Algorithm}}: {{Some Preliminary Investigations}}},
  author = {Hinton, Geoffrey},
  date = {2022},
  doi = {10.48550/ARXIV.2212.13345},
  url = {https://arxiv.org/abs/2212.13345},
  organization = {arXiv},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),notion}
}

@unpublished{hitzDiscreteExtremes2017,
  title = {Discrete Extremes},
  author = {Hitz, Adrien and Davis, Richard and Samorodnitsky, Gennady},
  date = {2017},
  eprint = {1707.05033},
  eprinttype = {arXiv},
  keywords = {notion}
}

@article{hoareAxiomaticBasisComputer1969,
  title = {An {{Axiomatic Basis}} for {{Computer Programming}}},
  author = {Hoare, C. A. R.},
  date = {1969-10},
  journaltitle = {Commun. ACM},
  volume = {12},
  number = {10},
  pages = {576--580},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {0001-0782},
  doi = {10.1145/363235.363259},
  url = {https://doi.org/10.1145/363235.363259},
  abstract = {In this paper an attempt is made to explore the logical foundations of computer programming by use of techniques which were first applied in the study of geometry and have later been extended to other branches of mathematics. This involves the elucidation of sets of axioms and rules of inference which can be used in proofs of the properties of computer programs. Examples are given of such axioms and rules, and a formal proof of a simple theorem is displayed. Finally, it is argued that important advantage, both theoretical and practical, may follow from a pursuance of these topics.},
  keywords = {axiomatic method,formal language definition,machine-independent programming,notion,program documentation,programming language design,theory of programming' proofs of programs}
}

@inproceedings{hodovanCoarseHierarchicalDelta2017,
  title = {Coarse {{Hierarchical Delta Debugging}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Software Maintenance}} and {{Evolution}} ({{ICSME}})},
  author = {Hodován, R. and Kiss, Á and Gyimóthy, T.},
  date = {2017-09},
  pages = {194--203},
  doi = {10.1109/ICSME.2017.26},
  abstract = {This paper introduces the Coarse Hierarchical Delta Debugging algorithm for efficient test case reduction. It can be used as a test case simplification algorithm in its own right if theoretical minimality is not a strict requirement, or it can act as a preprocessing step to the original Hierarchical Delta Debugging algorithm. Evaluation of artificial and real test cases shows that a coarse variant can produce reduced test cases with significantly fewer testing steps than the original algorithm (58\% gain on average, 79\% maximum), while still keeping the outputs acceptably small (never increasing the reduced test cases by more than 0.36\% of the input).},
  keywords = {artificial test cases,coarse algorithm,Coarse Hierarchical Delta Debugging algorithm,Computer crashes,data structures,Debugging,efficient test case reduction,Grammar,hierarchical delta debugging,Minimization,notion,preprocessing step,program debugging,program testing,real test cases,Software algorithms,Syntactics,test case reduction,Testing,theoretical minimality}
}

@inproceedings{hoffmannSlicingDroidsProgram2013,
  title = {Slicing {{Droids}}: {{Program Slicing}} for {{Smali Code}}},
  booktitle = {Proceedings of the 28th {{Annual ACM Symposium}} on {{Applied Computing}}},
  author = {Hoffmann, Johannes and Ussath, Martin and Holz, Thorsten and Spreitzenbarth, Michael},
  date = {2013},
  series = {{{SAC}} '13},
  pages = {1844--1851},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2480362.2480706},
  url = {http://doi.acm.org/10.1145/2480362.2480706},
  isbn = {978-1-4503-1656-9},
  venue = {Coimbra, Portugal},
  keywords = {notion,Program Slicing,SAAF,Smali}
}

@incollection{hoffmanOnlineLearningLatent2010,
  title = {Online {{Learning}} for {{Latent Dirichlet Allocation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 23},
  author = {Hoffman, Matthew and Bach, Francis R. and Blei, David M.},
  editor = {Lafferty, J. D. and Williams, C. K. I. and Shawe-Taylor, J. and Zemel, R. S. and Culotta, A.},
  date = {2010},
  pages = {856--864},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation.pdf},
  keywords = {Latent Dirichlet Allocation (LDA),notion}
}

@article{hofmanIntegratingExplanationPrediction2021,
  title = {Integrating Explanation and Prediction in Computational Social Science},
  author = {Hofman, Jake M. and Watts, Duncan J. and Athey, Susan and Garip, Filiz and Griffiths, Thomas L. and Kleinberg, Jon and Margetts, Helen and Mullainathan, Sendhil and Salganik, Matthew J. and Vazire, Simine and Vespignani, Alessandro and Yarkoni, Tal},
  date = {2021},
  journaltitle = {Nature},
  volume = {595},
  number = {7866},
  pages = {181--188},
  doi = {10.1038/s41586-021-03659-0},
  url = {https://doi.org/10.1038/s41586-021-03659-0},
  abstract = {Computational social science is more than just large repositories of digital data and the computational methods needed to construct and analyse them. It also represents a convergence of different fields with different ways of thinking about and doing science. The goal of this Perspective is to provide some clarity around how these approaches differ from one another and to propose how they might be productively integrated. Towards this end we make two contributions. The first is a schema for thinking about research activities along two dimensions—the extent to which work is explanatory, focusing on identifying and estimating causal effects, and the degree of consideration given to testing predictions of outcomes—and how these two priorities can complement, rather than compete with, one another. Our second contribution is to advocate that computational social scientists devote more attention to combining prediction and explanation, which we call integrative modelling, and to outline some practical suggestions for realizing this goal.},
  isbn = {1476-4687},
  keywords = {notion}
}

@inproceedings{hongLatentProgrammerDiscrete2021,
  title = {Latent {{Programmer}}: {{Discrete Latent Codes}} for {{Program Synthesis}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Hong, Joey and Dohan, David and Singh, Rishabh and Sutton, Charles and Zaheer, Manzil},
  editor = {Meila, Marina and Zhang, Tong},
  date = {2021-07-18},
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {139},
  pages = {4308--4318},
  publisher = {PMLR},
  url = {http://proceedings.mlr.press/v139/hong21a.html},
  abstract = {A key problem in program synthesis is searching over the large space of possible programs. Human programmers might decide the high-level structure of the desired program before thinking about the details; motivated by this intuition, we consider two-level search for program synthesis, in which the synthesizer first generates a plan, a sequence of symbols that describes the desired program at a high level, before generating the program. We propose to learn representations of programs that can act as plans to organize such a two-level search. Discrete latent codes are appealing for this purpose, and can be learned by applying recent work on discrete autoencoders. Based on these insights, we introduce the Latent Programmer (LP), a program synthesis method that first predicts a discrete latent code from input/output examples, and then generates the program in the target language. We evaluate the LP on two domains, demonstrating that it yields an improvement in accuracy, especially on longer programs for which search is most difficult.},
  keywords = {notion}
}

@article{hongMUSEUMDebuggingRealworld2017,
  title = {{{MUSEUM}}: {{Debugging}} Real-World Multilingual Programs Using Mutation Analysis},
  author = {Hong, Shin and Kwak, Taehoon and Lee, Byeongcheol and Jeon, Yiru and Ko, Bongseok and Kim, Yunho and Kim, Moonzoo},
  date = {2017},
  journaltitle = {Information and Software Technology},
  volume = {82},
  pages = {80--95},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2016.10.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0950584916302427},
  abstract = {Abstract Context: The programming language ecosystem has diversified over the last few decades. Non-trivial programs are likely to be written in more than a single language to take advantage of various control/data abstractions and legacy libraries. Objective: Debugging multilingual bugs is challenging because language interfaces are difficult to use correctly and the scope of fault localization goes beyond language boundaries. To locate the causes of real-world multilingual bugs, this article proposes a mutation-based fault localization technique (MUSEUM). Method: MUSEUM modifies a buggy program systematically with our new mutation operators as well as conventional mutation operators, observes the dynamic behavioral changes in a test suite, and reports suspicious statements. To reduce the analysis cost, MUSEUM selects a subset of mutated programs and test cases. Results: Our empirical evaluation shows that MUSEUM is (i) effective: it identifies the buggy statements as the most suspicious statements for both resolved and unresolved non-trivial bugs in real-world multilingual programming projects; and (ii) efficient: it locates the buggy statements in modest amount of time using multiple machines in parallel. Also, by applying selective mutation analysis (i.e., selecting subsets of mutants and test cases to use), MUSEUM achieves significant speedup with marginal accuracy loss compared to the full mutation analysis. Conclusion: It is concluded that MUSEUM locates real-world multilingual bugs accurately. This result shows that mutation analysis can provide an effective, efficient, and language semantics agnostic analysis on multilingual code. Our light-weight analysis approach would play important roles as programmers write and debug large and complex programs in diverse programming languages.},
  keywords = {Debugging,Foreign function interface,Language interoperability,mutation analysis,notion}
}

@inproceedings{horvathExperimentsInteractiveFault2020,
  title = {Experiments with {{Interactive Fault Localization Using Simulated}} and {{Real Users}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Software Maintenance}} and {{Evolution}} ({{ICSME}})},
  author = {Horváth, Ferenc and Beszédes, Árpád and Vancsics, Béla and Balogh, Gergő and Vidács, László and Gyimóthy, Tibor},
  date = {2020},
  pages = {290--300},
  doi = {10.1109/ICSME46990.2020.00036},
  keywords = {notion}
}

@article{horwitzBetterDebuggingOutput2009,
  title = {Better Debugging via Output Tracing and Callstack-Sensitive Slicing},
  author = {Horwitz, Susan and Liblit, Ben and Polishchuk, Marina},
  date = {2009},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {36},
  number = {1},
  pages = {7--19},
  publisher = {IEEE},
  keywords = {notion}
}

@article{horwitzInterproceduralSlicingUsing1988,
  title = {Interprocedural {{Slicing Using Dependence Graphs}}},
  author = {Horwitz, S. and Reps, T. and Binkley, D.},
  date = {1988-06},
  journaltitle = {SIGPLAN Not.},
  volume = {23},
  number = {7},
  pages = {35--46},
  publisher = {ACM},
  location = {New York, NY, USA},
  issn = {0362-1340},
  doi = {10.1145/960116.53994},
  url = {http://doi.acm.org/10.1145/960116.53994},
  keywords = {notion}
}

@inproceedings{hoscheleMiningInputGrammars2016,
  title = {Mining Input Grammars from Dynamic Taints},
  booktitle = {2016 31st {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Höschele, Matthias and Zeller, Andreas},
  date = {2016},
  pages = {720--725},
  publisher = {IEEE},
  keywords = {notion}
}

@inproceedings{huangApplyingAutomatedProgram2021a,
  title = {Applying {{Automated Program Repair}} to {{Dataflow Programming Languages}}},
  booktitle = {2021 {{IEEE}}/{{ACM International Workshop}} on {{Genetic Improvement}} ({{GI}})},
  author = {Huang, Yu and Ahmad, Hammad and Forrest, Stephanie and Weimer, Westley},
  date = {2021},
  pages = {21--22},
  doi = {10.1109/GI52543.2021.00013},
  keywords = {notion}
}

@article{huangBEACONDirectedGreyBox2022,
  title = {{{BEACON}}: {{Directed Grey-Box Fuzzing}} with {{Provable Path Pruning}}},
  author = {Huang, Heqing and Guo, Yiyuan and Shi, Qingkai and Yao, Peisen and Wu, Rongxin and Zhang, Charles},
  date = {2022},
  journaltitle = {2022 IEEE Symposium on Security and Privacy (SP)},
  pages = {36--50},
  url = {https://api.semanticscholar.org/CorpusID:237589338},
  keywords = {notion}
}

@inproceedings{huangNeuralProbabilisticModel2015,
  title = {A {{Neural Probabilistic Model}} for {{Context Based Citation Recommendation}}},
  booktitle = {Proceedings of the {{Twenty-Ninth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Huang, Wenyi and Wu, Zhaohui and Liang, Chen and Mitra, Prasenjit and Giles, C. Lee},
  date = {2015},
  series = {{{AAAI}}'15},
  pages = {2404--2410},
  publisher = {AAAI Press},
  location = {Austin, Texas},
  isbn = {0-262-51129-0},
  keywords = {notion}
}

@inproceedings{huangPangolinIncrementalHybrid2020,
  title = {Pangolin: {{Incremental Hybrid Fuzzing}} with {{Polyhedral Path Abstraction}}},
  booktitle = {2020 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  author = {Huang, Heqing and Yao, Peisen and Wu, Rongxin and Shi, Qingkai and Zhang, Charles},
  date = {2020},
  pages = {1613--1627},
  doi = {10.1109/SP40000.2020.00063},
  keywords = {notion}
}

@inproceedings{hutchinsExperimentsEffectivenessDataflow1994,
  title = {Experiments on the Effectiveness of Dataflow- and Control-Flow-Based Test Adequacy Criteria},
  booktitle = {Proceedings of 16th {{International Conference}} on {{Software Engineering}}},
  author = {Hutchins, M. and Foster, H. and Goradia, T. and Ostrand, T.},
  date = {1994-05},
  pages = {191--200},
  issn = {0270-5257},
  doi = {10.1109/ICSE.1994.296778},
  keywords = {Control systems,control-flow-based test adequacy criteria,Costs,coverage criteria,Data analysis,dataflow-based test adequacy criteria,Educational institutions,Fault detection,Fault diagnosis,faulty program versions,Investments,Monitoring,notion,program testing,software engineering,Software testing,System testing,test sets}
}

@inproceedings{hyttinenConstraintbasedCausalDiscovery2014,
  title = {Constraint-Based {{Causal Discovery}}: {{Conflict Resolution}} with {{Answer Set Programming}}.},
  booktitle = {{{UAI}}},
  author = {Hyttinen, Antti and Eberhardt, Frederick and Järvisalo, Matti},
  date = {2014},
  pages = {340--349},
  keywords = {causal discovery,notion}
}

@unpublished{hyttinenDiscoveringCyclicCausal2013,
  title = {Discovering Cyclic Causal Models with Latent Variables: {{A}} General {{SAT-based}} Procedure},
  author = {Hyttinen, Antti and Hoyer, Patrik O and Eberhardt, Frederick and Jarvisalo, Matti},
  date = {2013},
  eprint = {1309.6836},
  eprinttype = {arXiv},
  keywords = {notion}
}

@article{ioannidisWhyMostPublished2005,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  date = {2005-08},
  journaltitle = {PLOS Medicine},
  volume = {2},
  number = {8},
  pages = {null},
  publisher = {Public Library of Science},
  doi = {10.1371/journal.pmed.0020124},
  url = {https://doi.org/10.1371/journal.pmed.0020124},
  abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  keywords = {notion}
}

@article{ioffeBatchNormalizationAccelerating2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  date = {2015},
  journaltitle = {CoRR},
  volume = {abs/1502.03167},
  url = {http://arxiv.org/abs/1502.03167},
  keywords = {notion}
}

@article{islamCoherentClustersSource2014,
  title = {Coherent Clusters in Source Code},
  author = {Islam, Syed and Krinke, Jens and Binkley, David and Harman, Mark},
  date = {2014},
  journaltitle = {Journal of Systems and Software},
  volume = {88},
  pages = {1--24},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2013.07.040},
  url = {https://www.sciencedirect.com/science/article/pii/S016412121300188X},
  abstract = {This paper presents the results of a large scale empirical study of coherent dependence clusters. All statements in a coherent dependence cluster depend upon the same set of statements and affect the same set of statements; a coherent cluster's statements have `coherent' shared backward and forward dependence. We introduce an approximation to efficiently locate coherent clusters and show that it has a minimum precision of 97.76\%. Our empirical study also finds that, despite their tight coherence constraints, coherent dependence clusters are in abundance: 23 of the 30 programs studied have coherent clusters that contain at least 10\% of the whole program. Studying patterns of clustering in these programs reveals that most programs contain multiple substantial coherent clusters. A series of subsequent case studies uncover that all clusters of significant size map to a logical functionality and correspond to a program structure. For example, we show that for the program acct, the top five coherent clusters all map to specific, yet otherwise non-obvious, functionality. Cluster visualization also brings out subtle deficiencies in program structure and identifies potential refactoring candidates. A study of inter-cluster dependence is used to highlight how coherent clusters are connected to each other, revealing higher-level structures, which can be used in reverse engineering. Finally, studies are presented to illustrate how clusters are not correlated with program faults as they remain stable during most system evolution.},
  keywords = {Dependence analysis,notion,Program comprehension,Software clustering}
}

@article{isolaImageImageTranslationConditional2016,
  title = {Image-to-{{Image Translation}} with {{Conditional Adversarial Networks}}},
  author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
  date = {2016},
  journaltitle = {CoRR},
  volume = {abs/1611.07004},
  url = {http://arxiv.org/abs/1611.07004},
  keywords = {notion}
}

@inproceedings{ispoglouFuzzGenAutomaticFuzzer2020,
  title = {{{FuzzGen}}: {{Automatic Fuzzer Generation}}},
  booktitle = {29th {{USENIX Security Symposium}} ({{USENIX Security}} 20)},
  author = {Ispoglou, Kyriakos and Austin, Daniel and Mohan, Vishwath and Payer, Mathias},
  date = {2020-08},
  pages = {2271--2287},
  publisher = {USENIX Association},
  url = {https://www.usenix.org/conference/usenixsecurity20/presentation/ispoglou},
  isbn = {978-1-939133-17-5},
  keywords = {notion}
}

@inproceedings{itaniStructureLearningCausal2010,
  title = {Structure Learning in Causal Cyclic Networks},
  booktitle = {Causality: {{Objectives}} and Assessment},
  author = {Itani, Sleiman and Ohannessian, Mesrob and Sachs, Karen and Nolan, Garry P and Dahleh, Munther A},
  date = {2010},
  pages = {165--176},
  keywords = {notion}
}

@inproceedings{ivankovicProductiveCoverageImproving2024,
  title = {Productive {{Coverage}}: {{Improving}} the {{Actionability}} of {{Code Coverage}}},
  booktitle = {Proceedings of the 46th {{International Conference}} on {{Software Engineering}}: {{Software Engineering}} in {{Practice}}},
  author = {Ivankovic, Marko and Petrovic, Goran and Kulizhskaya, Yana and Lewko, Mateusz and Kalinovcic, Luka and Just, Rene and Fraser, Gordon},
  date = {2024},
  series = {{{ICSE-SEIP}} '24},
  pages = {58--68},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3639477.3639733},
  url = {https://doi.org/10.1145/3639477.3639733},
  abstract = {Code coverage is an intuitive and widely-used test adequacy measure. Established coverage measures treat each test goal (e.g., statement or branch) as equally important, and code-coverage adequacy requires every test goal to be covered. However, this is in contrast to how code coverage is used in practice. As a result, simply visualizing uncovered code is not actionable, and developers have to manually reason which uncovered code is critical to cover with tests and which code can be left untested. To make code coverage more actionable and further improve coverage in our codebase, we developed Productive Coverage — a novel approach to code coverage that guides developers to uncovered code that should be tested by (unit) tests. Specifically, Productive Coverage identifies uncovered code that is similar to existing tested and/or frequently in production executed code. We implemented and evaluated Productive Coverage for four programming languages (C++, Java, Go, and Python), and our evaluation shows: (1) The developer sentiment, measured at the point of use, is strongly positive; (2) Productive Coverage meaningfully increases test quality, compared to a strong baseline; (3) Productive Coverage has no negative effect on code authoring efficiency; (4) Productive Coverage modestly improves code-review efficiency; (5) Productive Coverage improves code quality and prevents defects from being introduced into the code.},
  isbn = {9798400705014},
  venue = {Lisbon, Portugal},
  keywords = {notion}
}

@inproceedings{jabbarvandMuDroidEnergyawareMutation2017,
  title = {\textbackslash mu\vphantom\{\}{{Droid}}: {{An Energy-aware Mutation Testing Framework}} for {{Android}}},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Jabbarvand, Reyhaneh and Malek, Sam},
  date = {2017},
  series = {{{ESEC}}/{{FSE}} 2017},
  pages = {208--219},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/3106237.3106244},
  url = {http://doi.acm.org/10.1145/3106237.3106244},
  isbn = {978-1-4503-5105-8},
  venue = {Paderborn, Germany},
  keywords = {Android,Energy Testing,f-droid,Mutation Testing,notion,Software Testing}
}

@inproceedings{jaszStaticExecuteReplacement2008,
  title = {Static {{Execute After}}/{{Before}} as a Replacement of Traditional Software Dependencies},
  booktitle = {2008 {{IEEE International Conference}} on {{Software Maintenance}}},
  author = {Jasz, J. and Beszedes, A. and Gyimothy, T. and Rajlich, V.},
  date = {2008-09},
  pages = {137--146},
  issn = {1063-6773},
  doi = {10.1109/ICSM.2008.4658062},
  keywords = {Algorithm design and analysis,Computer architecture,data flow analysis,Flow graphs,Image color analysis,notion,program analysis,program dependencies,program slicers,program slicing,Software,Software algorithms,software dependencies,software engineering,Software systems,Source code analysis,static execute after,Static Execute After,static execute before,system dependence graph,System Dependence Graph}
}

@inproceedings{jeffreyFaultLocalizationUsing2008,
  title = {Fault {{Localization Using Value Replacement}}},
  booktitle = {Proceedings of the 2008 {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Jeffrey, Dennis and Gupta, Neelam and Gupta, Rajiv},
  date = {2008},
  series = {{{ISSTA}} '08},
  pages = {167--178},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1390630.1390652},
  url = {https://doi.org/10.1145/1390630.1390652},
  abstract = {We present a value profile based approach for ranking program statements according to their likelihood of being faulty. The key idea is to see which program statements exercised during a failing run use values that can be altered so that the execution instead produces correct output. Our approach is effective in locating statements that are either faulty or directly linked to a faulty statement. We present experimental results showing the effectiveness and efficiency of our approach. Our approach outperforms Tarantula which, to our knowledge, is the most effective prior approach for statement ranking based fault localization using the benchmark programs we studied.},
  isbn = {978-1-60558-050-0},
  venue = {Seattle, WA, USA},
  keywords = {automated debugging,fault localization,interesting value mapping pair,notion,value replacement}
}

@article{jegourelCommandbasedImportanceSampling2016,
  title = {Command-Based Importance Sampling for Statistical Model Checking},
  author = {Jegourel, Cyrille and Legay, Axel and Sedwards, Sean},
  date = {2016},
  journaltitle = {Theoretical Computer Science},
  volume = {649},
  pages = {1--24},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2016.08.009},
  url = {https://www.sciencedirect.com/science/article/pii/S0304397516303966},
  abstract = {Statistical model checking avoids the exponential growth of states of numerical model checking, but rare properties are costly to verify. Importance sampling can reduce the cost if good importance sampling distributions can be found efficiently. Our approach uses a tractable cross-entropy minimisation algorithm to find an optimal parametrised importance sampling distribution. In contrast to previous work, our algorithm uses a naturally defined low dimensional vector to specify the distribution, thus avoiding an explicit representation of a transition matrix. Our parametrisation leads to a unique optimum and is shown to produce many orders of magnitude improvement in efficiency on various models. In this work we link the existence of optimal importance sampling distributions to logical properties and show how our parametrisation affects this link. We also motivate and present simple algorithms to create the initial distribution necessary for cross-entropy minimisation. Finally, we discuss the open challenge of defining error bounds with importance sampling and describe how our optimal parametrised distributions may be used to infer qualitative confidence.},
  keywords = {Cross entropy,Guarded commands,Importance sampling,Monte Carlo,notion,Rare events,Statistical model checking}
}

@inproceedings{jegourelImportanceSplittingStatistical2013,
  title = {Importance {{Splitting}} for {{Statistical Model Checking Rare Properties}}},
  booktitle = {International {{Conference}} on {{Computer Aided Verification}}},
  author = {Jégourel, Cyrille and Legay, Axel and Sedwards, Sean},
  date = {2013},
  keywords = {notion}
}

@inproceedings{jeongUTopiaAutomaticGeneration2023,
  title = {{{UTopia}}: {{Automatic Generation}} of {{Fuzz Driver}} Using {{Unit Tests}}},
  booktitle = {2023 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  author = {Jeong, Bokdeuk and Jang, Joonun and Yi, Hayoon and Moon, Jiin and Kim, Junsik and Jeon, Intae and Kim, Taesoo and Shim, WooChul and Hwang, Yong Ho},
  date = {2023},
  pages = {2676--2692},
  doi = {10.1109/SP46215.2023.10179394},
  keywords = {notion}
}

@article{jeonMachineLearningAlgorithmDisjunctive2019,
  title = {A {{Machine-Learning Algorithm}} with {{Disjunctive Model}} for {{Data-Driven Program Analysis}}},
  author = {Jeon, Minseok and Jeong, Sehun and Cha, Sungdeok and Oh, Hakjoo},
  date = {2019-06},
  journaltitle = {ACM Trans. Program. Lang. Syst.},
  volume = {41},
  number = {2},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {0164-0925},
  doi = {10.1145/3293607},
  url = {https://doi.org/10.1145/3293607},
  abstract = {We present a new machine-learning algorithm with disjunctive model for data-driven program analysis. One major challenge in static program analysis is a substantial amount of manual effort required for tuning the analysis performance. Recently, data-driven program analysis has emerged to address this challenge by automatically adjusting the analysis based on data through a learning algorithm. Although this new approach has proven promising for various program analysis tasks, its effectiveness has been limited due to simple-minded learning models and algorithms that are unable to capture sophisticated, in particular disjunctive, program properties. To overcome this shortcoming, this article presents a new disjunctive model for data-driven program analysis as well as a learning algorithm to find the model parameters. Our model uses Boolean formulas over atomic features and therefore is able to express nonlinear combinations of program properties. A key technical challenge is to efficiently determine a set of good Boolean formulas, as brute-force search would simply be impractical. We present a stepwise and greedy algorithm that efficiently learns Boolean formulas. We show the effectiveness and generality of our algorithm with two static analyzers: context-sensitive points-to analysis for Java and flow-sensitive interval analysis for C. Experimental results show that our automated technique significantly improves the performance of the state-of-the-art techniques including ones hand-crafted by human experts.},
  keywords = {context-sensitivity,Data-driven program analysis,flow-sensitivity,notion,static analysis}
}

@article{jiaAnalysisSurveyDevelopment2011,
  title = {An {{Analysis}} and {{Survey}} of the {{Development}} of {{Mutation Testing}}},
  author = {Jia, Y. and Harman, M.},
  date = {2011-09},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {37},
  number = {5},
  pages = {649--678},
  issn = {0098-5589},
  doi = {10.1109/TSE.2010.62},
  abstract = {Mutation Testing is a fault-based software testing technique that has been widely studied for over three decades. The literature on Mutation Testing has contributed a set of approaches, tools, developments, and empirical results. This paper provides a comprehensive analysis and survey of Mutation Testing. The paper also presents the results of several development trend analyses. These analyses provide evidence that Mutation Testing techniques and tools are reaching a state of maturity and applicability, while the topic of Mutation Testing itself is the subject of increasing interest.},
  keywords = {Mutation Testing,notion,Survey}
}

@article{jiaHigherOrderMutation2009,
  title = {Higher {{Order Mutation Testing}}},
  author = {Jia, Yue and Harman, Mark},
  date = {2009},
  journaltitle = {Information and Software Technology},
  volume = {51},
  number = {10},
  pages = {1379--1393},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2009.04.016},
  url = {http://www.sciencedirect.com/science/article/pii/S0950584909000688},
  abstract = {This paper introduces a new paradigm for Mutation Testing, which we call Higher Order Mutation Testing (HOM Testing). Traditional Mutation Testing considers only first order mutants, created by the injection of a single fault. Often these first order mutants denote trivial faults that are easily killed. Higher order mutants are created by the insertion of two or more faults. The paper introduces the concept of a subsuming HOM; one that is harder to kill than the first order mutants from which it is constructed. By definition, subsuming HOMs denote subtle fault combinations. The paper reports the results of an empirical study of HOM Testing using 10 programs, including several non-trivial real-world subjects for which test suites are available.},
  keywords = {Higher order mutant,Mutation Testing,notion,Search techniques}
}

@inproceedings{jiaLearningCombinatorialInteraction2015,
  title = {Learning {{Combinatorial Interaction Test Generation Strategies Using Hyperheuristic Search}}},
  booktitle = {2015 {{IEEE}}/{{ACM}} 37th {{IEEE International Conference}} on {{Software Engineering}}},
  author = {Jia, Y. and Cohen, M. B. and Harman, M. and Petke, J.},
  date = {2015-05},
  volume = {1},
  pages = {540--550},
  issn = {0270-5257},
  doi = {10.1109/ICSE.2015.71},
  abstract = {The surge of search based software engineering research has been hampered by the need to develop customized search algorithms for different classes of the same problem. For instance, two decades of bespoke Combinatorial Interaction Testing (CIT) algorithm development, our exemplar problem, has left software engineers with a bewildering choice of CIT techniques, each specialized for a particular task. This paper proposes the use of a single hyperheuristic algorithm that learns search strategies across a broad range of problem instances, providing a single generalist approach. We have developed a Hyperheuristic algorithm for CIT, and report experiments that show that our algorithm competes with known best solutions across constrained and unconstrained problems: For all 26 real-world subjects, it equals or outperforms the best result previously reported in the literature. We also present evidence that our algorithm's strong generic performance results from its unsupervised learning. Hyperheuristic search is thus a promising way to relocate CIT design intelligence from human to machine.},
  keywords = {Combinatorial Interaction Testing (CIT),Hyperheuristic Search (HS),notion}
}

@inproceedings{jiaMachineDeservesBetter2018,
  title = {Machine {{Deserves Better Logging}}: {{A Log Enhancement Approach}} for {{Automatic Fault Diagnosis}}},
  booktitle = {2018 {{IEEE International Symposium}} on {{Software Reliability Engineering Workshops}} ({{ISSREW}})},
  author = {Jia, Tong and Li, Ying and Zhang, Chengbo and Xia, Wensheng and Jiang, Jie and Liu, Yuhong},
  date = {2018},
  pages = {106--111},
  doi = {10.1109/ISSREW.2018.00-22},
  keywords = {notion}
}

@inproceedings{jiangCombiningSpectrumBasedFault2019,
  title = {Combining {{Spectrum-Based Fault Localization}} and {{Statistical Debugging}}: {{An Empirical Study}}},
  booktitle = {34th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Jiang, Jiajun and Wang, Ran and Xiong, Yingfei and Chen, Xiangping and Zhang, Lu},
  date = {2019},
  keywords = {notion}
}

@inproceedings{jiangDebuggingApproachJava2010,
  title = {A {{Debugging Approach}} for {{Java Runtime Exceptions Based}} on {{Program Slicing}} and {{Stack Traces}}},
  booktitle = {2010 10th {{International Conference}} on {{Quality Software}}},
  author = {Jiang, S. and Zhang, H. and Wang, Q. and Zhang, Y.},
  date = {2010-07},
  pages = {393--398},
  issn = {1550-6002},
  doi = {10.1109/QSIC.2010.23},
  abstract = {Program debugging is an important process to improve the quality of software. Runtime exception can occur often and can be difficult to debug. In this paper, we present a new approach for locating faults that cause runtime exceptions in Java programs due to error assignment of a value that finally leads to the exception. Our approach first uses program slicing to reduce the search scope, then performs a backward data flow analysis, starting from the point where the exception occurred, and then uses stack trace information to guide the analysis to determine the source statement that is responsible for the runtime exception. The paper also presents a case study to demonstrate that the method is available. The approach not only resolves partly the imprecise of static techniques, but also reduces partly the heavy performance overhead of dynamic techniques.},
  keywords = {Java,notion,Program Slicing,Runtime Exception}
}

@inproceedings{jiangLargeScaleEvaluationLog2024,
  title = {A {{Large-Scale Evaluation}} for {{Log Parsing Techniques}}: {{How Far Are We}}?},
  shorttitle = {A {{Large-Scale Evaluation}} for {{Log Parsing Techniques}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Jiang, Zhihan and Liu, Jinyang and Huang, Junjie and Li, Yichen and Huo, Yintong and Gu, Jiazhen and Chen, Zhuangbin and Zhu, Jieming and Lyu, Michael R.},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {223--234},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3652123},
  url = {https://dl.acm.org/doi/10.1145/3650212.3652123},
  urldate = {2024-09-20},
  abstract = {Log data have facilitated various tasks of software development and maintenance, such as testing, debugging and diagnosing. Due to the unstructured nature of logs, log parsing is typically required to transform log messages into structured data for automated log analysis. Given the abundance of log parsers that employ various techniques, evaluating these tools to comprehend their characteristics and performance becomes imperative. Loghub serves as a commonly used dataset for benchmarking log parsers, but it suffers from limited scale and representativeness, posing significant challenges for studies to comprehensively evaluate existing log parsers or develop new methods. This limitation is particularly pronounced when assessing these log parsers for production use. To address these limitations, we provide a new collection of annotated log datasets, denoted Loghub-2.0, which can better reflect the characteristics of log data in real-world software systems. Loghub-2.0 comprises 14 datasets with an average of 3.6 million log lines in each dataset. Based on Loghub-2.0, we conduct a thorough re-evaluation of 15 state-of-the-art log parsers in a more rigorous and practical setting. Particularly, we introduce a new evaluation metric to mitigate the sensitivity of existing metrics to imbalanced data distributions. We are also the first to investigate the granular performance of log parsers on logs that represent rare system events, offering in-depth details for software diagnosis. Accurately parsing such logs is essential, yet it remains a challenge. We believe this work could shed light on the evaluation and design of log parsers in practical settings, thereby facilitating their deployment in production systems.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/KRN2BZYZ/Jiang et al. - 2024 - A Large-Scale Evaluation for Log Parsing Techniques How Far Are We.pdf}
}

@article{jiangLocationPrivacypreservingMechanisms2021,
  title = {Location {{Privacy-preserving Mechanisms}} in {{Location-based Services}}: {{A Comprehensive Survey}}},
  author = {Jiang, Hongbo and Li, Jie and Zhao, Ping and Zeng, Fanzi and Xiao, Zhu and Iyengar, Arun},
  date = {2021-01},
  journaltitle = {ACM Comput. Surv.},
  volume = {54},
  number = {1},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {0360-0300},
  doi = {10.1145/3423165},
  url = {https://doi.org/10.1145/3423165},
  abstract = {Location-based services (LBSs) provide enhanced functionality and convenience of ubiquitous computing, but they open up new vulnerabilities that can be utilized to violate the users' privacy. The leakage of private location data in the LBS context has drawn significant attention from academics and industry due to its importance, leading to numerous research efforts aiming to confront the related challenges. However, to the best of our knowledge, none of relevant studies have performed a qualitative and quantitative comparison and analysis of the complex topic of designing countermeasures and discussed the viability of their use with different kinds of services and the potential elements that could be deployed to meet new challenges. Accordingly, the purpose of this survey is to examine the privacy-preserving techniques in LBSs. We categorize and provide an inside-out review of the existing techniques. Performing a retrospective analysis of several typical studies in each category, we summarize their basic principles and recent advances. Additionally, we highlight the use of privacy-preserving techniques in LBSs for enabling new research opportunities. Providing an up-to-date and comprehensive overview of existing studies, this survey may further stimulate new research efforts into this promising field.},
  keywords = {Location privacy,location-based services,notion,privacy-preserving mechanisms}
}

@article{jiangProgrammersChangeImpact2017,
  title = {Do {{Programmers}} Do {{Change Impact Analysis}} in {{Debugging}}?},
  author = {Jiang, Siyuan and McMillan, Collin and Santelices, Raul},
  date = {2017-04-01},
  journaltitle = {Empirical Software Engineering},
  volume = {22},
  number = {2},
  pages = {631--669},
  issn = {1573-7616},
  doi = {10.1007/s10664-016-9441-9},
  url = {https://doi.org/10.1007/s10664-016-9441-9},
  abstract = {“Change Impact Analysis” is the process of determining the consequences of a modification to software. In theory, change impact analysis should be done during software maintenance, to make sure changes do not introduce new bugs. Many approaches and techniques are proposed to help programmers do change impact analysis automatically. However, it is still an open question whether and how programmers do change impact analysis. In this paper, we conducted two studies, one in-depth study and one breadth study. For the in-depth study, we recorded videos of nine professional programmers repairing two bugs for two hours. For the breadth study, we surveyed 35 professional programmers using an online system. We found that the programmers in our studies did static change impact analysis before they made changes by using IDE navigational functionalities, and they did dynamic change impact analysis after they made changes by running the programs. We also found that they did not use any change impact analysis tools.},
  keywords = {notion}
}

@article{jimenezgilOpenChallengesProbabilistic2017,
  title = {Open {{Challenges}} for {{Probabilistic Measurement-Based Worst-Case Execution Time}}},
  author = {Jiménez Gil, Samuel and Bate, Iain and Lima, George and Santinelli, Luca and Gogonel, Adriana and Cucu-Grosjean, Liliana},
  date = {2017},
  journaltitle = {IEEE Embedded Systems Letters},
  volume = {9},
  number = {3},
  pages = {69--72},
  doi = {10.1109/LES.2017.2712858},
  keywords = {notion}
}

@inproceedings{jinBugReduxReproducingField2012,
  title = {{{BugRedux}}: {{Reproducing}} Field Failures for in-House Debugging},
  booktitle = {2012 34th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Jin, Wei and Orso, Alessandro},
  date = {2012},
  pages = {474--484},
  doi = {10.1109/ICSE.2012.6227168},
  keywords = {notion}
}

@inproceedings{jinF3FaultLocalization2013,
  title = {F3: {{Fault Localization}} for {{Field Failures}}},
  booktitle = {Proceedings of the 2013 {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Jin, Wei and Orso, Alessandro},
  date = {2013},
  series = {{{ISSTA}} 2013},
  pages = {213--223},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2483760.2483763},
  url = {https://doi.org/10.1145/2483760.2483763},
  abstract = {Reproducing and debugging field failures–failures that occur on user machines after release–are challenging tasks for developers. To help the first task, in previous work we have proposed BugRedux, a technique for reproducing, in-house, failures observed in the field. Although BugRedux can help developers reproduce field failures, it does not provide any specific support for debugging such failures. To address this limitation, in this paper we present F3, a novel technique that builds on BugRedux and extends it with support for fault localization. Specifically, in F3 we extend our previous technique in two main ways: first, we modify BugRedux so that it generates multiple failing and passing executions "similar" to the observed field failure; second, we add to BugRedux debugging capabilities by combining it with a customized fault-localization technique. The results of our empirical evaluation, performed on a set of real-world programs and field failures, are promising: for all the failures considered, F3 was able to (1) synthesize passing and failing executions and (2) successfully use the synthesized executions to perform fault localization and, ultimately, help debugging.},
  isbn = {978-1-4503-2159-4},
  venue = {Lugano, Switzerland},
  keywords = {Debugging,fault localization,field failures,notion}
}

@inproceedings{johnsonCausalTestingUnderstanding27,
  title = {Causal {{Testing}}: {{Understanding Defects}}' {{Root Causes}}},
  booktitle = {Proceedings of the 42nd {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Johnson, Brittany and Brun, Yuriy and Meliou, Alexandra},
  date = {0027/0029},
  location = {Seoul, Republic of Korea},
  doi = {10.1145/3377811.3380377},
  abstract = {{$<$}p{$>$}Isolating and repairing buggy software behavior requires finding where the bug is happening and understanding the root cause of the buggy behavior. While there exist tools that can help developers identify where a bug is located, existing work has paid little, if any, attention to helping developers understand the root cause of buggy behavior. We present Causal Testing, a new method of root-cause analysis that relies on the theory of counterfactual causality to identify a set of executions that likely hold key causal information necessary to understand and repair buggy behavior. Evaluating Causal Testing on the Defects4J benchmark, we find that Causal Testing could be applied to 71\% of real-world defects, and for 77\% of those, it can help developers identify the root cause of the defect. A controlled experiment with 37 developers showed that Causal Testing improved participants' ability to identify the cause of the defect: Users with standard testing tools correctly identified the cause 88\% of the time in comparison to 96\% of the time with Causal Testing. Overall, participants agreed Causal Testing provided useful information they could not get using tools like JUnit alone.{$<$}/p{$>$}},
  keywords = {notion}
}

@misc{joshyReproducingFailuresFault2023,
  title = {Reproducing {{Failures}} in {{Fault Signatures}}},
  author = {Joshy, Ashwin Kallingal and Steenhoek, Benjamin and Guo, Xiuyuan and Le, Wei},
  date = {2023},
  keywords = {notion}
}

@article{juangBiasTuringGoodEstimate1994,
  title = {On the Bias of the {{Turing-Good}} Estimate of Probabilities},
  author = {Juang, Biing-Hwang and family=Lo, given=SH, given-i=SH},
  date = {1994},
  journaltitle = {IEEE transactions on signal processing},
  volume = {42},
  number = {2},
  pages = {496--498},
  publisher = {IEEE},
  keywords = {notion}
}

@inproceedings{jungFuzzificationFuzzingTechniques2019,
  title = {Fuzzification: {{Anti-Fuzzing Techniques}}},
  booktitle = {28th {{USENIX Security Symposium}} ({{USENIX Security}} 19)},
  author = {Jung, Jinho and Hu, Hong and Solodukhin, David and Pagan, Daniel and Lee, Kyu Hyung and Kim, Taesoo},
  date = {2019-08},
  pages = {1913--1930},
  publisher = {USENIX Association},
  location = {Santa Clara, CA},
  url = {https://www.usenix.org/conference/usenixsecurity19/presentation/jung},
  isbn = {978-1-939133-06-9},
  keywords = {notion}
}

@inproceedings{jungTamingFalseAlarms2005,
  title = {Taming {{False Alarms}} from a {{Domain-Unaware C Analyzer}} by a {{Bayesian Statistical Post Analysis}}},
  booktitle = {Static {{Analysis}}},
  author = {Jung, Yungbum and Kim, Jaehwang and Shin, Jaeho and Yi, Kwangkeun},
  editor = {Hankin, Chris and Siveroni, Igor},
  date = {2005},
  pages = {203--217},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  abstract = {We present our experience of combining, in a realistic setting, a static analyzer with a statistical analysis. This combination is in order to reduce the inevitable false alarms from a domain-unaware static analyzer. Our analyzer named Airac(Array Index Range Analyzer for C) collects all the true buffer-overrun points in ANSI C programs. The soundness is maintained, and the analysis' cost-accuracy improvement is achieved by techniques that static analysis community has long accumulated. For still inevitable false alarms (e.g. Airac raised 970 buffer-overrun alarms in commercial C programs of 5.3 million lines and 737 among the 970 alarms were false), which are always apt for particular C programs, we use a statistical post analysis. The statistical analysis, given the analysis results (alarms), sifts out probable false alarms and prioritizes true alarms. It estimates the probability of each alarm being true. The probabilities are used in two ways: 1) only the alarms that have true-alarm probabilities higher than a threshold are reported to the user; 2) the alarms are sorted by the probability before reporting, so that the user can check highly probable errors first. In our experiments with Linux kernel sources, if we set the risk of missing true error is about 3 times greater than false alarming, 74.83\% of false alarms could be filtered; only 15.17\% of false alarms were mixed up until the user observes 50\% of the true alarms.},
  isbn = {978-3-540-31971-9},
  keywords = {notion}
}

@inproceedings{kalhaugeBinaryReductionDependency2019,
  title = {Binary {{Reduction}} of {{Dependency Graphs}}},
  booktitle = {Proceedings of the 2019 27th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Kalhauge, Christian Gram and Palsberg, Jens},
  date = {2019},
  series = {{{ESEC}}/{{FSE}} 2019},
  pages = {556--566},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3338906.3338956},
  url = {https://doi.org/10.1145/3338906.3338956},
  isbn = {978-1-4503-5572-8},
  venue = {Tallinn, Estonia},
  keywords = {Debugging,dependencies,notion,reduction}
}

@unpublished{kandasamyNeuralArchitectureSearch2018,
  title = {Neural {{Architecture Search}} with {{Bayesian Optimisation}} and {{Optimal Transport}}},
  author = {Kandasamy, Kirthevasan and Neiswanger, Willie and Schneider, Jeff and Poczos, Barnabas and Xing, Eric},
  date = {2018},
  eprint = {1802.07191},
  eprinttype = {arXiv},
  keywords = {notion}
}

@unpublished{kangBayesianFrameworkAutomated2022,
  title = {A {{Bayesian Framework}} for {{Automated Debugging}}},
  author = {Kang, Sungmin and Choi, Wonkeun and Yoo, Shin},
  date = {2022},
  eprint = {2212.13773},
  eprinttype = {arXiv},
  keywords = {notion}
}

@misc{kangGLADNeuralPredicate2022,
  title = {{{GLAD}}: {{Neural Predicate Synthesis}} to {{Repair Omission Faults}}},
  author = {Kang, Sungmin and Yoo, Shin},
  date = {2022},
  doi = {10.48550/ARXIV.2204.06771},
  url = {https://arxiv.org/abs/2204.06771},
  organization = {arXiv},
  keywords = {FOS: Computer and information sciences,notion,Software Engineering (cs.SE)}
}

@inproceedings{kangLargeLanguageModels2023,
  title = {Large {{Language Models}} Are {{Few-shot Testers}}: {{Exploring LLM-based General Bug Reproduction}}},
  booktitle = {2023 {{IEEE}}/{{ACM}} 45th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Kang, Sungmin and Yoon, Juyeon and Yoo, Shin},
  date = {2023},
  pages = {2312--2323},
  doi = {10.1109/ICSE48619.2023.00194},
  keywords = {Benchmark testing,Codes,Computer bugs,natural language processing,notion,Semantics,software engineering,test generation,Test pattern generators,Training data,Writing}
}

@inproceedings{kaniyurInterproceduralPathComplexity2024,
  title = {Interprocedural {{Path Complexity Analysis}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Kaniyur, Mira and Cavalcante-Studart, Ana and Yang, Yihan and Park, Sangeon and Chen, David and Lam, Duy and Bang, Lucas},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {162--173},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3652118},
  url = {https://dl.acm.org/doi/10.1145/3650212.3652118},
  urldate = {2024-09-21},
  abstract = {Software testing techniques like symbolic execution face significant challenges with path explosion. Asymptotic Path Complexity (APC) quantifies this path explosion complexity, but existing APC methods do not work for interprocedural functions in general. Our new algorithm, APC-IP, efficiently computes APC for a wider range of functions, including interprocedural ones, improving over previous methods in both speed and scope. We implement APC-IP atop the existing software Metrinome, and test it against a benchmark of C functions, comparing it to existing and baseline approaches as well as comparing it to the path explosion of the symbolic execution engine Klee. The results show that APC-IP not only aligns with previous APC values but also excels in performance, scalability, and handling complex source code. It also provides a complexity prediction of the number of paths explored by Klee, extending the APC metric's applicability and surpassing previous implementations.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/SFKK8474/Kaniyur et al. - 2024 - Interprocedural Path Complexity Analysis.pdf}
}

@inproceedings{karampatsisBigCodeBig2020,
  title = {Big {{Code}} != {{Big Vocabulary}}: {{Open-Vocabulary Models}} for {{Source Code}}},
  booktitle = {International {{Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Karampatsis, Rafael-Michael and Babii, Hlib and Robbes, Romain and Sutton, Charles and Janes, Andrea},
  date = {2020},
  keywords = {notion}
}

@inproceedings{kargenTurningProgramsEach2015,
  title = {Turning {{Programs}} against {{Each Other}}: {{High Coverage Fuzz-Testing Using Binary-Code Mutation}} and {{Dynamic Slicing}}},
  booktitle = {Proceedings of the 2015 10th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Kargén, Ulf and Shahmehri, Nahid},
  date = {2015},
  series = {{{ESEC}}/{{FSE}} 2015},
  pages = {782--792},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2786805.2786844},
  url = {https://doi.org/10.1145/2786805.2786844},
  abstract = {Mutation-based fuzzing is a popular and widely employed black-box testing technique for finding security and robustness bugs in software. It owes much of its success to its simplicity; a well-formed seed input is mutated, e.g. through random bit-flipping, to produce test inputs. While reducing the need for human effort, and enabling security testing even of closed-source programs with undocumented input formats, the simplicity of mutation-based fuzzing comes at the cost of poor code coverage. Often millions of iterations are needed, and the results are highly dependent on configuration parameters and the choice of seed inputs. In this paper we propose a novel method for automated generation of high-coverage test cases for robustness testing. Our method is based on the observation that, even for closed-source programs with proprietary input formats, an implementation that can generate well-formed inputs to the program is typically available. By systematically mutating the program code of such generating programs, we leverage information about the input format encoded in the generating program to produce high-coverage test inputs, capable of reaching deep states in the program under test. Our method works entirely at the machine-code level, enabling use-cases similar to traditional black-box fuzzing. We have implemented the method in our tool MutaGen, and evaluated it on 7 popular Linux programs. We found that, for most programs, our method improves code coverage by one order of magnitude or more, compared to two well-known mutation-based fuzzers. We also found a total of 8 unique bugs.},
  isbn = {978-1-4503-3675-8},
  venue = {Bergamo, Italy},
  keywords = {black-box,dynamic slicing,Fuzz testing,fuzzing,notion,program mutation}
}

@article{karimPlatformIndependentDynamicTaint2018,
  title = {Platform-{{Independent Dynamic Taint Analysis}} for {{JavaScript}}},
  author = {Karim, R. and Tip, F. and Sochurkova, A. and Sen, K.},
  date = {2018},
  journaltitle = {IEEE Transactions on Software Engineering},
  pages = {1--1},
  issn = {2326-3881},
  doi = {10.1109/TSE.2018.2878020},
  keywords = {Browsers,Data privacy,dynamic analysis,Engines,Gears,instrumentation,Instruments,JavaScript,notion,platform-independent,Privacy,Taint analysis,Tools}
}

@misc{karmakarWhatPretrainedCode2021,
  title = {What Do Pre-Trained Code Models Know about Code?},
  author = {Karmakar, Anjan and Robbes, Romain},
  date = {2021},
  keywords = {notion}
}

@article{katoenProbabilisticModelChecking2016,
  title = {The {{Probabilistic Model Checking Landscape}}*},
  author = {Katoen, Joost-Pieter},
  date = {2016},
  journaltitle = {2016 31st Annual ACM/IEEE Symposium on Logic in Computer Science (LICS)},
  pages = {1--15},
  keywords = {notion}
}

@inproceedings{kaufmanPrioritizingMutantsGuide2022,
  title = {Prioritizing {{Mutants}} to {{Guide Mutation Testing}}},
  booktitle = {2022 {{IEEE}}/{{ACM}} 44th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Kaufman, Samuel J. and Featherman, Ryan and Alvin, Justin and Kurtz, Bob and Ammann, Paul and Just, René},
  date = {2022},
  pages = {1743--1754},
  doi = {10.1145/3510003.3510187},
  keywords = {Analytical models,Codes,Current measurement,Focusing,machine learning,mutant selection,mutant utility,mutation testing,notion,Predictive models,Semantics,Syntactics,TCAP,test completeness advancement probability}
}

@inproceedings{kawamotoHybridStatisticalEstimation2016,
  title = {Hybrid Statistical Estimation of Mutual Information for Quantifying Information Flow},
  booktitle = {{{FM}} 2016: {{Formal Methods}}: 21st {{International Symposium}}, {{Limassol}}, {{Cyprus}}, {{November}} 9-11, 2016, {{Proceedings}}},
  author = {Kawamoto, Yusuke and Biondi, Fabrizio and Legay, Axel},
  date = {2016},
  pages = {406--425},
  publisher = {Springer},
  keywords = {notion}
}

@article{kendallNewMeasureRank1938,
  title = {A New Measure of Rank Correlation},
  author = {Kendall, Maurice G},
  date = {1938},
  journaltitle = {Biometrika},
  volume = {30},
  number = {1/2},
  pages = {81--93},
  publisher = {JSTOR},
  keywords = {notion}
}

@inproceedings{keRepairingProgramsSemantic2015,
  title = {Repairing {{Programs}} with {{Semantic Code Search}} ({{T}})},
  booktitle = {2015 30th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Ke, Y. and Stolee, K. T. and Goues, C. L. and Brun, Y.},
  date = {2015-11},
  pages = {295--306},
  doi = {10.1109/ASE.2015.60},
  abstract = {Automated program repair can potentially reduce debugging costs and improve software quality but recent studies have drawn attention to shortcomings in the quality of automatically generated repairs. We propose a new kind of repair that uses the large body of existing open-source code to find potential fixes. The key challenges lie in efficiently finding code semantically similar (but not identical) to defective code and then appropriately integrating that code into a buggy program. We present SearchRepair, a repair technique that addresses these challenges by(1) encoding a large database of human-written code fragments as SMT constraints on input-output behavior, (2) localizing a given defect to likely buggy program fragments and deriving the desired input-output behavior for code to replace those fragments, (3) using state-of-the-art constraint solvers to search the database for fragments that satisfy that desired behavior and replacing the likely buggy code with these potential patches, and (4) validating that the patches repair the bug against program testsuites. We find that SearchRepair repairs 150 (19\%) of 778 benchmark C defects written by novice students, 20 of which are not repaired by GenProg, TrpAutoRepair, and AE. We compare the quality of the patches generated by the four techniques by measuring how many independent, not-used-during-repairtests they pass, and find that SearchRepair-repaired programs pass 97.3\% ofthe tests, on average, whereas GenProg-, TrpAutoRepair-, and AE-repaired programs pass 68.7\%, 72.1\%, and 64.2\% of the tests, respectively. We concludethat SearchRepair produces higher-quality repairs than GenProg, TrpAutoRepair, and AE, and repairs some defects those tools cannot.},
  keywords = {AE-repaired programs,automated repair,Benchmark testing,buggy code,C defects,Computer bugs,debugging,fault localization,GenProg-repaired program,human-written code fragments,Indexing,Maintenance engineering,notion,program debugging,program diagnostics,program repair,repair quality,SearchRepair,SearchRepair technique,semantic code search,Semantics,SMT constraints,Software,software quality,TrpAutoRepair-repaired program}
}

@inproceedings{khemakhemVariationalAutoencodersNonlinear2020,
  title = {Variational {{Autoencoders}} and {{Nonlinear ICA}}: {{A Unifying Framework}}},
  booktitle = {Proceedings of the {{Twenty Third International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Khemakhem, Ilyes and Kingma, Diederik and Monti, Ricardo and Hyvarinen, Aapo},
  editor = {Chiappa, Silvia and Calandra, Roberto},
  date = {2020-08-26},
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {108},
  pages = {2207--2217},
  publisher = {PMLR},
  url = {https://proceedings.mlr.press/v108/khemakhem20a.html},
  abstract = {The framework of variational autoencoders allows us to efficiently learn deep latent-variable models, such that the model's marginal distribution over observed variables fits the data. Often, we're interested in going a step further, and want to approximate the true joint distribution over observed and latent variables, including the true prior and posterior distributions over latent variables. This is known to be generally impossible due to unidentifiability of the model. We address this issue by showing that for a broad family of deep latent-variable models, identification of the true joint distribution over observed and latent variables is actually possible up to very simple transformations, thus achieving a principled and powerful form of disentanglement. Our result requires a factorized prior distribution over the latent variables that is conditioned on an additionally observed variable, such as a class label or almost any other observation. We build on recent developments in nonlinear ICA, which we extend to the case with noisy, undercomplete or discrete observations, integrated in a maximum likelihood framework. The result also trivially contains identifiable flow-based generative models as a special case.},
  keywords = {notion}
}

@article{kimAsFuzzerDifferentialTesting2024,
  title = {{{AsFuzzer}}: {{Differential Testing}} of {{Assemblers}} with {{Error-Driven Grammar Inference}}},
  shorttitle = {{{AsFuzzer}}},
  author = {Kim, Hyungseok and {https://orcid.org/0009-0008-2158-9367} and {View Profile} and Kim, Soomin and {https://orcid.org/0000-0003-3129-3857} and {View Profile} and Lee, Jungwoo and {https://orcid.org/0009-0006-2631-2741} and {View Profile} and Cha, Sang Kil and {https://orcid.org/0000-0002-6012-7228} and {View Profile}},
  date = {2024-09-11},
  journaltitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
  series = {{{ACM Conferences}}},
  pages = {1099--1111},
  issn = {9798400706127},
  doi = {10.1145/3650212.3680345},
  url = {https://dl.acm.org/doi/10.1145/3650212.3680345},
  urldate = {2024-09-21},
  abstract = {Assembler is a critical component of the compiler toolchain, which has been less tested than the other components. Unfortunately, current grammar-based fuzzing techniques suffer from several challenges when testing assemblers. First, each different assembler accepts different grammar rules and syntaxes, and there are no existing assembly grammar specifications. Second, not every assembler is open-source, which makes it difficult to extract grammar rules from the source code. While existing black-box grammar inference approaches are applicable to such closed-source assemblers, they suffer from the scalability issue, which renders them impractical for testing assemblers. To address these challenges, we propose a novel way to test assemblers by automatically inferring their grammar rules with only a few queries to the target assemblers by leveraging their error messages. The key insight is that assembly error messages often deliver useful information to infer the underlying grammar rules. We have implemented our technique in a tool named AsFuzzer, and evaluated it on 4 real-world assemblers including Clang-integrated assembler (Clang), GNU assembler (GAS), Intel’s assembler (ICC), and Microsoft macro assembler (MASM). With AsFuzzer, we have successfully found 497 buggy instruction opcodes for six popular architectures, and reported them to the developers.},
  keywords = {assembler testing,compiler testing,grammar inference,notion},
  file = {/Users/bohrok/Zotero/storage/R4DCQDXG/Kim et al. - 2024 - AsFuzzer Differential Testing of Assemblers with Error-Driven Grammar Inference.pdf}
}

@inproceedings{kimAutomaticPatchGeneration2013,
  title = {Automatic {{Patch Generation Learned}} from {{Human-written Patches}}},
  booktitle = {Proceedings of the 2013 {{International Conference}} on {{Software Engineering}}},
  author = {Kim, Dongsun and Nam, Jaechang and Song, Jaewoo and Kim, Sunghun},
  date = {2013},
  series = {{{ICSE}} '13},
  pages = {802--811},
  publisher = {IEEE Press},
  location = {Piscataway, NJ, USA},
  url = {http://dl.acm.org/citation.cfm?id=2486788.2486893},
  isbn = {978-1-4673-3076-3},
  venue = {San Francisco, CA, USA},
  keywords = {notion}
}

@inproceedings{kimInvasiveSoftwareTesting2018,
  title = {Invasive {{Software Testing}}: {{Mutating Target Programs}} to {{Diversify Test Exploration}} for {{High Test Coverage}}},
  author = {Kim, Yunho and Hong, Shin and Ko, Bongseok and Phan, Duy and Kim, Moonzoo},
  date = {2018-04},
  pages = {239--249},
  doi = {10.1109/ICST.2018.00032},
  keywords = {notion}
}

@inproceedings{kimStatisticalModelChecking2013,
  title = {Statistical {{Model Checking}} for {{Safety Critical Hybrid Systems}}: {{An Empirical Evaluation}}},
  booktitle = {Hardware and {{Software}}: {{Verification}} and {{Testing}}},
  author = {Kim, Youngjoo and Kim, Moonzoo and Kim, Tai-Hyo},
  editor = {Biere, Armin and Nahir, Amir and Vos, Tanja},
  date = {2013},
  pages = {162--177},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  abstract = {As more computing systems are utilized in various areas of our society, the reliability of computing systems becomes a significant issue. However, as the complexity of computing systems increases, conventional verification and validation techniques such as testing and model checking have limitations to assess reliability of complex safety critical systems. Such systems often control highly complex continuous dynamics to interact with physical environments. To assure the reliability of safety critical hybrid systems, statistical model checking (SMC) techniques have been proposed. SMC techniques approximately compute probabilities for a target system to satisfy given requirements based on randomly sampled execution traces. In this paper, we empirically evaluated four state-ofthe- art SMC techniques on a fault-tolerant fuel control system in the automobile domain. Through the experiments, we could demonstrate that SMC is practically useful to assure the reliability of a safety critical hybrid system and we compared pros and cons of the four different SMC techniques.},
  isbn = {978-3-642-39611-3},
  keywords = {notion}
}

@inproceedings{kimTargetDrivenCompositionalConcolic2019,
  title = {Target-{{Driven Compositional Concolic Testing}} with {{Function Summary Refinement}} for {{Effective Bug Detection}}},
  booktitle = {Proceedings of the 2019 27th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Kim, Yunho and Hong, Shin and Kim, Moonzoo},
  date = {2019},
  series = {{{ESEC}}/{{FSE}} 2019},
  pages = {16--26},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3338906.3338934},
  url = {https://doi.org/10.1145/3338906.3338934},
  abstract = {Concolic testing is popular in unit testing because it can detect bugs quickly in a relatively small search space. But, in system-level testing, it suffers from the symbolic path explosion and often misses bugs. To resolve this problem, we have developed a focused compositional concolic testing technique, FOCAL, for effective bug detection. Focusing on a target unit failure v (a crash or an assert violation) detected by concolic unit testing, FOCAL generates a system-level test input that validates v. This test input is obtained by building and solving symbolic path formulas that represent system-level executions raising v. FOCAL builds such formulas by combining function summaries one by one backward from a function that raised v to main. If a function summary φa of function a conflicts with the summaries of the other functions, FOCAL refines φa to φa′ by applying a refining constraint learned from the conflict. FOCAL showed high system-level bug detection ability by detecting 71 out of the 100 real-world target bugs in the SIR benchmark, while other relevant cutting edge techniques (i.e., AFL-fast, KATCH, Mix-CCBSE) detected at most 40 bugs. Also, FOCAL detected 13 new crash bugs in popular file parsing programs.},
  isbn = {978-1-4503-5572-8},
  venue = {Tallinn, Estonia},
  keywords = {Automated test generation,craig interpolant,dynamic symbolic execution,function summary refinement,notion,target-driven compositional concolic testing}
}

@article{kimVFLVariablebasedFault2019,
  title = {{{VFL}}: {{Variable-based}} Fault Localization},
  author = {Kim, Jeongho and Kim, Jindae and Lee, Eunseok},
  date = {2019},
  journaltitle = {Information and Software Technology},
  volume = {107},
  pages = {179--191},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2018.11.009},
  url = {http://www.sciencedirect.com/science/article/pii/S0950584918302453},
  abstract = {ABSTRACT Context Fault localization is one of the most important debugging tasks. Hence, many automatic fault localization techniques have been proposed to reduce the burden on developers for such tasks. Among them, Spectrum-based Fault Localization (SFL) techniques leverage coverage information and localize faults based on the coverage difference between the failed and passed test cases. Objective However, such SFL techniques cannot localize faults effectively when coverage differences are not clear. To address this issue and improve the fault localization performance of the SFL techniques, we propose a Variable-based Fault Localization (VFL) technique. Method The VFL technique identifies suspicious variables and uses them to generate a ranked list of suspicious source code lines. Since it only requires additional information about variables that are also available in the SFL techniques, the proposed technique is lightweight and can be used to improve the performance of existing the SFL techniques. Results In an evaluation with 224 real Java faults and 120 C faults, the VFL technique outperforms the SFL techniques using the same similarity coefficient. The average Exam scores of the VFL techniques are reduced by more than 55\% compared to the SFL techniques, and the VFL techniques localize faults at a lower rank than the SFL techniques for about 73\% of the 344 faults. Conclusion We proposed a novel variable-based fault localization technique for more effective debugging. The VFL technique has better performance than the existing techniques and the results were more useful for actual fault localization tasks. In addition, this technique is very lightweight and scalable, so it is very easy to collaborate with other fault localization techniques.},
  keywords = {notion,Software debugging,Software testing,Spectrum-based fault localization,Suspicious variable,Variable-based fault localization}
}

@unpublished{kingmaAutoencodingVariationalBayes2013,
  title = {Auto-Encoding Variational Bayes},
  author = {Kingma, Diederik P and Welling, Max},
  date = {2013},
  eprint = {1312.6114},
  eprinttype = {arXiv},
  keywords = {notion}
}

@article{kingSymbolicExecutionProgram1976,
  title = {Symbolic Execution and Program Testing},
  author = {King, James C},
  date = {1976},
  journaltitle = {Communications of the ACM},
  volume = {19},
  number = {7},
  pages = {385--394},
  publisher = {ACM New York, NY, USA},
  keywords = {notion}
}

@article{kintisHowEffectiveAre2018,
  title = {How Effective Are Mutation Testing Tools? {{An}} Empirical Analysis of {{Java}} Mutation Testing Tools with Manual Analysis and Real Faults},
  author = {Kintis, Marinos and Papadakis, Mike and Papadopoulos, Andreas and Valvis, Evangelos and Malevris, Nicos and Le Traon, Yves},
  date = {2018},
  journaltitle = {Empirical Software Engineering},
  volume = {23},
  number = {4},
  pages = {2426--2463},
  doi = {10.1007/s10664-017-9582-5},
  url = {https://doi.org/10.1007/s10664-017-9582-5},
  abstract = {Mutation analysis is a well-studied, fault-based testing technique. It requires testers to design tests based on a set of artificial defects. The defects help in performing testing activities by measuring the ratio that is revealed by the candidate tests. Unfortunately, applying mutation to real-world programs requires automated tools due to the vast number of defects involved. In such a case, the effectiveness of the method strongly depends on the peculiarities of the employed tools. Thus, when using automated tools, their implementation inadequacies can lead to inaccurate results. To deal with this issue, we cross-evaluate four mutation testing tools for Java, namely PIT, muJava, Major and the research version of PIT, PITRV, with respect to their fault-detection capabilities. We investigate the strengths of the tools based on: a) a set of real faults and b) manual analysis of the mutants they introduce. We find that there are large differences between the tools'effectiveness and demonstrate that no tool is able to subsume the others. We also provide results indicating the application cost of the method. Overall, we find that PITRV achieves the best results. In particular, PITRV outperforms the other tools by finding 6\% more faults than the other tools combined.},
  isbn = {1573-7616},
  keywords = {notion}
}

@article{kirchnerFramaCSoftwareAnalysis2015,
  title = {Frama-{{C}}: {{A}} Software Analysis Perspective},
  author = {Kirchner, Florent and Kosmatov, Nikolai and Prevosto, Virgile and Signoles, Julien and Yakobowski, Boris},
  date = {2015},
  journaltitle = {Formal Aspects of Computing},
  volume = {27},
  number = {3},
  pages = {573--609},
  doi = {10.1007/s00165-014-0326-7},
  url = {https://doi.org/10.1007/s00165-014-0326-7},
  abstract = {Frama-C is a source code analysis platform that aims at conducting verification of industrial-size C programs. It provides its users with a collection of plug-ins that perform static analysis, deductive verification, and testing, for safety- and security-critical software. Collaborative verification across cooperating plug-ins is enabled by their integration on top of a shared kernel and datastructures, and their compliance to a common specification language. This foundational article presents a consolidated view of the platform, its main and composite analyses, and some of its industrial achievements.},
  isbn = {1433-299X},
  keywords = {notion}
}

@inproceedings{kirinukiHeyAreYou2014a,
  title = {Hey! {{Are You Committing Tangled Changes}}?},
  booktitle = {Proceedings of the {{22Nd International Conference}} on {{Program Comprehension}}},
  author = {Kirinuki, Hiroyuki and Higo, Yoshiki and Hotta, Keisuke and Kusumoto, Shinji},
  date = {2014},
  series = {{{ICPC}} 2014},
  pages = {262--265},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2597008.2597798},
  url = {http://doi.acm.org/10.1145/2597008.2597798},
  isbn = {978-1-4503-2879-1},
  venue = {Hyderabad, India},
  keywords = {Mining software repositories,notion,Tangled changes,Understanding commits}
}

@inproceedings{kirschnerDebuggingInputs2020,
  title = {Debugging Inputs},
  booktitle = {Proceedings of the {{ACM}}/{{IEEE}} 42nd {{International Conference}} on {{Software Engineering}}},
  author = {Kirschner, Lukas and Soremekun, Ezekiel and Zeller, Andreas},
  date = {2020},
  series = {{{ICSE}} '20},
  pages = {75--86},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3377811.3380329},
  url = {https://doi.org/10.1145/3377811.3380329},
  abstract = {When a program fails to process an input, it need not be the program code that is at fault. It can also be that the input data is faulty, for instance as result of data corruption. To get the data processed, one then has to debug the input data—that is, (1) identify which parts of the input data prevent processing, and (2) recover as much of the (valuable) input data as possible. In this paper, we present a general-purpose algorithm called ddmax that addresses these problems automatically. Through experiments, ddmax maximizes the subset of the input that can still be processed by the program, thus recovering and repairing as much data as possible; the difference between the original failing input and the "maximized" passing input includes all input fragments that could not be processed. To the best of our knowledge, ddmax is the first approach that fixes faults in the input data without requiring program analysis. In our evaluation, ddmax repaired about 69\% of input files and recovered about 78\% of data within one minute per input.},
  isbn = {978-1-4503-7121-6},
  venue = {Seoul, South Korea},
  keywords = {notion}
}

@article{kirschnerInputRepairSynthesis2022,
  title = {Input {{Repair}} via {{Synthesis}} and {{Lightweight Error Feedback}}},
  author = {Kirschner, Lukas and Soremekun, Ezekiel O. and Gopinath, Rahul and Zeller, Andreas},
  date = {2022},
  journaltitle = {ArXiv},
  volume = {abs/2208.08235},
  keywords = {notion}
}

@inproceedings{kissCombiningStaticDynamic2015,
  title = {Combining {{Static}} and {{Dynamic Analyses}} for {{Vulnerability Detection}}: {{Illustration}} on {{Heartbleed}}},
  booktitle = {Haifa {{Verification Conference}}},
  author = {Kiss, Balázs and Kosmatov, Nikolai and Pariente, Dillon and Puccetti, Armand},
  date = {2015},
  url = {https://api.semanticscholar.org/CorpusID:35939720},
  keywords = {notion}
}

@inproceedings{klebanovSATbasedAnalysisQuantification2013,
  title = {{{SAT-based}} Analysis and Quantification of Information Flow in Programs},
  booktitle = {International {{Conference}} on {{Quantitative Evaluation}} of {{Systems}}},
  author = {Klebanov, Vladimir and Manthey, Norbert and Muise, Christian},
  date = {2013},
  pages = {177--192},
  publisher = {Springer},
  keywords = {notion}
}

@article{klebanovSoundProbabilisticSAT2016,
  title = {Sound {{Probabilistic}} \#{{SAT}} with {{Projection}}},
  author = {Klebanov, Vladimir and Weigl, Alexander and Weisbarth, Jörg},
  date = {2016-10},
  journaltitle = {Electronic Proceedings in Theoretical Computer Science},
  volume = {227},
  pages = {15--29},
  publisher = {Open Publishing Association},
  doi = {10.4204/eptcs.227.2},
  url = {https://doi.org/10.4204%2Feptcs.227.2},
  keywords = {notion}
}

@inproceedings{kleinSeL4FormalVerification2009,
  title = {{{seL4}}: Formal Verification of an {{OS}} Kernel},
  booktitle = {Symposium on {{Operating Systems Principles}}},
  author = {Klein, Gerwin and Elphinstone, Kevin and Heiser, Gernot and Andronick, June and Cock, David A. and Derrin, Philip and Elkaduwe, Dhammika and Engelhardt, Kai and Kolanski, Rafal and Norrish, Michael and Sewell, Thomas and Tuch, Harvey and Winwood, Simon},
  date = {2009},
  keywords = {notion}
}

@article{kloosterContinuousFuzzingStudy2023,
  title = {Continuous {{Fuzzing}}: {{A Study}} of the {{Effectiveness}} and {{Scalability}} of {{Fuzzing}} in {{CI}}/{{CD Pipelines}}},
  author = {Klooster, Thijs and Turkmen, Fatih and Broenink, Gerben and family=Hove, given=Ruben, prefix=ten, useprefix=false and Böhme, Marcel},
  date = {2023},
  journaltitle = {2023 IEEE/ACM International Workshop on Search-Based and Fuzz Testing (SBFT)},
  pages = {25--32},
  url = {https://api.semanticscholar.org/CorpusID:260255199},
  keywords = {notion}
}

@article{kobayashiMiningCausalityNetwork2018,
  title = {Mining {{Causality}} of {{Network Events}} in {{Log Data}}},
  author = {Kobayashi, Satoru and Otomo, Kazuki and Fukuda, Kensuke and Esaki, Hiroshi},
  date = {2018},
  journaltitle = {IEEE Transactions on Network and Service Management},
  volume = {15},
  number = {1},
  pages = {53--67},
  doi = {10.1109/TNSM.2017.2778096},
  keywords = {notion}
}

@article{kochharLargeScaleStudy2016,
  title = {A {{Large Scale Study}} of {{Multiple Programming Languages}} and {{Code Quality}}},
  author = {Kochhar, Pavneet Singh and Wijedasa, Dinusha and Lo, D.},
  date = {2016},
  journaltitle = {2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)},
  volume = {1},
  pages = {563--573},
  keywords = {notion}
}

@article{koDesigningWhylineDebugging2004,
  title = {Designing the Whyline: A Debugging Interface for Asking Questions about Program Behavior},
  author = {Ko, Amy J. and Myers, Brad A.},
  date = {2004},
  journaltitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  keywords = {notion}
}

@article{koExploratoryStudyHow2006,
  title = {An Exploratory Study of How Developers Seek, Relate, and Collect Relevant Information during Software Maintenance Tasks},
  author = {Ko, Andrew J and Myers, Brad A and Coblenz, Michael J and Aung, Htet Htet},
  date = {2006},
  journaltitle = {IEEE Transactions on software engineering},
  volume = {32},
  number = {12},
  pages = {971--987},
  publisher = {IEEE},
  keywords = {notion}
}

@article{koExtractingAnsweringWhy2010,
  title = {Extracting and {{Answering Why}} and {{Why Not Questions About Java Program Output}}},
  author = {Ko, Andrew J. and Myers, Brad A.},
  date = {2010-09},
  journaltitle = {ACM Trans. Softw. Eng. Methodol.},
  volume = {20},
  number = {2},
  pages = {4:1--4:36},
  publisher = {ACM},
  location = {New York, NY, USA},
  issn = {1049-331X},
  doi = {10.1145/1824760.1824761},
  url = {http://doi.acm.org/10.1145/1824760.1824761},
  keywords = {Java,notion,Question}
}

@article{komuravelliAssumeGuaranteeAbstractionRefinement2012,
  title = {Assume-{{Guarantee Abstraction Refinement}} for {{Probabilistic Systems}}},
  author = {Komuravelli, Anvesh and Pasareanu, Corina S. and Clarke, Edmund M.},
  date = {2012},
  journaltitle = {ArXiv},
  volume = {abs/1207.5086},
  keywords = {notion}
}

@inproceedings{kopfApproximationRandomizationQuantitative2010,
  title = {Approximation and {{Randomization}} for {{Quantitative Information-Flow Analysis}}},
  booktitle = {2010 23rd {{IEEE Computer Security Foundations Symposium}}},
  author = {Köpf, Boris and Rybalchenko, Andrey},
  date = {2010},
  pages = {3--14},
  doi = {10.1109/CSF.2010.8},
  keywords = {notion}
}

@inproceedings{kopfInformationTheoreticModelAdaptive2007,
  title = {An {{Information-Theoretic Model}} for {{Adaptive Side-Channel Attacks}}},
  booktitle = {Proceedings of the 14th {{ACM Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Köpf, Boris and Basin, David},
  date = {2007},
  series = {{{CCS}} '07},
  pages = {286--296},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1315245.1315282},
  url = {https://doi.org/10.1145/1315245.1315282},
  abstract = {We present a model of adaptive side-channel attacks which we combine with information-theoretic metrics to quantify the information revealed to an attacker. This allows us to express an attacker's remaining uncertainty about a secret as a function of the number of side-channel measurements made. We present algorithms and approximation techniques for computing this measure. We also give examples of how they can be used to analyze the resistance of hardware implementations of cryptographic functions to both timing and power attacks.},
  isbn = {978-1-59593-703-2},
  venue = {Alexandria, Virginia, USA},
  keywords = {notion}
}

@article{korelAutomatedRegressionTest1998,
  title = {Automated Regression Test Generation},
  author = {Korel, Bogdan and Al-Yami, Ali M.},
  date = {1998-03},
  journaltitle = {SIGSOFT Softw. Eng. Notes},
  volume = {23},
  number = {2},
  pages = {143--152},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {0163-5948},
  doi = {10.1145/271775.271803},
  url = {https://doi.org/10.1145/271775.271803},
  abstract = {Regression testing involves testing the modified program in order to establish the confidence in the modifications. Existing regression testing methods generate test cases to satisfy selected testing criteria in the hope that this process may reveal faults in the modified program. In this paper we present a novel approach of automated regression test generation in which all generated test cases uncover an error(s). This approach is used to test the common functionality of the original program and its modified version, i.e., it is used for programs whose functionality is unchanged after modifications. The goal in this approach is to identify test cases for which the original program and the modified program produce different outputs. If such a test is found, then this test uncovers an error. The problem of finding such a test case may be reduced to the problem of finding program input on which a selected statement is executed. As a result, existing methods of automated test data generation for white-box testing may be used to generate these tests. Our experiments have shown that our approach may improve the chances of finding software errors as compared to the existing methods of regression testing. The advantage of our approach is that it is fully automated and that all generated test cases reveal an error(s).},
  keywords = {notion}
}

@inproceedings{korelAutomatedRegressionTest1998a,
  title = {Automated Regression Test Generation},
  booktitle = {Proceedings of the 1998 {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Korel, Bogdan and Al-Yami, Ali M.},
  date = {1998},
  series = {{{ISSTA}} '98},
  pages = {143--152},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/271771.271803},
  url = {https://doi.org/10.1145/271771.271803},
  abstract = {Regression testing involves testing the modified program in order to establish the confidence in the modifications. Existing regression testing methods generate test cases to satisfy selected testing criteria in the hope that this process may reveal faults in the modified program. In this paper we present a novel approach of automated regression test generation in which all generated test cases uncover an error(s). This approach is used to test the common functionality of the original program and its modified version, i.e., it is used for programs whose functionality is unchanged after modifications. The goal in this approach is to identify test cases for which the original program and the modified program produce different outputs. If such a test is found, then this test uncovers an error. The problem of finding such a test case may be reduced to the problem of finding program input on which a selected statement is executed. As a result, existing methods of automated test data generation for white-box testing may be used to generate these tests. Our experiments have shown that our approach may improve the chances of finding software errors as compared to the existing methods of regression testing. The advantage of our approach is that it is fully automated and that all generated test cases reveal an error(s).},
  isbn = {0-89791-971-8},
  venue = {Clearwater Beach, Florida, USA},
  keywords = {notion}
}

@article{korelAutomatedSoftwareTest1990,
  title = {Automated Software Test Data Generation},
  author = {Korel, B.},
  date = {1990-08},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {16},
  number = {8},
  pages = {870--879},
  issn = {0098-5589},
  doi = {10.1109/32.57624},
  abstract = {An alternative approach to test-data generation based on actual execution of the program under test, function-minimization methods and dynamic data-flow analysis is presented. Test data are developed for the program using actual values of input variables. When the program is executed, the program execution flow is monitored. If during program execution an undesirable execution flow is observed then function-minimization search algorithms are used to automatically locate the values of input variables for which the selected path is traversed. In addition, dynamic data-flow analysis is used to determine those input variables responsible for the undesirable program behavior, significantly increasing the speed of the search process. The approach to generating test data is then extended to programs with dynamic data structures and a search method based on dynamic data-flow analysis and backtracking is presented. In the approach described, values of array indexes and pointers are known at each step of program execution; this information is used to overcome difficulties of array and pointer handling},
  keywords = {Alternating Variable Method (AVM),notion,Software Testing,Symbolic Execution,Test Generation}
}

@article{korelDynamicProgramSlicing1988,
  title = {Dynamic Program Slicing},
  author = {Korel, Bogdan and Laski, Janusz},
  date = {1988},
  journaltitle = {Information Processing Letters},
  volume = {29},
  number = {3},
  pages = {155--163},
  issn = {0020-0190},
  doi = {10.1016/0020-0190(88)90054-3},
  url = {http://www.sciencedirect.com/science/article/pii/0020019088900543},
  abstract = {A dynamic program slice is an executable subset of the original program that produces the same computations on a subset of selected variables and inputs. It differs from the static slice (Weiser, 1982, 1984) in that it is entirely defined on the basis of a computation. The two main advantages are the following: Arrays and dynamic data structures can be handled more precisely and the size of slice can be significantly reduced, leading to a finer localization of the fault. The approach is being investigated as a possible extension of the debugging capabilities of STAD, a recently developed System for Testing and Debugging (Korel and Laski, 1987; Laski, 1987).},
  keywords = {control dependence,data dependence,debugging,dynamic slice,notion,Slicing,trajectory}
}

@inproceedings{korelDynamicProgramSlicing1997,
  title = {Dynamic Program Slicing in Understanding of Program Execution},
  booktitle = {Proceedings {{Fifth International Workshop}} on {{Program Comprehension}}. {{IWPC}}'97},
  author = {Korel, B. and Rilling, J.},
  date = {1997-05},
  pages = {80--89},
  issn = {1092-8138},
  doi = {10.1109/WPC.1997.601269},
  keywords = {Computer science,dynamic program slicing,dynamic slice computation,dynamic slicing tools,notion,program debugging,program execution understanding,Programming profession,reverse engineering,Software debugging,Software maintenance,Software testing,software tools,subprogram,textual form,textual representation}
}

@inproceedings{kosmidisPUBPathUpperBounding2014,
  title = {{{PUB}}: {{Path Upper-Bounding}} for {{Measurement-Based Probabilistic Timing Analysis}}},
  booktitle = {2014 26th {{Euromicro Conference}} on {{Real-Time Systems}}},
  author = {Kosmidis, Leonidas and Abella, Jaume and Wartel, Franck and Quiñones, Eduardo and Colin, Antoine and Cazorla, Francisco J.},
  date = {2014},
  pages = {276--287},
  doi = {10.1109/ECRTS.2014.34},
  keywords = {notion}
}

@article{krallGALEGeometricActive2015,
  title = {{{GALE}}: {{Geometric Active Learning}} for {{Search-Based Software Engineering}}},
  author = {Krall, J. and Menzies, T. and Davies, M.},
  date = {2015-10},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {41},
  number = {10},
  pages = {1001--1018},
  issn = {0098-5589},
  doi = {10.1109/TSE.2015.2432024},
  abstract = {Multi-objective evolutionary algorithms (MOEAs) help software engineers find novel solutions to complex problems. When automatic tools explore too many options, they are slow to use and hard to comprehend. GALE is a near-linear time MOEA that builds a piecewise approximation to the surface of best solutions along the Pareto frontier. For each piece, GALE mutates solutions towards the better end. In numerous case studies, GALE finds comparable solutions to standard methods (NSGA-II, SPEA2) using far fewer evaluations (e.g. 20 evaluations, not 1,000). GALE is recommended when a model is expensive to evaluate, or when some audience needs to browse and understand how an MOEA has made its conclusions.},
  keywords = {Active Learning,GALE,Multi-objective Optimization,notion,SBSE}
}

@article{kraskovEstimatingMutualInformation2003,
  title = {Estimating Mutual Information.},
  author = {Kraskov, Alexander and Stögbauer, Harald and Grassberger, Peter},
  date = {2003},
  journaltitle = {Physical review. E, Statistical, nonlinear, and soft matter physics},
  volume = {69 6 Pt 2},
  pages = {066138},
  keywords = {notion}
}

@article{kreindlEfficientMultilanguageDynamic2019,
  title = {Towards Efficient, Multi-Language Dynamic Taint Analysis},
  author = {Kreindl, Jacob and Bonetta, Daniele and Mössenböck, Hanspeter},
  date = {2019},
  journaltitle = {Proceedings of the 16th ACM SIGPLAN International Conference on Managed Programming Languages and Runtimes},
  keywords = {notion}
}

@article{krizhevskyLearningMultipleLayers2009,
  title = {Learning Multiple Layers of Features from Tiny Images},
  author = {Krizhevsky, Alex and Hinton, Geoffrey and others},
  date = {2009},
  publisher = {Toronto, ON, Canada},
  keywords = {notion}
}

@inproceedings{kuangAnalyzingClosenessCode2017,
  title = {Analyzing Closeness of Code Dependencies for Improving {{IR-based Traceability Recovery}}},
  booktitle = {2017 {{IEEE}} 24th {{International Conference}} on {{Software Analysis}}, {{Evolution}} and {{Reengineering}} ({{SANER}})},
  author = {Kuang, Hongyu and Nie, Jia and Hu, Hao and Rempel, Patrick and Lü, Jian and Egyed, Alexander and Mäder, Patrick},
  date = {2017},
  pages = {68--78},
  doi = {10.1109/SANER.2017.7884610},
  keywords = {notion}
}

@inproceedings{kucukImpactRareFailures2019,
  title = {The {{Impact}} of {{Rare Failures}} on {{Statistical Fault Localization}}: {{The Case}} of the {{Defects4J Suite}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Software Maintenance}} and {{Evolution}} ({{ICSME}})},
  author = {Küçük, Y. and Henderson, T. A. D. and Podgurski, A.},
  date = {2019-09},
  pages = {24--28},
  issn = {1063-6773},
  doi = {10.1109/ICSME.2019.00012},
  keywords = {Conferences,coverage profiles,Defects4j,Defects4J suite,embedded estimator,Fault Localization,Frequency estimation,Google,notion,poor effectiveness,previous SFL research,Probability,program debugging,program testing,rare failures,related quantities,SFL effectiveness,SFL metrics,simple estimators,software fault tolerance,Software maintenance,software reliability,Standards,statistical analysis,statistical estimators,statistical fault localization,Statistical Fault Localization,statistical metrics,test suite}
}

@inproceedings{kucukImprovingFaultLocalization2021,
  title = {Improving {{Fault Localization}} by {{Integrating Value}} and {{Predicate Based Causal Inference Techniques}}},
  booktitle = {International {{Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Kucuk, Yigit and Henderson, Tim and Podgurski, Andy},
  date = {2021},
  keywords = {causal inference,notion}
}

@article{kuhrmannPragmaticDesignLiterature2017,
  title = {On the Pragmatic Design of Literature Studies in Software Engineering: An Experience-Based Guideline},
  author = {Kuhrmann, Marco and Fernández, Daniel Méndez and Daneva, Maya},
  date = {2017-01-06},
  journaltitle = {Empirical Software Engineering},
  issn = {1573-7616},
  doi = {10.1007/s10664-016-9492-y},
  url = {https://doi.org/10.1007/s10664-016-9492-y},
  abstract = {Systematic literature studies have received much attention in empirical software engineering in recent years. They have become a powerful tool to collect and structure reported knowledge in a systematic and reproducible way. We distinguish systematic literature reviews to systematically analyze reported evidence in depth, and systematic mapping studies to structure a field of interest in a broader, usually quantified manner. Due to the rapidly increasing body of knowledge in software engineering, researchers who want to capture the published work in a domain often face an extensive amount of publications, which need to be screened, rated for relevance, classified, and eventually analyzed. Although there are several guidelines to conduct literature studies, they do not yet help researchers coping with the specific difficulties encountered in the practical application of these guidelines. In this article, we present an experience-based guideline to aid researchers in designing systematic literature studies with special emphasis on the data collection and selection procedures. Our guideline aims at providing a blueprint for a practical and pragmatic path through the plethora of currently available practices and deliverables capturing the dependencies among the single steps. The guideline emerges from various mapping studies and literature reviews conducted by the authors and provides recommendations for the general study design, data collection, and study selection procedures. Finally, we share our experiences and lessons learned in applying the different practices of the proposed guideline.},
  keywords = {notion,Systematic Literature Review}
}

@inproceedings{kukuckaCONFETTIAmplifyingConcolic2022,
  title = {{{CONFETTI}}: {{Amplifying Concolic Guidance}} for {{Fuzzers}}},
  booktitle = {International {{Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Kukucka, James and Pina, Luís and Ammann, Paul and Bell, Jonathan},
  date = {2022},
  keywords = {notion}
}

@inproceedings{kukuckaEmpiricalExaminationFuzzer2024,
  title = {An {{Empirical Examination}} of {{Fuzzer Mutator Performance}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Kukucka, James and Pina, Luís and Ammann, Paul and Bell, Jonathan},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {1631--1642},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3680387},
  url = {https://dl.acm.org/doi/10.1145/3650212.3680387},
  urldate = {2024-09-20},
  abstract = {Over the past decade, hundreds of fuzzers have been published in top-tier security and software engineering conferences. Fuzzers are used to automatically test programs, ideally creating high-coverage input corpora and finding bugs. Modern “greybox” fuzzers evolve a corpus of inputs by applying mutations to inputs and then executing those new inputs while collecting coverage. New inputs that are “interesting” (e.g. reveal new coverage) are saved to the corpus. Given their non-deterministic nature, the impact of each design decision on the fuzzer’s performance can be difficult to predict. Some design decisions (e.g., ” Should the fuzzer perform deterministic mutations of inputs? ”) are exposed to end-users as configuration flags, but others (e.g., ” What kinds of random mutations to apply to inputs?”) are typically baked into the fuzzer code itself. This paper describes our over 12.5-CPU-year evaluation of the set of mutation operators employed by the popular AFL++ fuzzer, including the havoc phase, splicing, and , exploring the impact of adjusting some of those unexposed configurations.   In this experience paper, we propose a methodology for determining different fuzzers’ behavioral diversity with respect to branch coverage and bug detection using rigorous statistical methods. Our key finding is that, across a range of targets, disabling certain mutation operators (some of which were previously “baked-in” to the fuzzer) resulted in inputs that cover different lines of code and reveal different bugs. A surprising result is disabling certain mutators leads to more diverse coverage and allows the fuzzer to find more bugs faster. We call for researchers to investigate seemingly simple design decisions in fuzzers more thoroughly and encourage fuzzer developers to expose more configuration parameters pertaining to these design decisions to end users.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/5PDYK4WY/Kukucka et al. - 2024 - An Empirical Examination of Fuzzer Mutator Performance.pdf}
}

@article{kulaDevelopersUpdateTheir2018,
  title = {Do Developers Update Their Library Dependencies?},
  author = {Kula, Raula Gaikovina and German, Daniel M. and Ouni, Ali and Ishio, Takashi and Inoue, Katsuro},
  date = {2018-02-01},
  journaltitle = {Empirical Software Engineering},
  volume = {23},
  number = {1},
  pages = {384--417},
  doi = {10.1007/s10664-017-9521-5},
  url = {https://doi.org/10.1007/s10664-017-9521-5},
  abstract = {Third-party library reuse has become common practice in contemporary software development, as it includes several benefits for developers. Library dependencies are constantly evolving, with newly added features and patches that fix bugs in older versions. To take full advantage of third-party reuse, developers should always keep up to date with the latest versions of their library dependencies. In this paper, we investigate the extent of which developers update their library dependencies. Specifically, we conducted an empirical study on library migration that covers over 4,600 GitHub software projects and 2,700 library dependencies. Results show that although many of these systems rely heavily on dependencies, 81.5\% of the studied systems still keep their outdated dependencies. In the case of updating a vulnerable dependency, the study reveals that affected developers are not likely to respond to a security advisory. Surveying these developers, we find that 69\% of the interviewees claimed to be unaware of their vulnerable dependencies. Moreover, developers are not likely to prioritize a library update, as it is perceived to be extra workload and responsibility. This study concludes that even though third-party reuse is common practice, updating a dependency is not as common for many developers.},
  isbn = {1573-7616},
  keywords = {notion}
}

@unpublished{kulkarniLearningHighlyRecursive2021,
  title = {Learning {{Highly Recursive Input Grammars}}},
  author = {Kulkarni, Neil and Lemieux, Caroline and Sen, Koushik},
  date = {2021},
  eprint = {2108.13340},
  eprinttype = {arXiv},
  keywords = {notion}
}

@article{kusumotoExperimentalEvaluationProgram2002,
  title = {Experimental {{Evaluation}} of {{Program Slicing}} for {{Fault Localization}}},
  author = {Kusumoto, Shinji and Nishimatsu, Akira and Nishie, Keisuke and Inoue, Katsuro},
  date = {2002-03},
  journaltitle = {Empirical Softw. Engg.},
  volume = {7},
  number = {1},
  pages = {49--76},
  publisher = {Kluwer Academic Publishers},
  location = {USA},
  issn = {1382-3256},
  doi = {10.1023/A:1014823126938},
  url = {https://doi.org/10.1023/A:1014823126938},
  keywords = {empirical evaluation,fault localization,measurement,notion,Program slice,tool}
}

@article{kwonLDXCausalityInference2016,
  title = {{{LDX}}: {{Causality Inference}} by {{Lightweight Dual Execution}}},
  author = {Kwon, Yonghwi and Kim, Dohyeong and Sumner, William Nick and Kim, Kyungtae and Saltaformaggio, Brendan and Zhang, Xiangyu and Xu, Dongyan},
  date = {2016-03},
  journaltitle = {SIGARCH Comput. Archit. News},
  volume = {44},
  number = {2},
  pages = {503--515},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {0163-5964},
  doi = {10.1145/2980024.2872395},
  url = {https://doi.org/10.1145/2980024.2872395},
  abstract = {Causality inference, such as dynamic taint anslysis, has many applications (e.g., information leak detection). It determines whether an event e is causally dependent on a preceding event c during execution. We develop a new causality inference engine LDX. Given an execution, it spawns a slave execution, in which it mutates c and observes whether any change is induced at e. To preclude non-determinism, LDX couples the executions by sharing syscall outcomes. To handle path differences induced by the perturbation, we develop a novel on-the-fly execution alignment scheme that maintains a counter to reflect the progress of execution. The scheme relies on program analysis and compiler transformation. LDX can effectively detect information leak and security attacks with an average overhead of 6.08\% while running the master and the slave concurrently on separate CPUs, much lower than existing systems that require instruction level monitoring. Furthermore, it has much better accuracy in causality inference.},
  keywords = {causality inference,dual execution,dynamic analysis,notion}
}

@inproceedings{kwonLDXCausalityInference2016a,
  title = {{{LDX}}: {{Causality Inference}} by {{Lightweight Dual Execution}}},
  booktitle = {Proceedings of the {{Twenty-First International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  author = {Kwon, Yonghwi and Kim, Dohyeong and Sumner, William Nick and Kim, Kyungtae and Saltaformaggio, Brendan and Zhang, Xiangyu and Xu, Dongyan},
  date = {2016},
  series = {{{ASPLOS}} '16},
  pages = {503--515},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2872362.2872395},
  url = {https://doi.org/10.1145/2872362.2872395},
  abstract = {Causality inference, such as dynamic taint anslysis, has many applications (e.g., information leak detection). It determines whether an event e is causally dependent on a preceding event c during execution. We develop a new causality inference engine LDX. Given an execution, it spawns a slave execution, in which it mutates c and observes whether any change is induced at e. To preclude non-determinism, LDX couples the executions by sharing syscall outcomes. To handle path differences induced by the perturbation, we develop a novel on-the-fly execution alignment scheme that maintains a counter to reflect the progress of execution. The scheme relies on program analysis and compiler transformation. LDX can effectively detect information leak and security attacks with an average overhead of 6.08\% while running the master and the slave concurrently on separate CPUs, much lower than existing systems that require instruction level monitoring. Furthermore, it has much better accuracy in causality inference.},
  isbn = {978-1-4503-4091-5},
  venue = {Atlanta, Georgia, USA},
  keywords = {causality inference,dual execution,dynamic analysis,notion}
}

@misc{lacerdaDiscoveringCyclicCausal2012,
  title = {Discovering {{Cyclic Causal Models}} by {{Independent Components Analysis}}},
  author = {Lacerda, Gustavo and Spirtes, Peter L. and Ramsey, Joseph and Hoyer, Patrik O.},
  date = {2012},
  keywords = {notion}
}

@inproceedings{ladkinPracticalIssuesStatistically2015,
  title = {Some {{Practical Issues}} in {{Statistically Evaluating Critical Software}}},
  author = {Ladkin, Peter B.},
  date = {2015},
  keywords = {notion}
}

@inproceedings{ladkinPracticalStatisticalEvaluation2015,
  title = {Practical {{Statistical Evaluation}} of {{Critical Software}}},
  author = {Ladkin, Peter B.},
  date = {2015},
  keywords = {notion}
}

@article{lamIDFlakiesFrameworkDetecting2019,
  title = {{{iDFlakies}}: {{A Framework}} for {{Detecting}} and {{Partially Classifying Flaky Tests}}},
  author = {Lam, Wing and Oei, Reed and Shi, August and Marinov, Darko and Xie, Tao},
  date = {2019},
  journaltitle = {2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST)},
  pages = {312--322},
  keywords = {notion}
}

@article{landiUndecidabilityStaticAnalysis1992,
  title = {Undecidability of {{Static Analysis}}},
  author = {Landi, William},
  date = {1992-12},
  journaltitle = {ACM Lett. Program. Lang. Syst.},
  volume = {1},
  number = {4},
  pages = {323--337},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {1057-4514},
  doi = {10.1145/161494.161501},
  url = {https://doi.org/10.1145/161494.161501},
  abstract = {Static analysis of programs is indispensable to any software tool, environment, or system that requires compile-time information about the semantics of programs. With the emergence of languages like C and LISP, static analysis of programs with dynamic storage and recursive data structures has become a field of active research. Such analysis is difficult, and the static-analysis community has recognized the need for simplifying assumptions and approximate solutions. However, even under the common simplifying assumptions, such analyses are harder than previously recognized. Two fundamental static-analysis problems are may alias and must alias. The former is not recursive (is undecidable), and the latter is not recursively enumerable (is uncomputable), even when all paths are executable in the program being analyzed for languages with if statements, loops, dynamic storage, and recursive data structures.},
  keywords = {abstract interpretation,alias analysis,data flow analysis,halting problem,notion,static analysis}
}

@misc{landsbergDoricFoundationsStatistical2018,
  title = {Doric: {{Foundations}} for {{Statistical Fault Localisation}}},
  author = {Landsberg, David and Barr, Earl},
  date = {2018},
  keywords = {notion}
}

@article{langdonEfficientMultiobjectiveHigher2010,
  title = {Efficient Multi-Objective Higher Order Mutation Testing with Genetic Programming},
  author = {Langdon, William B. and Harman, Mark and Jia, Yue},
  date = {2010},
  journaltitle = {Journal of Systems and Software},
  volume = {83},
  number = {12},
  pages = {2416--2430},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2010.07.027},
  url = {http://www.sciencedirect.com/science/article/pii/S0164121210001883},
  abstract = {It is said 90\% of faults that survive manufacturer's testing procedures are complex. That is, the corresponding bug fix contains multiple changes. Higher order mutation testing is used to study defect interactions and their impact on software testing for fault finding. We adopt a multi-objective Pareto optimal approach using Monte Carlo sampling, genetic algorithms and genetic programming to search for higher order mutants which are both hard-to-kill and realistic. The space of complex faults (higher order mutants) is much larger than that of traditional first order mutations which correspond to simple faults, nevertheless search based approaches make this scalable. The problems of non-determinism and efficiency are overcome. Easy to detect faults may become harder to detect when they interact and impossible to detect single faults may be brought to light when code contains two such faults. We use strong typing and BNF grammars in search based mutation testing to find examples of both in ancient heavily optimised every day C code.},
  keywords = {Genetic algorithm,Genetic programming,Grammar based GP,Gzip,Higher order mutation,Monte Carlo,Mutation testing,Non-determinism,notion,NSGA-II,Pareto optimality,SBSE,Schedule,Strongly typed GP,tcas,Triangle}
}

@article{langdonGeneticImprovementICSE2020a,
  title = {Genetic {{Improvement}} @ {{ICSE}} 2020},
  author = {Langdon, William B. and Weimer, Westley and Petke, Justyna and Fredericks, Erik and Lee, Seongmin and Winter, Emily and Basios, Michail and Cohen, Myra B. and Blot, Aymeric and Wagner, Markus and Bruce, Bobby R. and Yoo, Shin and Gerasimou, Simos and Krauss, Oliver and Huang, Yu and Gerten, Michael},
  date = {2020-10-12},
  journaltitle = {SIGSOFT Softw. Eng. Notes},
  journal = {SIGSOFT Softw. Eng. Notes},
  year = {2020},
  volume = {45},
  number = {4},
  pages = {24--30},
  issn = {0163-5948},
  doi = {10.1145/3417564.3417575},
  url = {https://dl.acm.org/doi/10.1145/3417564.3417575},
  urldate = {2024-11-18},
  abstract = {Following Prof. Mark Harman of Facebook's keynote and formal presentations (which are recorded in the proceed- ings) there was a wide ranging discussion at the eighth inter- national Genetic Improvement workshop, GI-2020 @ ICSE (held as part of the International Conference on Software En- gineering on Friday 3rd July 2020). Topics included industry take up, human factors, explainabiloity (explainability, jus- tifyability, exploitability) and GI benchmarks. We also con- trast various recent online approaches (e.g. SBST 2020) to holding virtual computer science conferences and workshops via the WWW on the Internet without face to face interac- tion. Finally we speculate on how the Coronavirus Covid-19 Pandemic will a ect research next year and into the future.},
  file = {/Users/bohrok/Zotero/storage/FVIPX9SK/Langdon et al. - 2020 - Genetic Improvement @ ICSE 2020.pdf}
}

@inproceedings{langdonVisualisingSearchLandscape2017,
  title = {Visualising the {{Search Landscape}} of the {{Triangle Program}}},
  booktitle = {Genetic {{Programming}}},
  author = {Langdon, William B. and Veerapen, Nadarajen and Ochoa, Gabriela},
  editor = {McDermott, James and Castelli, Mauro and Sekanina, Lukas and Haasdijk, Evert and García-Sánchez, Pablo},
  date = {2017},
  pages = {96--113},
  publisher = {Springer International Publishing},
  location = {Cham},
  abstract = {High order mutation analysis of a software engineering benchmark, including schema and local optima networks, suggests program improvements may not be as hard to find as is often assumed. (1) Bit-wise genetic building blocks are not deceptive and can lead to all global optima. (2) There are many neutral networks, plateaux and local optima, nevertheless in most cases near the human written C source code there are hill climbing routes including neutral moves to solutions.},
  isbn = {978-3-319-55696-3},
  keywords = {notion}
}

@article{langvilleUpdatingMarkovChains2005,
  title = {Updating {{Markov Chains}} with an {{Eye}} on {{Google}}'s {{PageRank}}},
  author = {Langville, Amy Nicole and Meyer, Carl Dean},
  date = {2005},
  journaltitle = {SIAM J. Matrix Anal. Appl.},
  volume = {27},
  pages = {968--987},
  keywords = {notion}
}

@article{lannersFeatureImportanceDistance2023,
  title = {From {{Feature Importance}} to {{Distance Metric}}: {{An Almost Exact Matching Approach}} for {{Causal Inference}}},
  author = {Lanners, Quinn M. and Parikh, Harsh and Volfovsky, Alexander and Rudin, Cynthia and Page, David},
  date = {2023},
  journaltitle = {ArXiv},
  volume = {abs/2302.11715},
  keywords = {notion}
}

@article{laskiErrorMaskingComputer1995,
  title = {Error Masking in Computer Programs},
  author = {Laski, Janusz and Szermer, Wojciech and Luczycki, Piotr},
  date = {1995},
  journaltitle = {Software Testing, Verification and Reliability},
  volume = {5},
  number = {2},
  pages = {81--105},
  publisher = {Wiley Online Library},
  keywords = {notion}
}

@inproceedings{lawrieNormalizingSourceCode2010,
  title = {Normalizing {{Source Code Vocabulary}}},
  booktitle = {2010 17th {{Working Conference}} on {{Reverse Engineering}}},
  author = {Lawrie, D. and Binkley, D. and Morrell, C.},
  date = {2010-10},
  pages = {3--12},
  issn = {1095-1350},
  doi = {10.1109/WCRE.2010.10},
  abstract = {Information Retrieval (IR) based tools complement traditional static and dynamic analysis tools by exploiting the natural language found within a program's text. Tools incorporating IR have tackled problems, such as feature location, that previously required considerable human effort. However, to reap the full benefit of IR-based techniques, the language used across all software artifacts (e.g., requirement and design documents, test plans, as well as the source code) must be consistent. Vocabulary normalization aligns the vocabulary found in source code with that found in other software artifacts. Normalization both splits an identifier into its constituent parts and expands each part into a full dictionary word to match vocabulary in other artifacts. An algorithm for normalization is presented. Its current implementation incorporates a greatly improved splitter that exploits a collection of resources including several dictionaries, frequency distributions derived from the corpus of programs, and co-occurrence data. Empirical study of this new splitter, GenTest, on almost 8000 identifiers finds that it correctly splits 82\%, outperforming the current state-of-the-art. A preliminary experiment with the normalization algorithm finds it improving the FLAT ̂ 3 feature locator's scores of relevant code from 0.60 to 0.95 on a scale from 0 to 1.},
  keywords = {Natural Language,notion,Vocabulary Normalization}
}

@article{leCompilerValidationEquivalence2014,
  title = {Compiler {{Validation}} via {{Equivalence}} modulo {{Inputs}}},
  author = {Le, Vu and Afshari, Mehrdad and Su, Zhendong},
  date = {2014-06},
  journaltitle = {SIGPLAN Not.},
  volume = {49},
  number = {6},
  pages = {216--226},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {0362-1340},
  doi = {10.1145/2666356.2594334},
  url = {https://doi.org/10.1145/2666356.2594334},
  abstract = {We introduce equivalence modulo inputs (EMI), a simple, widely applicable methodology for validating optimizing compilers. Our key insight is to exploit the close interplay between (1) dynamically executing a program on some test inputs and (2) statically compiling the program to work on all possible inputs. Indeed, the test inputs induce a natural collection of the original program's EMI variants, which can help differentially test any compiler and specifically target the difficult-to-find miscompilations.To create a practical implementation of EMI for validating C compilers, we profile a program's test executions and stochastically prune its unexecuted code. Our extensive testing in eleven months has led to 147 confirmed, unique bug reports for GCC and LLVM alone. The majority of those bugs are miscompilations, and more than 100 have already been fixed.Beyond testing compilers, EMI can be adapted to validate program transformation and analysis systems in general. This work opens up this exciting, new direction.},
  keywords = {automated testing,compiler testing,equivalent program variants,miscompilation,notion}
}

@inproceedings{leCompilerValidationEquivalence2014a,
  title = {Compiler {{Validation}} via {{Equivalence}} modulo {{Inputs}}},
  booktitle = {Proceedings of the 35th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Le, Vu and Afshari, Mehrdad and Su, Zhendong},
  date = {2014},
  series = {{{PLDI}} '14},
  pages = {216--226},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2594291.2594334},
  url = {https://doi.org/10.1145/2594291.2594334},
  abstract = {We introduce equivalence modulo inputs (EMI), a simple, widely applicable methodology for validating optimizing compilers. Our key insight is to exploit the close interplay between (1) dynamically executing a program on some test inputs and (2) statically compiling the program to work on all possible inputs. Indeed, the test inputs induce a natural collection of the original program's EMI variants, which can help differentially test any compiler and specifically target the difficult-to-find miscompilations.To create a practical implementation of EMI for validating C compilers, we profile a program's test executions and stochastically prune its unexecuted code. Our extensive testing in eleven months has led to 147 confirmed, unique bug reports for GCC and LLVM alone. The majority of those bugs are miscompilations, and more than 100 have already been fixed.Beyond testing compilers, EMI can be adapted to validate program transformation and analysis systems in general. This work opens up this exciting, new direction.},
  isbn = {978-1-4503-2784-8},
  venue = {Edinburgh, United Kingdom},
  keywords = {automated testing,compiler testing,equivalent program variants,miscompilation,notion}
}

@article{lecuyerRareEventsSplitting2007,
  title = {Rare {{Events}}, {{Splitting}}, and {{Quasi-Monte Carlo}}},
  author = {L'Ecuyer, Pierre and Demers, Valérie and Tuffin, Bruno},
  date = {2007-04},
  journaltitle = {ACM Trans. Model. Comput. Simul.},
  volume = {17},
  number = {2},
  pages = {9--es},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {1049-3301},
  doi = {10.1145/1225275.1225280},
  url = {https://doi.org/10.1145/1225275.1225280},
  abstract = {In the context of rare-event simulation, splitting and importance sampling (IS) are the primary approaches to make important rare events happen more frequently in a simulation and yet recover an unbiased estimator of the target performance measure, with much smaller variance than a straightforward Monte Carlo (MC) estimator. Randomized quasi-Monte Carlo (RQMC) is another class of methods for reducing the noise of simulation estimators, by sampling more evenly than with standard MC. It typically works well for simulations that depend mostly on very few random numbers. In splitting and IS, on the other hand, we often simulate Markov chains whose sample paths are a function of a long sequence of independent random numbers generated during the simulation. In this article, we show that RQMC can be used jointly with splitting and/or IS to construct better estimators than those obtained by either of these methods alone. We do that in a setting where the goal is to estimate the probability of reaching B before reaching (or returning to) A when starting A from a distinguished state not in B, where A and B are two disjoint subsets of the state space, and B is very rarely reached. This problem has several practical applications. The article is in fact a two-in-one: the first part provides a guided tour of splitting techniques, introducing along the way some improvements in the implementation of multilevel splitting. At the end of the article, we also give examples of situations where splitting is not effective. For these examples, we compare different ways of applying IS and combining it with RQMC.},
  keywords = {highly-reliable Markovian systems,importance sampling,Markov chain,notion,Quasi-Monte Carlo,RESTART,splitting,variance reduction}
}

@inproceedings{lee2025accounting,
  title = {Accounting for Missing Events in Statistical Information Leakage Analysis},
  booktitle = {2025 {{IEEE}}/{{ACM}} 47th International Conference on Software Engineering ({{ICSE}})},
  author = {Lee, Seongmin and Minocha, Shreyas and Böhme, Marcel},
  date = {2025},
  year = {2025},
}

@report{leeCausalProgramDependence2021,
  title = {Causal {{Program Dependence Analysis}} and {{Causal Fault Localization}}},
  author = {Lee, Seongmin and Binkley, Dave and Feldt, Robert and Gold, Nicolas and Yoo, Shin},
  date = {2021-01},
  number = {CS-TR-2021-423},
  institution = {{Korea Advanced Institute of Science and Technology}},
  location = {291 Daehak-ro, Yuseong-gu, Daejeon, Korea 34141},
  keywords = {notion}
}

@article{leeCausalProgramDependence2025,
  title = {Causal Program Dependence Analysis},
  author = {Lee, Seongmin and Binkley, Dave and Feldt, Robert and Gold, Nicolas and Yoo, Shin},
  date = {2025-02-01},
  journaltitle = {Science of Computer Programming},
  shortjournal = {Science of Computer Programming},
  volume = {240},
  pages = {103208},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2024.103208},
  url = {https://www.sciencedirect.com/science/article/pii/S016764232400131X},
  urldate = {2024-11-18},
  abstract = {Discovering how program components affect one another plays a fundamental role in aiding engineers comprehend and maintain a software system. Despite the fact that the degree to which one program component depends upon another can vary in strength, traditional dependence analysis typically ignores such nuance. To account for this nuance in dependence-based analysis, we propose Causal Program Dependence Analysis (CPDA), a framework based on causal inference that captures the degree (or strength) of the dependence between program elements. For a given program, CPDA intervenes in the program execution to observe changes in value at selected points in the source code. It observes the association between program elements by constructing and executing modified versions of a program (requiring only light-weight parsing rather than sophisticated static analysis). CPDA applies causal inference to the observed changes to identify and estimate the strength of the dependence relations between program elements. We explore the advantages of CPDA's quantified dependence by presenting results for several applications. Our further qualitative evaluation demonstrates 1) that observing different levels of dependence facilitates grouping various functional aspects found in a program and 2) how focusing on the relative strength of the dependences for a particular program element provides a detailed context for that element. Furthermore, a case study that applies CPDA to debugging illustrates how it can improve engineer productivity.},
  keywords = {Causal inference,CPDA,Dependency analysis,Observation-based analysis},
  file = {/Users/bohrok/Zotero/storage/U6KQEUIR/Lee et al. - 2025 - Causal program dependence analysis.pdf;/Users/bohrok/Zotero/storage/NTXLF6SC/S016764232400131X.html}
}

@article{leeCausalProgramDependence2025a,
  title = {Causal Program Dependence Analysis},
  author = {Lee, Seongmin and Binkley, Dave and Feldt, Robert and Gold, Nicolas and Yoo, Shin},
  date = {2025-02-01},
  journaltitle = {Science of Computer Programming},
  shortjournal = {Science of Computer Programming},
  journal = {Science of Computer Programming},
  year = {2025},
  volume = {240},
  pages = {103208},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2024.103208},
  url = {https://www.sciencedirect.com/science/article/pii/S016764232400131X},
  urldate = {2024-11-18},
  abstract = {Discovering how program components affect one another plays a fundamental role in aiding engineers comprehend and maintain a software system. Despite the fact that the degree to which one program component depends upon another can vary in strength, traditional dependence analysis typically ignores such nuance. To account for this nuance in dependence-based analysis, we propose Causal Program Dependence Analysis (CPDA), a framework based on causal inference that captures the degree (or strength) of the dependence between program elements. For a given program, CPDA intervenes in the program execution to observe changes in value at selected points in the source code. It observes the association between program elements by constructing and executing modified versions of a program (requiring only light-weight parsing rather than sophisticated static analysis). CPDA applies causal inference to the observed changes to identify and estimate the strength of the dependence relations between program elements. We explore the advantages of CPDA's quantified dependence by presenting results for several applications. Our further qualitative evaluation demonstrates 1) that observing different levels of dependence facilitates grouping various functional aspects found in a program and 2) how focusing on the relative strength of the dependences for a particular program element provides a detailed context for that element. Furthermore, a case study that applies CPDA to debugging illustrates how it can improve engineer productivity.},
  keywords = {Causal inference,CPDA,Dependency analysis,Observation-based analysis},
  file = {/Users/bohrok/Zotero/storage/97WAJF8N/Lee et al. - 2025 - Causal program dependence analysis.pdf;/Users/bohrok/Zotero/storage/639LMRXI/S016764232400131X.html}
}

@inproceedings{leeClassifyingFalsePositive2019,
  title = {Classifying {{False Positive Static Checker Alarms}} in {{Continuous Integration Using Convolutional Neural Networks}}},
  booktitle = {2019 12th {{IEEE Conference}} on {{Software Testing}}, {{Validation}} and {{Verification}} ({{ICST}})},
  author = {Lee, Seongmin and Hong, Shin and Yi, Jungbae and Kim, Taeksu and Kim, Chul-Joo and Yoo, Shin},
  date = {2019-04},
  year = {2019},
  pages = {391--401},
  issn = {2159-4848},
  doi = {10.1109/ICST.2019.00048},
  url = {https://ieeexplore.ieee.org/abstract/document/8730202},
  urldate = {2024-11-18},
  abstract = {Static code analysis in Continuous Integration (CI) environment can significantly improve the quality of a software system because it enables early detection of defects without any test executions or user interactions. However, being a conservative over-approximation of system behaviours, static analysis also produces a large number of false positive alarms, identification of which takes up valuable developer time. We present an automated classifier based on Convolutional Neural Networks (CNNs). We hypothesise that many false positive alarms can be classified by identifying specific lexical patterns in the parts of the code that raised the alarm: human engineers adopt a similar tactic. We train a CNN based classifier to learn and detect these lexical patterns, using a total of about 10K historical static analysis alarms generated by six static analysis checkers for over 27 million LOC, and their labels assigned by actual developers. The results of our empirical evaluation suggest that our classifier can be highly effective for identifying false positive alarms, with the average precision across all six checkers of 79.72\%.},
  eventtitle = {2019 12th {{IEEE Conference}} on {{Software Testing}}, {{Validation}} and {{Verification}} ({{ICST}})},
  keywords = {Classification,Databases,False alarms,Feature extraction,Machine learning,Pipelines,Quality assurance,Software,Static analysis},
  file = {/Users/bohrok/Zotero/storage/3YR95PFF/Lee et al. - 2019 - Classifying False Positive Static Checker Alarms in Continuous Integration Using Convolutional Neura.pdf;/Users/bohrok/Zotero/storage/RYFKSZAG/8730202.html}
}

@article{leeEvaluatingLexicalApproximation2020a,
  title = {Evaluating Lexical Approximation of Program Dependence},
  author = {Lee, Seongmin and Binkley, David and Gold, Nicolas and Islam, Syed and Krinke, Jens and Yoo, Shin},
  date = {2020-02-01},
  journaltitle = {Journal of Systems and Software},
  shortjournal = {Journal of Systems and Software},
  journal = {Journal of Systems and Software},
  year = {2020},
  volume = {160},
  pages = {110459},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2019.110459},
  url = {https://www.sciencedirect.com/science/article/pii/S016412121930233X},
  urldate = {2024-11-18},
  abstract = {Complex dependence analysis typically provides an underpinning approximation of true program dependence. We investigate the effectiveness of using lexical information to approximate such dependence, introducing two new deletion operators to Observation-Based Slicing (ORBS). ORBS provides direct observation of program dependence, computing a slice using iterative, speculative deletion of program parts. Deletions become permanent if they do not affect the slicing criterion. The original ORBS uses a bounded deletion window operator that attempts to delete consecutive lines together. Our new deletion operators attempt to delete multiple, non-contiguous lines that are lexically similar to each other. We evaluate the lexical dependence approximation by exploring the trade-off between the precision and the speed of dependence analysis performed with new deletion operators. The deletion operators are evaluated independently, as well as collectively via a novel generalization of ORBS that exploits multiple deletion operators: Multi-operator Observation-Based Slicing (MOBS). An empirical evaluation using three Java projects, six C projects, and one multi-lingual project written in Python and C finds that the lexical information provides a useful approximation to the underlying dependence. On average, MOBS can delete 69\% of lines deleted by the original ORBS, while taking only 36\% of the wall clock time required by ORBS.},
  keywords = {Lexical analysis,ORBS,Program slicing},
  file = {/Users/bohrok/Zotero/storage/XP6DAKSV/Lee et al. - 2020 - Evaluating lexical approximation of program dependence.pdf;/Users/bohrok/Zotero/storage/3QLID8MQ/S016412121930233X.html}
}

@online{leeHowMuchUnseen2024a,
  title = {How {{Much}} Is {{Unseen Depends Chiefly}} on {{Information About}} the {{Seen}}},
  author = {Lee, Seongmin and Böhme, Marcel},
  date = {2024-02-08},
  eprint = {2402.05835},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2402.05835},
  url = {http://arxiv.org/abs/2402.05835},
  urldate = {2024-11-18},
  abstract = {It might seem counter-intuitive at first: We find that, in expectation, the proportion of data points in an unknown population-that belong to classes that do not appear in the training data-is almost entirely determined by the number \$f\_k\$ of classes that do appear in the training data the same number of times. While in theory we show that the difference of the induced estimator decays exponentially in the size of the sample, in practice the high variance prevents us from using it directly for an estimator of the sample coverage. However, our precise characterization of the dependency between \$f\_k\$'s induces a large search space of different representations of the expected value, which can be deterministically instantiated as estimators. Hence, we turn to optimization and develop a genetic algorithm that, given only the sample, searches for an estimator with minimal mean-squared error (MSE). In our experiments, our genetic algorithm discovers estimators that have a substantially smaller MSE than the state-of-the-art Good-Turing estimator. This holds for over 96\% of runs when there are at least as many samples as classes. Our estimators' MSE is roughly 80\% of the Good-Turing estimator's.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/bohrok/Zotero/storage/K6PMKWFM/Lee and Böhme - 2024 - How Much is Unseen Depends Chiefly on Information About the Seen.pdf;/Users/bohrok/Zotero/storage/KQZGQ8B2/2402.html}
}

@inproceedings{leeHyperheuristicObservationBased2017a,
  title = {Hyperheuristic {{Observation Based Slicing}} of {{Guava}}},
  booktitle = {Search {{Based Software Engineering}}},
  author = {Lee, Seongmin and Yoo, Shin},
  editor = {Menzies, Tim and Petke, Justyna},
  date = {2017},
  year = {2017},
  pages = {175--180},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-66299-2_16},
  abstract = {Observation Based Slicing is a program slicing technique that depends purely on the observation of dynamic program behaviours. It iteratively applies a deletion operator to the source code, and accepts the deletion (i.e. slices the program) if the program is observed to behave in the same was as the original with respect to the slicing criterion. While the original observation based slicing only used a single deletion operator based on deletion window, the catalogue of applicable deletion operators grew recently with the addition of deletion operators based on lexical similarity. We apply a hyperheuristic approach to the problem of selecting the best deletion operator to each program line. Empirical evaluation using four slicing criteria from Guava shows that the Hyperheuristic Observation Based Slicing (HOBBES) can significantly improve the effeciency of observation based slicing.},
  isbn = {978-3-319-66299-2},
  langid = {english},
  keywords = {Delete Window,Deletion Operator,Hyperheuristic Approach,Lexical Similarity,Slicing Criterion},
  file = {/Users/bohrok/Zotero/storage/BRXBQM4D/Lee and Yoo - 2017 - Hyperheuristic Observation Based Slicing of Guava.pdf}
}

@inproceedings{leeMOADModelingObservationBased2019,
  title = {{{MOAD}}: {{Modeling Observation-Based Approximate Dependency}}},
  shorttitle = {{{MOAD}}},
  booktitle = {2019 19th {{International Working Conference}} on {{Source Code Analysis}} and {{Manipulation}} ({{SCAM}})},
  author = {Lee, Seongmin and Binkley, David and Feldt, Robert and Gold, Nicolas and Yoo, Shin},
  date = {2019-09},
  year = {2019},
  pages = {12--22},
  issn = {2470-6892},
  doi = {10.1109/SCAM.2019.00011},
  url = {https://ieeexplore.ieee.org/abstract/document/8930847},
  urldate = {2024-11-18},
  abstract = {While dependency analysis is foundational to many applications of program analysis, the static nature of many existing techniques presents challenges such as limited scalability and inability to cope with multi-lingual systems. We present a novel dependency analysis technique that aims to approximate program dependency from a relatively small number of perturbed executions. Our technique, called MOAD (Modeling Observation-based Approximate Dependency), reformulates program dependency as the likelihood that one program element is dependent on another, instead of a more classical Boolean relationship. MOAD generates a set of program variants by deleting parts of the source code, and executes them while observing the impacts of the deletions on various program points. From these observations, MOAD infers a model of program dependency that captures the dependency relationship between the modification and observation points. While MOAD is a purely dynamic dependency analysis technique similar to Observation Based Slicing (ORBS), it does not require iterative deletions. Rather, MOAD makes a much smaller number of multiple, independent observations in parallel and infers dependency relationships for multiple program elements simultaneously, significantly reducing the cost of dynamic dependency analysis. We evaluate MOAD by instantiating program slices from the obtained probabilistic dependency model. Compared to ORBS, MOAD's model construction requires only 18.7\% of the observations used by ORBS, while its slices are only 16\% larger than the corresponding ORBS slice, on average.},
  eventtitle = {2019 19th {{International Working Conference}} on {{Source Code Analysis}} and {{Manipulation}} ({{SCAM}})},
  keywords = {Analytical models,Dependency analysis,Inference algorithms,Model learning,Performance analysis,Probabilistic logic,Program slicing,Semantics,Tools,Trajectory},
  file = {/Users/bohrok/Zotero/storage/VPZKNYZ9/Lee et al. - 2019 - MOAD Modeling Observation-Based Approximate Dependency.pdf;/Users/bohrok/Zotero/storage/UIJF7J8A/8930847.html}
}

@inproceedings{leeMOBSMultioperatorObservationbased2018a,
  title = {{{MOBS}}: Multi-Operator Observation-Based Slicing Using Lexical Approximation of Program Dependence},
  shorttitle = {{{MOBS}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Software Engineering}}: {{Companion Proceeedings}}},
  author = {Lee, Seongmin and Binkley, David and Gold, Nicolas and Islam, Syed and Krinke, Jens and Yoo, Shin},
  date = {2018-05-27},
  series = {{{ICSE}} '18},
  pages = {302--303},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3183440.3194981},
  url = {https://dl.acm.org/doi/10.1145/3183440.3194981},
  urldate = {2024-11-18},
  abstract = {Observation-Based Slicing (ORBS) is a recently introduced program slicing technique based on direct observation of program semantics. Previous ORBS implementations slice a program by iteratively deleting adjacent lines of code. This paper introduces two new deletion operators based on lexical similarity. Furthermore, it presents a generalization of O RBS that can exploit multiple deletion operators: Multi-operator Observation-Based Slicing (MOBS). An empirical evaluation of MOBS using three real world Java projects finds that the use of lexical information, improves the efficiency of ORBS: MOBS can delete up to 87\% of lines while taking only about 33\% of the execution time with respect to the original ORBS.},
  isbn = {978-1-4503-5663-3},
  file = {/Users/bohrok/Zotero/storage/YMINTHJP/Lee et al. - 2018 - MOBS multi-operator observation-based slicing using lexical approximation of program dependence.pdf}
}

@article{leeObservationbasedApproximateDependency2021a,
  title = {Observation-Based Approximate Dependency Modeling and Its Use for Program Slicing},
  author = {Lee, Seongmin and Binkley, David and Feldt, Robert and Gold, Nicolas and Yoo, Shin},
  date = {2021-09-01},
  journaltitle = {Journal of Systems and Software},
  shortjournal = {Journal of Systems and Software},
  journal = {Journal of Systems and Software},
  year = {2021},
  volume = {179},
  pages = {110988},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2021.110988},
  url = {https://www.sciencedirect.com/science/article/pii/S0164121221000856},
  urldate = {2024-11-18},
  abstract = {While dependency analysis is foundational to much program analysis, many techniques have limited scalability and handle only monolingual systems. We present a novel dependency analysis technique that aims to approximate program dependency from a relatively small number of perturbed executions. Our technique, MOAD (Modeling Observation-based Approximate Dependency), reformulates program dependency as the likelihood that one program element is dependent on another (instead of a Boolean relationship). MOAD generates program variants by deleting parts of the source code and executing them while observing the impact. MOAD thus infers a model of program dependency that captures the relationship between the modification and observation points. We evaluate MOAD using program slices obtained from the resulting probabilistic dependency models. Compared to the existing observation-based backward slicing technique, ORBS, MOAD requires only 18.6\% of the observations, while the resulting slices are only 12\% larger on average. Furthermore, we introduce the notion of the observation-based forward slices. Unlike ORBS, which inherently computes backward slices, MOAD’s model’s dependences can be traversed in either direction allowing us to easily compute forward slices. In comparison to the static forward slice, MOAD only misses deleting 0–6 lines (median 0), while excessively deleting 0–37 lines (median 8) from the slice.},
  keywords = {Dependency analysis,MOAD,Model learning,Program slicing},
  file = {/Users/bohrok/Zotero/storage/RTQPYZPB/Lee et al. - 2021 - Observation-based approximate dependency modeling and its use for program slicing.pdf;/Users/bohrok/Zotero/storage/ZR3ZXK2T/S0164121221000856.html}
}

@inproceedings{leeScalableApproximateProgram2020a,
  title = {Scalable and Approximate Program Dependence Analysis},
  booktitle = {Proceedings of the {{ACM}}/{{IEEE}} 42nd {{International Conference}} on {{Software Engineering}}: {{Companion Proceedings}}},
  author = {Lee, Seongmin},
  date = {2020-10-01},
  year = {2020},
  series = {{{ICSE}} '20},
  pages = {162--165},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3377812.3381392},
  url = {https://dl.acm.org/doi/10.1145/3377812.3381392},
  urldate = {2024-11-18},
  abstract = {Program dependence is a fundamental concept to many software engineering tasks, yet the traditional dependence analysis struggles to cope with common modern development practices such as multi-lingual implementations and use of third-party libraries. While Observation-based Slicing (ORBS) solves these issues and produces an accurate slice, it has a scalability problem due to the need to build and execute the target program multiple times. We would like to propose a radical change of perspective: a useful dependence analysis needs to be scalable even if it approximates the dependency. Our goal is a scalable approximate program dependence analysis via estimating the likelihood of dependence. We claim that 1) using external information such as lexical analysis or a development history, 2) learning dependence model from partial observations, and 3) merging static, and observation-based approach would assist the proposition. We expect that our technique would introduce a new perspective of program dependence analysis into the likelihood of dependence. It would also broaden the capability of the dependence analysis towards large and complex software.},
  isbn = {978-1-4503-7122-3},
  file = {/Users/bohrok/Zotero/storage/JVDPSM5T/Lee - 2020 - Scalable and approximate program dependence analysis.pdf}
}

@thesis{leeStatisticalProgramDependence2022,
  type = {phdthesis},
  title = {Statistical Program Dependence Approximation},
  author = {Lee, Seongmin},
  date = {2022-06},
  institution = {{Korea Advanced Institute of Science and Technology (KAIST), Daejeon}},
  url = {https://library.kaist.ac.kr/search/ctlgSearch/thesis/view.do?bibctrlno=1007884&se=t0&ty=B&_csrf=f7c9c960-dd77-46d4-81e7-76c011e04043},
  keywords = {Causal inference,Dynamic analysis,Lexical analysis,notion,Observation-based dependence analysis,Program dependence,Statistical model}
}

@inproceedings{leeStatisticalReachabilityAnalysis2023a,
  title = {Statistical {{Reachability Analysis}}},
  booktitle = {Proceedings of the 31st {{ACM Joint European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Lee, Seongmin and Böhme, Marcel},
  date = {2023-11-30},
  year = {2023},
  series = {{{ESEC}}/{{FSE}} 2023},
  pages = {326--337},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3611643.3616268},
  url = {https://dl.acm.org/doi/10.1145/3611643.3616268},
  urldate = {2024-11-18},
  abstract = {Given a target program state (or statement) s, what is the probability that an input reaches s? This is the quantitative reachability analysis problem. For instance, quantitative reachability analysis can be used to approximate the reliability of a program (where s is a bad state). Traditionally, quantitative reachability analysis is solved as a model counting problem for a formal constraint that represents the (approximate) reachability of s along paths in the program, i.e., probabilistic reachability analysis. However, in preliminary experiments, we failed to run state-of-the-art probabilistic reachability analysis on reasonably large programs. In this paper, we explore statistical methods to estimate reachability probability. An advantage of statistical reasoning is that the size and composition of the program are insubstantial as long as the program can be executed. We are particularly interested in the error compared to the state-of-the-art probabilistic reachability analysis. We realize that existing estimators do not exploit the inherent structure of the program and develop structure-aware estimators to further reduce the estimation error given the same number of samples. Our empirical evaluation on previous and new benchmark programs shows that (i) our statistical reachability analysis outperforms state-of-the-art probabilistic reachability analysis tools in terms of accuracy, efficiency, and scalability, and (ii) our structure-aware estimators further outperform (blackbox) estimators that do not exploit the inherent program structure. We also identify multiple program properties that limit the applicability of the existing probabilistic analysis techniques.},
  isbn = {9798400703270},
  file = {/Users/bohrok/Zotero/storage/BIB8Q3WC/Lee and Böhme - 2023 - Statistical Reachability Analysis.pdf}
}

@online{leeStructureawareResidualRisk2025,
  title = {Structure-Aware {{Residual Risk Analysis}}},
  author = {Lee, Seongmin and Böhme, Marcel},
  date = {2025},
  pubstate = {prepublished}
}

@online{cleverest,
  title = {Can {{LLM Generate Regression Tests for Software Commits?}}},
  author = {Liu, Jing and Lee, Seongmin and Böhme, Marcel},
  date = {2025},
  pubstate = {prepublished}
}

@inproceedings{leeTempuraTemporalDimension2015,
  title = {Tempura: {{Temporal Dimension}} for {{IDEs}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Software Engineering}} - {{Volume}} 1},
  author = {Lee, Yun Young and Marinov, Darko and Johnson, Ralph E.},
  date = {2015},
  series = {{{ICSE}} '15},
  pages = {212--222},
  publisher = {IEEE Press},
  location = {Piscataway, NJ, USA},
  url = {http://dl.acm.org/citation.cfm?id=2818754.2818783},
  isbn = {978-1-4799-1934-5},
  venue = {Florence, Italy},
  keywords = {Code Completion,Multiple Revision,notion,Tempura}
}

@inproceedings{legayStatisticalModelChecking2010,
  title = {Statistical {{Model Checking}}: {{An Overview}}},
  booktitle = {Runtime {{Verification}}},
  author = {Legay, Axel and Delahaye, Benoît and Bensalem, Saddek},
  date = {2010},
  keywords = {notion}
}

@inproceedings{leHistoryDrivenProgram2016,
  title = {History {{Driven Program Repair}}},
  booktitle = {2016 {{IEEE}} 23rd {{International Conference}} on {{Software Analysis}}, {{Evolution}}, and {{Reengineering}} ({{SANER}})},
  author = {Le, X. B. D. and Lo, D. and Goues, C. L.},
  date = {2016-03},
  volume = {1},
  pages = {213--224},
  doi = {10.1109/SANER.2016.76},
  keywords = {APR techniques,Automated Program Repair,automated program repair techniques,bug fix pattern mining,buggy program,Computer bugs,data mining,Data mining,fix candidates,Graph Mining,History,history driven program repair,Java,Java programs,Maintenance engineering,mathematical operators,multicore cloud environment,mutation operators,Mutation Testing,notion,program debugging,program testing,random processes,random search process,search problems,Software,software maintenance,Testing}
}

@inproceedings{leInformationRetrievalSpectrum2015,
  title = {Information {{Retrieval}} and {{Spectrum Based Bug Localization}}: {{Better Together}}},
  booktitle = {Proceedings of the 2015 10th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Le, Tien-Duy B. and Oentaryo, Richard J. and Lo, David},
  date = {2015},
  series = {{{ESEC}}/{{FSE}} 2015},
  pages = {579--590},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2786805.2786880},
  url = {http://doi.acm.org/10.1145/2786805.2786880},
  isbn = {978-1-4503-3675-8},
  venue = {Bergamo, Italy},
  keywords = {Bug Localization,Information Retrieval,notion,Program Spectra}
}

@article{leOverfittingSemanticsbasedAutomated,
  title = {Overfitting in {{Semantics-based Automated Program Repair}}},
  author = {Le, Xuan Bach D and Thung, Ferdian and Lo, David and Le Goues, Claire},
  keywords = {notion}
}

@inproceedings{leS3SyntaxSemanticguided2017,
  title = {S3: {{Syntax-}} and {{Semantic-guided Repair Synthesis}} via {{Programming}} by {{Examples}}},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Le, Xuan-Bach D. and Chu, Duc-Hiep and Lo, David and Le Goues, Claire and Visser, Willem},
  date = {2017},
  series = {{{ESEC}}/{{FSE}} 2017},
  pages = {593--604},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/3106237.3106309},
  url = {http://doi.acm.org/10.1145/3106237.3106309},
  isbn = {978-1-4503-5105-8},
  venue = {Paderborn, Germany},
  keywords = {Inductive Synthesis,notion,Program Repair,Programming by Examples,Symbolic Execution}
}

@article{liblitBugIsolationRemote2003,
  title = {Bug {{Isolation}} via {{Remote Program Sampling}}},
  author = {Liblit, Ben and Aiken, Alex and Zheng, Alice X. and Jordan, Michael I.},
  date = {2003-05},
  journaltitle = {SIGPLAN Not.},
  volume = {38},
  number = {5},
  pages = {141--154},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {0362-1340},
  doi = {10.1145/780822.781148},
  url = {https://doi.org/10.1145/780822.781148},
  abstract = {We propose a low-overhead sampling infrastructure for gathering information from the executions experienced by a program's user community. Several example applications illustrate ways to use sampled instrumentation to isolate bugs. Assertion-dense code can be transformed to share the cost of assertions among many users. Lacking assertions, broad guesses can be made about predicates that predict program errors and a process of elimination used to whittle these down to the true bug. Finally, even for non-deterministic bugs such as memory corruption, statistical modeling based on logistic regression allows us to identify program behaviors that are strongly correlated with failure and are therefore likely places to look for the error.},
  keywords = {assertions,bug isolation,feature selection,logistic regression,notion,random sampling,statistical debugging}
}

@inproceedings{liblitBugIsolationRemote2003a,
  title = {Bug {{Isolation}} via {{Remote Program Sampling}}},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 2003 {{Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Liblit, Ben and Aiken, Alex and Zheng, Alice X. and Jordan, Michael I.},
  date = {2003},
  series = {{{PLDI}} '03},
  pages = {141--154},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/781131.781148},
  url = {https://doi.org/10.1145/781131.781148},
  abstract = {We propose a low-overhead sampling infrastructure for gathering information from the executions experienced by a program's user community. Several example applications illustrate ways to use sampled instrumentation to isolate bugs. Assertion-dense code can be transformed to share the cost of assertions among many users. Lacking assertions, broad guesses can be made about predicates that predict program errors and a process of elimination used to whittle these down to the true bug. Finally, even for non-deterministic bugs such as memory corruption, statistical modeling based on logistic regression allows us to identify program behaviors that are strongly correlated with failure and are therefore likely places to look for the error.},
  isbn = {1-58113-662-5},
  venue = {San Diego, California, USA},
  keywords = {assertions,bug isolation,feature selection,logistic regression,notion,random sampling,statistical debugging}
}

@article{liblitScalableStatisticalBug2005,
  title = {Scalable {{Statistical Bug Isolation}}},
  author = {Liblit, Ben and Naik, Mayur and Zheng, Alice X. and Aiken, Alex and Jordan, Michael I.},
  date = {2005-06},
  journaltitle = {SIGPLAN Not.},
  volume = {40},
  number = {6},
  pages = {15--26},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {0362-1340},
  doi = {10.1145/1064978.1065014},
  url = {https://doi.org/10.1145/1064978.1065014},
  abstract = {We present a statistical debugging algorithm that isolates bugs in programs containing multiple undiagnosed bugs. Earlier statistical algorithms that focus solely on identifying predictors that correlate with program failure perform poorly when there are multiple bugs. Our new technique separates the effects of different bugs and identifies predictors that are associated with individual bugs. These predictors reveal both the circumstances under which bugs occur as well as the frequencies of failure modes, making it easier to prioritize debugging efforts. Our algorithm is validated using several case studies, including examples in which the algorithm identified previously unknown, significant crashing bugs in widely used systems.},
  keywords = {bug isolation,feature selection,invariants,notion,random sampling,statistical debugging}
}

@inproceedings{liblitScalableStatisticalBug2005a,
  title = {Scalable {{Statistical Bug Isolation}}},
  booktitle = {Proceedings of the 2005 {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Liblit, Ben and Naik, Mayur and Zheng, Alice X. and Aiken, Alex and Jordan, Michael I.},
  date = {2005},
  series = {{{PLDI}} '05},
  pages = {15--26},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1065010.1065014},
  url = {https://doi.org/10.1145/1065010.1065014},
  abstract = {We present a statistical debugging algorithm that isolates bugs in programs containing multiple undiagnosed bugs. Earlier statistical algorithms that focus solely on identifying predictors that correlate with program failure perform poorly when there are multiple bugs. Our new technique separates the effects of different bugs and identifies predictors that are associated with individual bugs. These predictors reveal both the circumstances under which bugs occur as well as the frequencies of failure modes, making it easier to prioritize debugging efforts. Our algorithm is validated using several case studies, including examples in which the algorithm identified previously unknown, significant crashing bugs in widely used systems.},
  isbn = {1-59593-056-6},
  venue = {Chicago, IL, USA},
  keywords = {bug isolation,feature selection,invariants,notion,random sampling,statistical debugging}
}

@incollection{liDeepFLIntegratingMultiple2019,
  title = {{{DeepFL}}: {{Integrating Multiple Fault Diagnosis Dimensions}} for {{Deep Fault Localization}}},
  booktitle = {Proceedings of the 28th {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Li, Xia and Li, Wei and Zhang, Yuqun and Zhang, Lingming},
  date = {2019},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {https://doi.org/10.1145/3293882.3330574},
  abstract = {Learning-based fault localization has been intensively studied recently. Prior studies have shown that traditional Learning-to-Rank techniques can help precisely diagnose fault locations using various dimensions of fault-diagnosis features, such as suspiciousness values computed by various off-the-shelf fault localization techniques. However, with the increasing dimensions of features considered by advanced fault localization techniques, it can be quite challenging for the traditional Learning-to-Rank algorithms to automatically identify effective existing/latent features. In this work, we propose DeepFL, a deep learning approach to automatically learn the most effective existing/latent features for precise fault localization. Although the approach is general, in this work, we collect various suspiciousness-value-based, fault-proneness-based and textual-similarity-based features from the fault localization, defect prediction and information retrieval areas, respectively. DeepFL has been studied on 395 real bugs from the widely used Defects4J benchmark. The experimental results show DeepFL can significantly outperform state-of-the-art TraPT/FLUCCS (e.g., localizing 50+ more faults within Top-1). We also investigate the impacts of deep model configurations (e.g., loss functions and epoch settings) and features. Furthermore, DeepFL is also surprisingly effective for cross-project prediction.},
  isbn = {978-1-4503-6224-5},
  keywords = {notion}
}

@inproceedings{liDeepLVSuggestingLog2021,
  title = {{{DeepLV}}: {{Suggesting Log Levels Using Ordinal Based Neural Networks}}},
  booktitle = {2021 {{IEEE}}/{{ACM}} 43rd {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Li, Zhenhao and Li, Heng and Chen, Tse-Hsun Peter and Shang, Weiyi},
  date = {2021},
  pages = {1461--1472},
  publisher = {IEEE},
  keywords = {notion}
}

@inproceedings{liDroidbotLightweightUiguided2017,
  title = {Droidbot: A Lightweight Ui-Guided Test Input Generator for Android},
  booktitle = {2017 {{IEEE}}/{{ACM}} 39th {{International Conference}} on {{Software Engineering Companion}} ({{ICSE-C}})},
  author = {Li, Yuanchun and Yang, Ziyue and Guo, Yao and Chen, Xiangqun},
  date = {2017},
  pages = {23--26},
  publisher = {IEEE},
  keywords = {notion}
}

@article{liEffectiveFaultLocalization2014,
  title = {Effective {{Fault Localization Using Weighted Test Cases}}},
  author = {Li, Yihan and Liu, Chao},
  date = {2014},
  journaltitle = {J. Softw.},
  volume = {9},
  pages = {2112--2119},
  keywords = {notion}
}

@article{liEnlightenedDebugging2018,
  title = {Enlightened {{Debugging}}},
  author = {Li, Xiangyu and Zhu, Shaowei and family=Amorim, given=Marcelo, prefix=d', useprefix=true and Orso, Alessandro},
  date = {2018},
  keywords = {notion}
}

@inproceedings{liHavocParadoxGeneratorBased2024,
  title = {The {{Havoc Paradox}} in {{Generator-Based Fuzzing}} ({{Registered Report}})},
  booktitle = {Proceedings of the 3rd {{ACM International Fuzzing Workshop}}},
  author = {Li, Ao and Huang, Madonna and Lemieux, Caroline and Padhye, Rohan},
  date = {2024-09-13},
  series = {{{FUZZING}} 2024},
  pages = {3--12},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3678722.3685529},
  url = {https://dl.acm.org/doi/10.1145/3678722.3685529},
  urldate = {2024-09-19},
  abstract = {Parametric generators are a simple way to combine coverage-guided and generator-based fuzzing. Parametric generators can be thought of as decoders of an arbitrary byte sequence into a structured input. This allows mutations on the byte sequence to map to mutations on the structured input, without requiring the writing of specialized mutators. However, this technique is prone to the havoc effect, where small mutations on the byte sequence cause large, destructive mutations to the structured input. This registered report first provides a preliminary investigation of the paradoxical nature of the havoc effect for generator-based fuzzing in Java. In particular, we measure mutation characteristics and confirm the existence of the havoc effect, as well as scenarios where it may be more detrimental. The proposed evaluation extends this investigation over more benchmarks, with the tools Zest, JQF’s EI, BeDivFuzz, and Zeugma.},
  isbn = {9798400711121},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/IIFAE8P3/Li et al. - 2024 - The Havoc Paradox in Generator-Based Fuzzing (Registered Report).pdf}
}

@inproceedings{liIdentifyCoincidentalCorrect2016,
  title = {Identify {{Coincidental Correct Test Cases Based}} on {{Fuzzy Classification}}},
  booktitle = {2016 {{International Conference}} on {{Software Analysis}}, {{Testing}} and {{Evolution}} ({{SATE}})},
  author = {Li, Zheng and Li, Meiying and Liu, Yong and Geng, Jingyao},
  date = {2016},
  pages = {72--77},
  doi = {10.1109/SATE.2016.19},
  keywords = {notion}
}

@article{liIdentifyingCoincidentalCorrectness2014,
  title = {Identifying Coincidental Correctness in Fault Localization via Cluster Analysis},
  author = {Li, Y and Liu, C},
  date = {2014},
  journaltitle = {J. Softw. Eng},
  volume = {8},
  number = {4},
  pages = {328--344},
  keywords = {notion}
}

@misc{limDifferentiableAlgorithmMarginalising2019,
  title = {Differentiable {{Algorithm}} for {{Marginalising Changepoints}}},
  author = {Lim, Hyoungjin and Che, Gwonsoo and Lee, Wonyeol and Yang, Hongseok},
  date = {2019},
  keywords = {notion}
}

@inproceedings{liMoreAccurateDynamic2020,
  title = {More {{Accurate Dynamic Slicing}} for {{Better Supporting Software Debugging}}},
  booktitle = {{{IEEE}} 13th {{International Conference}} on {{Software Testing}}, {{Validation}} and {{Verification}} ({{ICST}})},
  author = {Li, Xiangyu and Orso, Alessandro},
  date = {2020},
  publisher = {IEEE},
  keywords = {notion}
}

@inproceedings{linDerivingInputSyntactic2008,
  title = {Deriving {{Input Syntactic Structure}} from {{Execution}}},
  booktitle = {Proceedings of the 16th {{ACM SIGSOFT International Symposium}} on {{Foundations}} of {{Software Engineering}}},
  author = {Lin, Zhiqiang and Zhang, Xiangyu},
  date = {2008},
  series = {{{SIGSOFT}} '08/{{FSE-16}}},
  pages = {83--93},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1453101.1453114},
  url = {https://doi.org/10.1145/1453101.1453114},
  abstract = {Program input syntactic structure is essential for a wide range of applications such as test case generation, software debugging and network security. However, such important information is often not available (e.g., most malware programs make use of secret protocols to communicate) or not directly usable by machines (e.g., many programs specify their inputs in plain text or other random formats). Furthermore, many programs claim they accept inputs with a published format, but their implementations actually support a subset or a variant. Based on the observations that input structure is manifested by the way input symbols are used during execution and most programs take input with top-down or bottom-up grammars, we devise two dynamic analyses, one for each grammar category. Our evaluation on a set of real-world programs shows that our technique is able to precisely reverse engineer input syntactic structure from execution.},
  isbn = {978-1-59593-995-1},
  venue = {Atlanta, Georgia},
  keywords = {bottom-up grammar,control dependence,input lineage,notion,reverse engineering,syntax tree,top-down grammar}
}

@inproceedings{linFeedbackBasedDebugging2017,
  title = {Feedback-{{Based Debugging}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Software Engineering}}},
  author = {Lin, Yun and Sun, Jun and Xue, Yinxing and Liu, Yang and Dong, Jinsong},
  date = {2017},
  series = {{{ICSE}} '17},
  pages = {393--403},
  publisher = {IEEE Press},
  location = {Buenos Aires, Argentina},
  doi = {10.1109/ICSE.2017.43},
  url = {https://doi.org/10.1109/ICSE.2017.43},
  abstract = {Software debugging has long been regarded as a time and effort consuming task. In the process of debugging, developers usually need to manually inspect many program steps to see whether they deviate from their intended behaviors. Given that intended behaviors usually exist nowhere but in human mind, the automation of debugging turns out to be extremely hard, if not impossible.In this work, we propose a feedback-based debugging approach, which (1) builds on light-weight human feedbacks on a buggy program and (2) regards the feedbacks as partial program specification to infer suspicious steps of the buggy execution. Given a buggy program, we record its execution trace and allow developers to provide light-weight feedback on trace steps. Based on the feedbacks, we recommend suspicious steps on the trace. Moreover, our approach can further learn and approximate bug-free paths, which helps reduce required feedbacks to expedite the debugging process. We conduct an experiment to evaluate our approach with simulated feedbacks on 3409 mutated bugs across 3 open source projects. The results show that our feedback-based approach can detect 92.8\% of the bugs and 65\% of the detected bugs require less than 20 feedbacks. In addition, we implement our proof-of-concept tool, Microbat, and conduct a user study involving 16 participants on 3 debugging tasks. The results show that, compared to the participants using the baseline tool, Whyline, the ones using Microbat can spend on average 55.8\% less time to locate the bugs.},
  isbn = {978-1-5386-3868-2},
  keywords = {notion}
}

@inproceedings{linQuixBugsMultilingualProgram2017,
  title = {{{QuixBugs}}: {{A Multi-lingual Program Repair Benchmark Set Based}} on the {{Quixey Challenge}}},
  booktitle = {Proceedings {{Companion}} of the 2017 {{ACM SIGPLAN International Conference}} on {{Systems}}, {{Programming}}, {{Languages}}, and {{Applications}}: {{Software}} for {{Humanity}}},
  author = {Lin, Derrick and Koppel, James and Chen, Angela and Solar-Lezama, Armando},
  date = {2017},
  series = {{{SPLASH Companion}} 2017},
  pages = {55--56},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/3135932.3135941},
  url = {http://doi.acm.org/10.1145/3135932.3135941},
  isbn = {978-1-4503-5514-8},
  venue = {Vancouver, BC, Canada},
  keywords = {automated program repair,benchmark,notion}
}

@inproceedings{lippEmpiricalStudyEffectiveness2022,
  title = {An Empirical Study on the Effectiveness of Static {{C}} Code Analyzers for Vulnerability Detection},
  booktitle = {Proceedings of the 31st {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Lipp, Stephan and Banescu, Sebastian and Pretschner, Alexander},
  date = {2022},
  pages = {544--555},
  keywords = {notion}
}

@article{lippFuzzTasticFinegrainedFuzzeragnostic2022,
  title = {{{FuzzTastic}}: {{A Fine-grained}}, {{Fuzzer-agnostic Coverage Analyzer}}},
  author = {Lipp, Stephan and Elsner, Daniel and Hutzelmann, Thomas and Banescu, Sebastian and Pretschner, Alexander and Böhme, Marcel},
  date = {2022},
  journaltitle = {2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)},
  pages = {75--79},
  keywords = {notion}
}

@article{liptonMythosModelInterpretability2018,
  title = {The {{Mythos}} of {{Model Interpretability}}: {{In}} Machine Learning, the Concept of Interpretability Is Both Important and Slippery.},
  author = {Lipton, Zachary C},
  date = {2018},
  journaltitle = {Queue},
  volume = {16},
  number = {3},
  pages = {31--57},
  publisher = {ACM New York, NY, USA},
  keywords = {notion}
}

@article{liRoleCalibrationValidation2016,
  title = {Role of Calibration, Validation, and Relevance in Multi-Level Uncertainty Integration},
  author = {Li, Chenzhao and Mahadevan, Sankaran},
  date = {2016},
  journaltitle = {Reliab. Eng. Syst. Saf.},
  volume = {148},
  pages = {32--43},
  keywords = {notion}
}

@article{liSemanticSlicingSoftware2018,
  title = {Semantic {{Slicing}} of {{Software Version Histories}}},
  author = {Li, Y. and Zhu, C. and Rubin, J. and Chechik, M.},
  date = {2018-02},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {44},
  number = {2},
  pages = {182--201},
  issn = {2326-3881},
  doi = {10.1109/TSE.2017.2664824},
  keywords = {Computer bugs,configuration management,configuration management system,Context,CSlicer,dependency,History,Java,Minimization,notion,open-source software repositories,program analysis,program slicing,public domain software,semantic slicing problem,semantically-related commits,Semantics,Software,Software changes,software developers,software version histories,version control}
}

@article{liTestOracleStrategies2017,
  title = {Test {{Oracle Strategies}} for {{Model-Based Testing}}},
  author = {Li, Nan and Offutt, Jeff},
  date = {2017},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {43},
  number = {4},
  pages = {372--395},
  doi = {10.1109/TSE.2016.2597136},
  keywords = {Concrete,Context,model-based testing,notion,Observability,RIPR model,Software,subsumption,System testing,test automation,Test oracle,test oracle strategy,Unified modeling language}
}

@misc{liTransRegexMultimodalRegular2021,
  title = {{{TransRegex}}: {{Multi-modal Regular Expression Synthesis}} by {{Generate-and-Repair}}},
  author = {Li, Yeting and Li, Shuaimin and Xu, Zhiwu and Cao, Jialun and Chen, Zixuan and Hu, Yun and Chen, Haiming and Cheung, Shing-Chi},
  date = {2021},
  keywords = {notion}
}

@article{littlewoodConservativeStoppingRules1997,
  title = {Some Conservative Stopping Rules for the Operational Testing of Safety Critical Software},
  author = {Littlewood, B. and Wright, D.},
  date = {1997-11},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {23},
  number = {11},
  pages = {673--683},
  issn = {1939-3520},
  doi = {10.1109/32.637384},
  url = {https://ieeexplore.ieee.org/abstract/document/637384},
  urldate = {2024-10-29},
  abstract = {Operational testing, which aims to generate sequences of test cases with the same statistical properties as those that would be experienced in real operational use, can be used to obtain quantitative measures of the reliability of software. In the case of safety critical software it is common to demand that all known faults are removed. This means that if there is a failure during the operational testing, the offending fault must be identified and removed. Thus an operational test for safety critical software takes the form of a specified number of test cases (or a specified period of working) that must be executed failure-free. This paper addresses the problem of specifying the numbers of test cases (or time periods) required for a test, when the previous test has terminated as a result of a failure. It has been proposed that, after the obligatory fix of the offending fault, the software should be treated as if it were completely novel, and be required to pass exactly the same test as originally specified. The reasoning here claims to be conservative, in as much as no credit is given for any previous failure-free operation prior to the failure that terminated the test. We show that, in fact, this is not a conservative approach in all cases, and propose instead some new Bayesian stopping rules. We show that the degree of conservatism in stopping rules depends upon the precise way in which the reliability requirement is expressed. We define a particular form of conservatism that seems desirable on intuitive grounds, and show that the stopping rules that exhibit this conservatism are also precisely the ones that seem preferable on other grounds.},
  eventtitle = {{{IEEE Transactions}} on {{Software Engineering}}},
  keywords = {Battery powered vehicles,Bayesian methods,Licenses,notion,Phase frequency detector,Protection,Regulators,Software measurement,Software safety,Software testing,System testing},
  file = {/Users/bohrok/Zotero/storage/97JSEAUJ/Littlewood and Wright - 1997 - Some conservative stopping rules for the operational testing of safety critical software.pdf;/Users/bohrok/Zotero/storage/Q6RD7LHR/637384.html}
}

@inproceedings{liuFailureProximityFault2006,
  title = {Failure Proximity: A Fault Localization-Based Approach},
  booktitle = {Proceedings of the 14th {{ACM SIGSOFT}} International Symposium on {{Foundations}} of Software Engineering},
  author = {Liu, Chao and Han, Jiawei},
  date = {2006},
  pages = {46--56},
  keywords = {notion}
}

@inproceedings{liuLogzipExtractingHidden2019,
  title = {Logzip: {{Extracting Hidden Structures}} via {{Iterative Clustering}} for {{Log Compression}}},
  booktitle = {2019 34th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Liu, Jinyang and Zhu, Jieming and He, Shilin and He, Pinjia and Zheng, Zibin and Lyu, Michael R.},
  date = {2019},
  pages = {863--873},
  doi = {10.1109/ASE.2019.00085},
  keywords = {notion}
}

@inproceedings{liuLSRepairLiveSearch2018,
  title = {{{LSRepair}}: {{Live Search}} of {{Fix Ingredients}} for {{Automated Program Repair}}},
  author = {Liu, Kui and Koyuncu, Anil and Kim, Kisub and Kim, Dongsun and Bissyande, Tegawendé François D. Assise},
  date = {2018},
  keywords = {notion}
}

@article{liuRethinkingIncrementalParallel2019,
  title = {Rethinking {{Incremental}} and {{Parallel Pointer Analysis}}},
  author = {Liu, Bozhen and Huang, Jeff and Rauchwerger, Lawrence},
  date = {2019},
  journaltitle = {ACM Transactions on Programming Languages and Systems (TOPLAS)},
  volume = {41},
  pages = {1--31},
  url = {https://api.semanticscholar.org/CorpusID:69167519},
  keywords = {notion}
}

@article{liuSOBERStatisticalModelbased2005,
  title = {{{SOBER}}: Statistical Model-Based Bug Localization},
  author = {Liu, Chao and Yan, Xifeng and Fei, Long and Han, Jiawei and Midkiff, Samuel P},
  date = {2005},
  journaltitle = {ACM SIGSOFT Software Engineering Notes},
  volume = {30},
  number = {5},
  pages = {286--295},
  publisher = {ACM New York, NY, USA},
  keywords = {notion}
}

@article{liuTBarRevisitingTemplatebased2019,
  title = {{{TBar}}: Revisiting Template-Based Automated Program Repair},
  author = {Liu, Kui and Koyuncu, Anil and Kim, Dongsun and Bissyandé, Tegawendé F.},
  date = {2019},
  journaltitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
  keywords = {notion}
}

@article{liuWhichVariablesShould2021,
  title = {Which {{Variables Should I Log}}?},
  author = {Liu, Zhongxin and Xia, Xin and Lo, David and Xing, Zhenchang and Hassan, Ahmed E. and Li, Shanping},
  date = {2021},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {47},
  number = {9},
  pages = {2012--2031},
  doi = {10.1109/TSE.2019.2941943},
  keywords = {notion}
}

@inproceedings{livadasProgramDependenceAnalysis1992,
  title = {Program {{Dependence Analysis}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Software Maintenance}} 1992},
  author = {Livadas, Panos E. and Roy, Prabal K.},
  date = {1992-11},
  pages = {356--365},
  publisher = {cspress},
  location = {Los Alamitos, California, USA},
  abstract = {It is generally recognized that one of the reasons that software maintenance is so costly is that each modification to a program must take into account the numerous complex interrelationships in the existing software; an understanding of program dependences is fundamental to efficient software change. Such dependences can be of the following types, data flow, calling, and functional dependences. Furthermore, as the software community gradually begins to move toward a more object-oriented perspective on software development, it will become increasingly important to be able to 'objectify' existing software systems. Successful maintenance requires precise knowledge of the data items in a system, the ways these items are created and modified, and their relationships between one another. In this paper the authors address these two issues. First, they will discuss three methods of identifying objects the first two of which were suggested by Liu and Wilde; the third method is one that is proposed in this paper and is based on the determination of the receiver of a procedure. We believe that the latter method is one that is more natural and precise than the former two. Second, algorithms that perform precise interprocedural flow-sensitive dependency analysis, as well as algorithms that identify 'objects', are introduced. Furthermore, the internal program representation that we emply, the parse-tree-based system dependence graph (SDG), is briefly discussed. Finally, a unique tool that we have developed is presented that accepts a subset of ANSI C (or Pascal) as input and which implements all algorithms discussed in this paper.},
  keywords = {notion}
}

@inproceedings{liWhereShallWe2020,
  title = {Where {{Shall We Log}}? {{Studying}} and {{Suggesting Logging Locations}} in {{Code Blocks}}},
  booktitle = {2020 35th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Li, Zhenhao and Chen, Tse-Hsun and Shang, Weiyi},
  date = {2020},
  pages = {361--372},
  keywords = {notion}
}

@inproceedings{liyanageExtrapolatingCoverageRate2024a,
  title = {Extrapolating {{Coverage Rate}} in {{Greybox Fuzzing}}},
  booktitle = {Proceedings of the {{IEEE}}/{{ACM}} 46th {{International Conference}} on {{Software Engineering}}},
  author = {Liyanage, Danushka and Lee, Seongmin and Tantithamthavorn, Chakkrit and Böhme, Marcel},
  date = {2024-04-12},
  year = {2024},
  series = {{{ICSE}} '24},
  pages = {1--12},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3597503.3639198},
  url = {https://dl.acm.org/doi/10.1145/3597503.3639198},
  urldate = {2024-11-18},
  abstract = {A fuzzer can literally run forever. However, as more resources are spent, the coverage rate continuously drops, and the utility of the fuzzer declines. To tackle this coverage-resource tradeoff, we could introduce a policy to stop a campaign whenever the coverage rate drops below a certain threshold value, say 10 new branches covered per 15 minutes. During the campaign, can we predict the coverage rate at some point in the future? If so, how well can we predict the future coverage rate as the prediction horizon or the current campaign length increases? How can we tackle the statistical challenge of adaptive bias, which is inherent in greybox fuzzing (i.e., samples are not independent and identically distributed)?In this paper, we i) evaluate existing statistical techniques to predict the coverage rate U(t0 + k) at any time t0 in the campaign after a period of k units of time in the future and ii) develop a new extrapolation methodology that tackles the adaptive bias. We propose to efficiently simulate a large number of blackbox campaigns from the collected coverage data, estimate the coverage rate for each of these blackbox campaigns and conduct a simple regression to extrapolate the coverage rate for the greybox campaign.Our empirical evaluation using the Fuzztastic fuzzer benchmark demonstrates that our extrapolation methodology exhibits at least one order of magnitude lower error compared to the existing benchmark for 4 out of 5 experimental subjects we investigated. Notably, compared to the existing extrapolation methodology, our extrapolator excels in making long-term predictions, such as those extending up to three times the length of the current campaign.},
  isbn = {9798400702174},
  file = {/Users/bohrok/Zotero/storage/P5YD5LK2/Liyanage et al. - 2024 - Extrapolating Coverage Rate in Greybox Fuzzing.pdf}
}

@inproceedings{liyanageReachableCoverageEstimating2023,
  title = {Reachable {{Coverage}}: {{Estimating Saturation}} in {{Fuzzing}}},
  booktitle = {2023 {{IEEE}}/{{ACM}} 45th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Liyanage, Danushka and Böhme, Marcel and Tantithamthavorn, Chakkrit and Lipp, Stephan},
  date = {2023},
  pages = {371--383},
  doi = {10.1109/ICSE48619.2023.00042},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/RCKV472F/Liyanage et al. - 2023 - Reachable Coverage Estimating Saturation in Fuzzing.pdf}
}

@inproceedings{liyanageSecurityGuaranteesAutomated2021,
  title = {Security {{Guarantees}} for {{Automated Software Testing}}},
  booktitle = {Proceedings of the 29th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Liyanage, Danushka},
  date = {2021},
  series = {{{ESEC}}/{{FSE}} 2021},
  pages = {1610--1614},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3468264.3473097},
  url = {https://doi.org/10.1145/3468264.3473097},
  abstract = {Before making important decisions about an ongoing fuzzing campaign, we believe an engineer may want to know: (i) the achieved level of confidence about the program's correctness (residual risk), (ii) the expected increase in confidence about the program's correctness if we invest more time for the current campaign (cost-benefit trade-off), and (iii) the total number of bugs that the fuzzer can find in the limit (effectiveness). The ability to accurately estimate the above quantities through observed data of the fuzzing campaign allows engineers to make required decisions with quantifiable accuracy. Currently, there are popular data-driven approaches to provide such quantitative guidance on decision making for white- and blackbox fuzzing campaigns. However, none of these prevailing techniques can guarantee unbiased estimation of residual risk, cost-benefit trade-off, or effectiveness for greybox fuzzing – the most popular automated software vulnerability discovery technique to date. Greybox fuzzers introduce an adaptive bias to existing estimators that needs to be corrected during the quantitative analysis to make accurate decisions about the campaign. In this thesis, our primary objective is to develop a rich statistical framework that supports quantitative decision-making for greybox fuzzing campaigns. We leverage this framework to introduce appropriate bias correction strategies to existing estimators and propose novel estimators that account for adaptive bias in greybox fuzzing.},
  isbn = {978-1-4503-8562-6},
  venue = {Athens, Greece},
  keywords = {estimation,fuzzing,notion,probability,software testing,statistics}
}

@article{loeraEffectiveLatticePoint2004,
  title = {Effective Lattice Point Counting in Rational Convex Polytopes},
  author = {Loera, Jesús A. De and Hemmecke, Raymond and Tauzer, Jeremiah and Yoshida, Ruriko},
  date = {2004},
  journaltitle = {Journal of Symbolic Computation},
  volume = {38},
  number = {4},
  pages = {1273--1302},
  issn = {0747-7171},
  doi = {10.1016/j.jsc.2003.04.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0747717104000422},
  abstract = {This paper discusses algorithms and software for the enumeration of all lattice points inside a rational convex polytope: we describe LattE, a computer package for lattice point enumeration which contains the first implementation of A. Barvinok's algorithm (Math. Oper. Res. 19 (1994) 769). We report on computational experiments with multiway contingency tables, knapsack type problems, rational polygons, and flow polytopes. We prove that these kinds of symbolic–algebraic ideas surpass the traditional branch-and-bound enumeration and in some instances LattE is the only software capable of counting. Using LattE, we have also computed new formulas of Ehrhart (quasi-)polynomials for interesting families of polytopes (hypersimplices, truncated cubes, etc). We end with a survey of other “algebraic–analytic” algorithms, including a “homogeneous” variation of Barvinok's algorithm which is very fast when the number of facet-defining inequalities is much smaller compared to the number of vertices.},
  keywords = {Barvinok's algorithm,Convex rational polyhedra,Ehrhart quasi-polynomials,Enumeration of lattice points,Generating functions,Lattice points,notion,Rational functions}
}

@inproceedings{longAutomaticInferenceCode2017,
  title = {Automatic Inference of Code Transforms for Patch Generation},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}, {{ESEC}}/{{FSE}}},
  author = {Long, Fan and Amidon, Peter and Rinard, Martin},
  date = {2017},
  volume = {2017},
  keywords = {Automated Patch Generation,Automated Program Repair,notion}
}

@inproceedings{longAutomaticPatchGeneration2016,
  title = {Automatic {{Patch Generation}} by {{Learning Correct Code}}},
  booktitle = {Proceedings of the 43rd {{Annual ACM SIGPLAN-SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Long, Fan and Rinard, Martin},
  date = {2016},
  series = {{{POPL}} '16},
  pages = {298--312},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2837614.2837617},
  url = {http://doi.acm.org/10.1145/2837614.2837617},
  isbn = {978-1-4503-3549-2},
  venue = {St. Petersburg, FL, USA},
  keywords = {Code correctness model,Learning correct code,notion,Program repair}
}

@inproceedings{longStagedProgramRepair2015,
  title = {Staged {{Program Repair}} with {{Condition Synthesis}}},
  booktitle = {Proceedings of the 2015 10th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Long, Fan and Rinard, Martin},
  date = {2015},
  series = {{{ESEC}}/{{FSE}} 2015},
  pages = {166--178},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2786805.2786811},
  url = {http://doi.acm.org/10.1145/2786805.2786811},
  isbn = {978-1-4503-3675-8},
  venue = {Bergamo, Italy},
  keywords = {Condition synthesis,notion,Program repair,Staged repair}
}

@incollection{lopezStudyConcurrencyBugs2018,
  title = {A Study of Concurrency Bugs and Advanced Development Support for Actor-Based Programs},
  booktitle = {Programming with {{Actors}}},
  author = {Lopez, Carmen Torres and Marr, Stefan and Boix, Elisa Gonzalez and Mössenböck, Hanspeter},
  date = {2018},
  pages = {155--185},
  publisher = {Springer},
  keywords = {notion}
}

@article{louBoostingCoveragebasedFault2021,
  title = {Boosting Coverage-Based Fault Localization via Graph-Based Representation Learning},
  author = {Lou, Yiling and Zhu, Qihao and Dong, Jinhao and Li, Xia and Sun, Zeyu and Hao, Dan and Zhang, Lu and Zhang, Lingming},
  date = {2021},
  journaltitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  keywords = {notion}
}

@inproceedings{louCanAutomatedProgram2020,
  title = {Can {{Automated Program Repair Refine Fault Localization}}? {{A Unified Debugging Approach}}},
  booktitle = {Proceedings of the 29th {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Lou, Yiling and Ghanbari, Ali and Li, Xia and Zhang, Lingming and Zhang, Haotian and Hao, Dan and Zhang, Lu},
  date = {2020},
  series = {{{ISSTA}} 2020},
  pages = {75--87},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3395363.3397351},
  url = {https://doi.org/10.1145/3395363.3397351},
  abstract = {A large body of research efforts have been dedicated to automated software debugging, including both automated fault localization and program repair. However, existing fault localization techniques have limited effectiveness on real-world software systems while even the most advanced program repair techniques can only fix a small ratio of real-world bugs. Although fault localization and program repair are inherently connected, their only existing connection in the literature is that program repair techniques usually use off-the-shelf fault localization techniques (e.g., Ochiai) to determine the potential candidate statements/elements for patching. In this work, we propose the unified debugging approach to unify the two areas in the other direction for the first time, i.e., can program repair in turn help with fault localization? In this way, we not only open a new dimension for more powerful fault localization, but also extend the application scope of program repair to all possible bugs (not only the bugs that can be directly automatically fixed). We have designed ProFL to leverage patch-execution results (from program repair) as the feedback information for fault localization. The experimental results on the widely used Defects4J benchmark show that the basic ProFL can already at least localize 37.61\% more bugs within Top-1 than state-of-the-art spectrum and mutation based fault localization. Furthermore, ProFL can boost state-of-the-art fault localization via both unsupervised and supervised learning. Meanwhile, we have demonstrated ProFL's effectiveness under different settings and through a case study within Alipay, a popular online payment system with over 1 billion global users.},
  isbn = {978-1-4503-8008-9},
  venue = {Virtual Event, USA},
  keywords = {Automated Program Repair,Fault Localization,notion,Unified Debugging}
}

@inproceedings{luckowExactApproximateProbabilistic2014,
  title = {Exact and {{Approximate Probabilistic Symbolic Execution}} for {{Nondeterministic Programs}}},
  booktitle = {Proceedings of the 29th {{ACM}}/{{IEEE International Conference}} on {{Automated Software Engineering}}},
  author = {Luckow, Kasper and Păsăreanu, Corina S. and Dwyer, Matthew B. and Filieri, Antonio and Visser, Willem},
  date = {2014},
  series = {{{ASE}} '14},
  pages = {575--586},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2642937.2643011},
  url = {https://doi.org/10.1145/2642937.2643011},
  abstract = {Probabilistic software analysis seeks to quantify the likelihood of reaching a target event under uncertain environments. Recent approaches compute probabilities of execution paths using symbolic execution, but do not support nondeterminism. Nondeterminism arises naturally when no suitable probabilistic model can capture a program behavior, e.g., for multithreading or distributed systems.In this work, we propose a technique, based on symbolic execution, to synthesize schedulers that resolve nondeterminism to maximize the probability of reaching a target event. To scale to large systems, we also introduce approximate algorithms to search for good schedulers, speeding up established random sampling and reinforcement learning results through the quantification of path probabilities based on symbolic execution.We implemented the techniques in Symbolic PathFinder and evaluated them on nondeterministic Java programs. We show that our algorithms significantly improve upon a state-of-the-art statistical model checking algorithm, originally developed for Markov Decision Processes.},
  isbn = {978-1-4503-3013-8},
  venue = {Vasteras, Sweden},
  keywords = {nondeterministic programs,notion,probabilistic software analysis,symbolic execution}
}

@incollection{luckowMonteCarloTree2018,
  title = {Monte {{Carlo Tree Search}} for {{Finding Costly Paths}} in {{Programs}}},
  booktitle = {Software {{Engineering}} and {{Formal Methods}}},
  author = {Luckow, Kasper and Păsăreanu, Corina S. and Visser, Willem},
  date = {2018},
  pages = {123--138},
  publisher = {Springer International Publishing},
  doi = {10.1007/978-3-319-92970-5_8},
  url = {https://doi.org/10.1007%2F978-3-319-92970-5_8},
  keywords = {notion}
}

@inproceedings{lukinsSourceCodeRetrieval2008,
  title = {Source {{Code Retrieval}} for {{Bug Localization Using Latent Dirichlet Allocation}}},
  booktitle = {2008 15th {{Working Conference}} on {{Reverse Engineering}}},
  author = {Lukins, S. K. and Kraft, N. A. and Etzkorn, L. H.},
  date = {2008-10},
  pages = {155--164},
  issn = {1095-1350},
  doi = {10.1109/WCRE.2008.33},
  abstract = {In bug localization, a developer uses information about a bug to locate the portion of the source code to modify to correct the bug. Developers expend considerable effort performing this task. Some recent static techniques for automatic bug localization have been built around modern information retrieval (IR) models such as latent semantic indexing (LSI); however, latent Dirichlet allocation (LDA), a modular and extensible IR model, has significant advantages over both LSI and probabilistic LSI (pLSI). In this paper we present an LDA-based static technique for automating bug localization. We describe the implementation of our technique and three case studies that measure its effectiveness. For two of the case studies we directly compare our results to those from similar studies performed using LSI. The results demonstrate our LDA-based technique performs at least as well as the LSI-based techniques for all bugs and performs better, often significantly so, than the LSI-based techniques for most bugs.},
  keywords = {Aging,automatic bug localization,automating bug localization,bug localization,Computer bugs,Costs,indexing,Indexing,information retrieval,Information retrieval,Large scale integration,latent Dirichlet allocation,latent semantic indexing,LDA,LDA-based static technique,Linear discriminant analysis,LSI,notion,program comprehension,Reverse engineering,software engineering,Software maintenance,Software systems,source code retrieval}
}

@article{luLearningConceptDrift2018,
  title = {Learning under Concept Drift: {{A}} Review},
  author = {Lu, Jie and Liu, Anjin and Dong, Fan and Gu, Feng and Gama, Joao and Zhang, Guangquan},
  date = {2018},
  journaltitle = {IEEE transactions on knowledge and data engineering},
  volume = {31},
  number = {12},
  pages = {2346--2363},
  publisher = {IEEE},
  keywords = {notion}
}

@article{luoEmpiricalAnalysisFlaky2014,
  title = {An Empirical Analysis of Flaky Tests},
  author = {Luo, Qingzhou and Hariri, Farah and Eloussi, Lamyaa and Marinov, Darko},
  date = {2014},
  journaltitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
  keywords = {notion}
}

@article{luoWhenCausalInference2020,
  title = {When Causal Inference Meets Deep Learning},
  author = {Luo, Yunan and Peng, Jian and Ma, Jianzhu},
  date = {2020-08-01},
  journaltitle = {Nature Machine Intelligence},
  volume = {2},
  number = {8},
  pages = {426--427},
  doi = {10.1038/s42256-020-0218-x},
  url = {https://doi.org/10.1038/s42256-020-0218-x},
  abstract = {Bayesian networks can capture causal relations, but learning such a network from data is NP-hard. Recent work has made it possible to approximate this problem as a continuous optimization task that can be solved efficiently with well-established numerical techniques.},
  isbn = {2522-5839},
  keywords = {notion}
}

@inproceedings{luProgramSplicing2018,
  title = {Program {{Splicing}}},
  booktitle = {2018 {{IEEE}}/{{ACM}} 40th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Lu, Yanxin and Chaudhuri, Swarat and Jermaine, Chris and Melski, David},
  date = {2018},
  pages = {338--349},
  doi = {10.1145/3180155.3180190},
  keywords = {notion}
}

@inproceedings{luStatisticalResponseTimeAnalysis2012,
  title = {A {{Statistical Response-Time Analysis}} of {{Real-Time Embedded Systems}}},
  booktitle = {2012 {{IEEE}} 33rd {{Real-Time Systems Symposium}}},
  author = {Lu, Yue and Nolte, Thomas and Bate, Iain and Cucu-Grosjean, Liliana},
  date = {2012},
  pages = {351--362},
  doi = {10.1109/RTSS.2012.85},
  keywords = {notion}
}

@book{lyu1996handbook,
  title = {Handbook of Software Reliability Engineering},
  author = {Lyu, Michael R and others},
  date = {1996},
  volume = {222},
  publisher = {IEEE computer society press Los Alamitos},
  keywords = {notion}
}

@inproceedings{lyuDetectingBuildDependency2024,
  title = {Detecting {{Build Dependency Errors}} in {{Incremental Builds}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Lyu, Jun and Li, Shanshan and Zhang, He and Zhang, Yang and Rong, Guoping and Rigger, Manuel},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {1--12},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3652105},
  url = {https://dl.acm.org/doi/10.1145/3650212.3652105},
  urldate = {2024-09-21},
  abstract = {Incremental and parallel builds performed by build tools such as Make are the heart of modern C/C++ software projects. Their correct and efficient execution depends on build scripts. However, build scripts are prone to errors. The most prevalent errors are missing dependencies (MDs) and redundant dependencies (RDs). The state-of-the-art methods for detecting these errors rely on clean builds (i.e., full builds of a subset of software configurations in a clean environment), which is costly and takes up to a few hours for large-scale projects. To address these challenges, we propose a novel approach called EChecker to detect build dependency errors in the context of incremental builds. The core idea of EChecker is to automatically update actual build dependencies by inferring them from C/C++ pre-processor directives and Makefile changes from new commits, which avoids clean builds when possible. EChecker achieves higher efficiency than the methods that rely on clean builds while maintaining effectiveness. We selected 12 representative projects, with their sizes ranging from small to large, with 240 commits (20 commits for each project), based on which we evaluated the effectiveness and efficiency of EChecker. We compared the evaluation results with a state-of-the-art build dependency error detection tool. The evaluation shows that the F-1 score of EChecker improved by 0.18 over the state-of-the-art method. EChecker increases the build dependency error detection efficiency by an average of 85.14 times (with a median of 16.30 times). The results demonstrate that EChecker can support practitioners in detecting build dependency errors efficiently.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/FW7PLAYZ/Lyu et al. - 2024 - Detecting Build Dependency Errors in Incremental Builds.pdf}
}

@inproceedings{lyuMOPTOptimizedMutation2019,
  title = {{{MOPT}}: {{Optimized Mutation Scheduling}} for {{Fuzzers}}},
  booktitle = {28th {{USENIX Security Symposium}} ({{USENIX Security}} 19)},
  author = {Lyu, Chenyang and Ji, Shouling and Zhang, Chao and Li, Yuwei and Lee, Wei-Han and Song, Yu and Beyah, Raheem},
  date = {2019-08},
  pages = {1949--1966},
  publisher = {USENIX Association},
  location = {Santa Clara, CA},
  url = {https://www.usenix.org/conference/usenixsecurity19/presentation/lyu},
  isbn = {978-1-939133-06-9},
  keywords = {notion}
}

@article{maamarFaultLocalizationUsing2017,
  title = {Fault Localization Using Itemset Mining under Constraints},
  author = {Maamar, Mehdi and Lazaar, Nadjib and Loudni, Samir and Lebbah, Yahia},
  date = {2017-06-01},
  journaltitle = {Automated Software Engineering},
  volume = {24},
  number = {2},
  pages = {341--368},
  issn = {1573-7535},
  doi = {10.1007/s10515-015-0189-z},
  url = {https://doi.org/10.1007/s10515-015-0189-z},
  abstract = {We introduce in this paper an itemset mining approach to tackle the fault localization problem, which is one of the most difficult processes in software debugging. We formalize the problem of fault localization as finding the k best patterns satisfying a set of constraints modelling the most suspicious statements. We use a Constraint Programming (CP) approach to model and to solve our itemset based fault localization problem. Our approach consists of two steps: (i) mining top-k suspicious suites of statements; (ii) fault localization by processing top-k patterns. Experiments performed on standard benchmark programs show that our approach enables to propose a more precise localization than a standard approach.},
  keywords = {notion}
}

@inproceedings{maAPIMisuseDetection2024,
  title = {{{API Misuse Detection}} via {{Probabilistic Graphical Model}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Ma, Yunlong and Tian, Wentong and Gao, Xiang and Sun, Hailong and Li, Li},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {88--99},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3652112},
  url = {https://dl.acm.org/doi/10.1145/3650212.3652112},
  urldate = {2024-09-21},
  abstract = {API misuses can cause a range of issues in software development, including program crashes, bugs, and vulnerabilities. Different approaches have been developed to automatically detect API misuses by checking the program against usage rules extracted from extensive codebase or API documents. However, these mined rules may not be precise or complete, leading to high false positive/negative rates. In this paper, we propose a novel solution to this problem by representing the mined API usage rules as a probabilistic graphical model, where each rule's probability value represents its trustworthiness of being correct.   Our approach automatically constructs probabilistic usage rules by mining codebase and documents, and aggregating knowledge from different sources.   Here, the usage rules obtained from the codebase initialize the probabilistic model, while the knowledge from the documents serves as a supplement for adjusting and complementing the probabilities accordingly.   We evaluate our approach on the MuBench benchmark.   Experimental results show that our approach achieves 42.0\% precision and 54.5\% recall, significantly outperforming state-of-the-art approaches.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/6MDTPTYW/Ma et al. - 2024 - API Misuse Detection via Probabilistic Graphical Model.pdf}
}

@article{maCommitAwareMutationTesting2020,
  title = {Commit-{{Aware Mutation Testing}}},
  author = {Ma, Wei and Laurent, Thomas and Ojdanić, Milo\textbackslash vs and Chekam, Thierry Titcheu and Ventresque, Anthony and Papadakis, Mike},
  date = {2020},
  journaltitle = {2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
  pages = {394--405},
  keywords = {notion}
}

@article{maGENERALIZEDSAMPLECOVERAGE1993,
  title = {{{GENERALIZED SAMPLE COVERAGE WITH AN APPLICATION TO CHINESE POEMS}}},
  author = {Ma, M.-C. and Chao, Anne},
  date = {1993},
  journaltitle = {Statistica Sinica},
  volume = {3},
  number = {1},
  eprint = {24304935},
  eprinttype = {jstor},
  pages = {19--34},
  publisher = {Institute of Statistical Science, Academia Sinica},
  issn = {10170405, 19968507},
  url = {http://www.jstor.org/stable/24304935},
  urldate = {2024-08-26},
  abstract = {The sample coverage for a random sample is defined as the sum of the class probabilities of the observed classes in multinomial sampling for which only one class occurs in each independent observation. This study generalizes the concept of sample coverage to the case that multiple possibly dependent classes can occur for each observation. A consistent estimator for the generalized sample coverage and its mean squared error properties are developed. The resulting estimator is shown to be an approximate empirical Bayes estimator. A data set on Chinese poems is given for illustration. Results of a simulation study are reported to show the general performance of the proposed estimator and to suggest that the usual estimator, without considering the dependence among classes, may yield severe bias in some situations.},
  keywords = {notion}
}

@inproceedings{magliacaneAncestralCausalInference2017,
  title = {Ancestral {{Causal Inference}}},
  booktitle = {In {{Proceedings}} of {{Advances}} in {{Neural Information Processing Systems}} 29 ({{NIPS}} 2016)},
  author = {Magliacane, Sara and Claassen, Tom and Mooij, Joris M.},
  date = {2017},
  keywords = {causal discovery,notion}
}

@misc{mahajanXFixAutomatedTool2017,
  title = {{{XFix}}: {{An Automated Tool}} for the {{Repair}} of {{Layout Cross Browser Issues}}},
  author = {Mahajan, S. and Alameer, A. and McMinn, P. S. and Halfond, W. G. J.},
  date = {2017-05},
  journaltitle = {International Symposium on Software Testing and Analysis (ISSTA 2017)},
  url = {http://eprints.whiterose.ac.uk/116992/},
  organization = {ACM},
  keywords = {Alternating Variable Method (AVM),Automated Program Repair,notion,SBSE,Web Application}
}

@article{mahendranUnderstandingDeepImage2014,
  title = {Understanding {{Deep Image Representations}} by {{Inverting Them}}},
  author = {Mahendran, Aravindh and Vedaldi, Andrea},
  date = {2014},
  journaltitle = {CoRR},
  volume = {abs/1412.0035},
  url = {http://arxiv.org/abs/1412.0035},
  keywords = {notion}
}

@inproceedings{mairhoferSearchbasedSoftwareTesting2011,
  title = {Search-Based {{Software Testing}} and {{Test Data Generation}} for a {{Dynamic Programming Language}}},
  booktitle = {Proceedings of the 13th {{Annual Conference}} on {{Genetic}} and {{Evolutionary Computation}}},
  author = {Mairhofer, Stefan and Feldt, Robert and Torkar, Richard},
  date = {2011},
  series = {{{GECCO}} '11},
  pages = {1859--1866},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2001576.2001826},
  url = {http://doi.acm.org/10.1145/2001576.2001826},
  isbn = {978-1-4503-0557-0},
  venue = {Dublin, Ireland},
  keywords = {Dynamic Language,notion,SBST}
}

@article{malacariaAlgebraicFoundationsInformation2011,
  title = {Algebraic {{Foundations}} for {{Information Theoretical}}, {{Probabilistic}} and {{Guessability}} Measures of {{Information Flow}}},
  author = {Malacaria, Pasquale},
  date = {2011},
  journaltitle = {ArXiv},
  volume = {abs/1101.3453},
  url = {https://api.semanticscholar.org/CorpusID:3605083},
  keywords = {notion}
}

@article{maMuDeltaDeltaOrientedMutation2021,
  title = {{{MuDelta}}: {{Delta-Oriented Mutation Testing}} at {{Commit Time}}},
  author = {Ma, Wei and Chekam, Thierry Titcheu and Papadakis, Mike and Harman, Mark},
  date = {2021},
  journaltitle = {2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},
  pages = {897--909},
  keywords = {notion}
}

@inproceedings{manesAnkouGuidingGreyBox2020,
  title = {Ankou: {{Guiding Grey-Box Fuzzing}} towards {{Combinatorial Difference}}},
  booktitle = {Proceedings of the {{ACM}}/{{IEEE}} 42nd {{International Conference}} on {{Software Engineering}}},
  author = {Manès, Valentin J. M. and Kim, Soomin and Cha, Sang Kil},
  date = {2020},
  series = {{{ICSE}} '20},
  pages = {1024--1036},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3377811.3380421},
  url = {https://doi.org/10.1145/3377811.3380421},
  abstract = {Grey-box fuzzing is an evolutionary process, which maintains and evolves a population of test cases with the help of a fitness function. Fitness functions used by current grey-box fuzzers are not informative in that they cannot distinguish different program executions as long as those executions achieve the same coverage. The problem is that current fitness functions only consider a union of data, but not their combination. As such, fuzzers often get stuck in a local optimum during their search. In this paper, we introduce Ankou, the first grey-box fuzzer that recognizes different combinations of execution information, and present several scalability challenges encountered while designing and implementing Ankou. Our experimental results show that Ankou is 1.94× and 8.0× more effective in finding bugs than AFL and Angora, respectively.},
  isbn = {978-1-4503-7121-6},
  venue = {Seoul, South Korea},
  keywords = {fuzz testing,grey-box fuzzing,guided fuzzing,notion,principal component analysis,software testing}
}

@article{manesArtScienceEngineering2019,
  title = {The Art, Science, and Engineering of Fuzzing: {{A}} Survey},
  author = {Manès, Valentin Jean Marie and Han, HyungSeok and Han, Choongwoo and Cha, Sang Kil and Egele, Manuel and Schwartz, Edward J and Woo, Maverick},
  date = {2019},
  journaltitle = {IEEE Transactions on Software Engineering},
  publisher = {IEEE},
  keywords = {notion}
}

@inproceedings{maoSapienzMultiobjectiveAutomated2016,
  title = {Sapienz: {{Multi-objective Automated Testing}} for {{Android Applications}}},
  booktitle = {Proceedings of the 25th {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Mao, Ke and Harman, Mark and Jia, Yue},
  date = {2016},
  series = {{{ISSTA}} 2016},
  pages = {94--105},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2931037.2931054},
  url = {http://doi.acm.org/10.1145/2931037.2931054},
  isbn = {978-1-4503-4390-9},
  venue = {Saarbr\&\#252;cken, Germany},
  keywords = {Android,notion,Sapienz,SBSE,Test Generation}
}

@inproceedings{marcozziTimeCleanYour2018,
  title = {Time to Clean Your Test Objectives},
  booktitle = {40th {{International Conference}} on {{Software Engineering}}, {{May}} 27-3 {{June}} 2018, {{Gothenburg}}, {{Sweden}}},
  author = {Marcozzi, Michaël and Bardin, Sébastien and Kosmatov, Nikolai and Papadakis, Mike and Prevosto, Virgile and Correnson, Loïc},
  date = {2018},
  keywords = {notion}
}

@article{margaritisEfficientMarkovNetwork2009,
  title = {Efficient {{Markov Network Discovery Using Particle Filters}}},
  author = {Margaritis, Dimitris and Brom},
  date = {2009-11},
  journaltitle = {Computational Intelligence},
  volume = {25},
  pages = {367--394},
  doi = {10.1111/j.1467-8640.2009.00347.x},
  keywords = {causal discovery,notion}
}

@inproceedings{margineanAutomatedTransplantationCall2015,
  title = {Automated {{Transplantation}} of {{Call Graph}} and {{Layout Features}} into {{Kate}}},
  booktitle = {Search-{{Based Software Engineering}}},
  author = {Marginean, Alexandru and Barr, Earl T. and Harman, Mark and Jia, Yue},
  editor = {Barros, Márcio and Labiche, Yvan},
  date = {2015},
  pages = {262--268},
  publisher = {Springer International Publishing},
  location = {Cham},
  abstract = {We report the automated transplantation of two features currently missing from Kate: call graph generation and automatic layout for C programs, which have been requested by users on the Kate development forum. Our approach uses a lightweight annotation system with Search Based techniques augmented by static analysis for automated transplantation. The results are promising: on average, our tool requires 101 min of standard desktop machine time to transplant the call graph feature, and 31 min to transplant the layout feature. We repeated each experiment 20 times and validated the resulting transplants using unit, regression and acceptance test suites. In 34 of 40 experiments conducted our search-based autotransplantation tool, \vphantom\{\}\vphantom\{\}\textbackslash backslashmu \vphantom\{\}\vphantom\{\}μ Scalpel, was able to successfully transplant the new functionality, passing all tests.},
  isbn = {978-3-319-22183-0},
  keywords = {notion}
}

@inproceedings{marinescuKATCHHighcoverageTesting2013,
  title = {{{KATCH}}: {{High-coverage Testing}} of {{Software Patches}}},
  booktitle = {Proceedings of the 2013 9th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Marinescu, Paul Dan and Cadar, Cristian},
  date = {2013},
  series = {{{ESEC}}/{{FSE}} 2013},
  pages = {235--245},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2491411.2491438},
  url = {http://doi.acm.org/10.1145/2491411.2491438},
  isbn = {978-1-4503-2237-9},
  venue = {Saint Petersburg, Russia},
  keywords = {KATCH,notion,Software Testing,Symbolic Execution}
}

@article{marques-silvaExplainabilityNotGame2024,
  title = {Explainability Is {{Not}} a {{Game}}},
  author = {Marques-Silva, Joao and Huang, Xuanxiang},
  date = {2024-06},
  journaltitle = {Commun. ACM},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {0001-0782},
  doi = {10.1145/3635301},
  url = {https://doi.org/10.1145/3635301},
  abstract = {Explainable artificial intelligence (XAI) aims to help human decision-makers in understanding complex machine learning (ML) models. One of the hallmarks of XAI are measures of relative feature importance, which are theoretically justified through the use of SHAP scores, that is, the uses of Shapley values in XAI. This article builds on recent work and offers a simple argument for why SHAP scores can provide misleading measures of relative feature importance, by assigning more importance to features that are irrelevant for a prediction, and assigning less importance to features that are relevant for a prediction. The significance of these results is that they effectively challenge the many proposed uses of measures of relative feature importance in a fast-growing range of high-stakes application domains.},
  keywords = {Abductive reasoning,Explainable AI,notion,SHAP scores,Shapley values}
}

@article{martin-lopezOnlineTestingRESTful2022,
  title = {Online {{Testing}} of {{RESTful APIs}}: {{Promises}} and {{Challenges}}},
  author = {Martin-Lopez, Alberto and Segura, Sergio and Ruiz-Cortés, Antonio},
  date = {2022},
  keywords = {notion}
}

@inproceedings{martinezAstorProgramRepair2016,
  title = {Astor: {{A}} Program Repair Library for Java},
  booktitle = {Proceedings of the 25th {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Martinez, Matias and Monperrus, Martin},
  date = {2016},
  pages = {441--444},
  publisher = {ACM},
  keywords = {notion}
}

@article{martinezAutomaticRepairReal2017,
  title = {Automatic Repair of Real Bugs in Java: A Large-Scale Experiment on the Defects4j Dataset},
  author = {Martinez, Matias and Durieux, Thomas and Sommerard, Romain and Xuan, Jifeng and Monperrus, Martin},
  date = {2017-08-01},
  journaltitle = {Empirical Software Engineering},
  volume = {22},
  number = {4},
  pages = {1936--1964},
  issn = {1573-7616},
  doi = {10.1007/s10664-016-9470-4},
  url = {https://doi.org/10.1007/s10664-016-9470-4},
  abstract = {Defects4J is a large, peer-reviewed, structured dataset of real-world Java bugs. Each bug in Defects4J comes with a test suite and at least one failing test case that triggers the bug. In this paper, we report on an experiment to explore the effectiveness of automatic test-suite based repair on Defects4J. The result of our experiment shows that the considered state-of-the-art repair methods can generate patches for 47 out of 224 bugs. However, those patches are only test-suite adequate, which means that they pass the test suite and may potentially be incorrect beyond the test-suite satisfaction correctness criterion. We have manually analyzed 84 different patches to assess their real correctness. In total, 9 real Java bugs can be correctly repaired with test-suite based repair. This analysis shows that test-suite based repair suffers from under-specified bugs, for which trivial or incorrect patches still pass the test suite. With respect to practical applicability, it takes on average 14.8 minutes to find a patch. The experiment was done on a scientific grid, totaling 17.6 days of computation time. All the repair systems and experimental results are publicly available on Github in order to facilitate future research on automatic repair.},
  keywords = {Automated Program Repair,notion}
}

@article{maruSoftwareFaultLocalization2019,
  title = {Software Fault Localization Using {{BP}} Neural Network Based on Function and Branch Coverage},
  author = {Maru, Abha and Dutta, Arpita and Kumar, K. Vinod and Mohapatra, Durga Prasad},
  date = {2019},
  journaltitle = {Evolutionary Intelligence},
  doi = {10.1007/s12065-019-00318-2},
  url = {https://doi.org/10.1007/s12065-019-00318-2},
  abstract = {Software failure is inevitable with the increase in scale and complexity of the software. Existing fault localization techniques based on neural networks take statement coverage information and test case execution results into account to train the network. In this paper, we propose an effective approach for fault localization based on back-propagation neural network which utilizes branch and function coverage information along with test case execution results to train the network. We investigated our approach using Siemens suite. Our experimental result shows that our proposed approach performs on average 23.50–44.27\% better than existing fault localization techniques.},
  isbn = {1864-5917},
  keywords = {notion}
}

@inproceedings{masriCleansingTestSuites2010,
  title = {Cleansing Test Suites from Coincidental Correctness to Enhance Fault-Localization},
  booktitle = {2010 Third International Conference on Software Testing, Verification and Validation},
  author = {Masri, Wes and Abou Assi, Rawad},
  date = {2010},
  pages = {165--174},
  publisher = {IEEE},
  keywords = {notion}
}

@inproceedings{masriEmpiricalStudyFactors2009,
  title = {An Empirical Study of the Factors That Reduce the Effectiveness of Coverage-Based Fault Localization},
  booktitle = {Proceedings of the 2nd {{International Workshop}} on {{Defects}} in {{Large Software Systems}}: Held in Conjunction with the {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}} ({{ISSTA}} 2009)},
  author = {Masri, Wes and Abou-Assi, Rawad and El-Ghali, Marwa and Al-Fatairi, Nour},
  date = {2009},
  pages = {1--5},
  keywords = {notion}
}

@inproceedings{masriEnhancingFaultLocalization2012,
  title = {Enhancing Fault Localization via Multivariate Visualization},
  booktitle = {2012 {{IEEE Fifth International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}}},
  author = {Masri, Wes and Abou Assi, Rawad and Zaraket, Fadi and Fatairi, Nour},
  date = {2012},
  pages = {737--741},
  publisher = {IEEE},
  keywords = {notion}
}

@article{masriMeasuringStrengthInformation2009,
  title = {Measuring the {{Strength}} of {{Information Flows}} in {{Programs}}},
  author = {Masri, Wes and Podgurski, Andy},
  date = {2009-10},
  journaltitle = {ACM Trans. Softw. Eng. Methodol.},
  volume = {19},
  number = {2},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {1049-331X},
  doi = {10.1145/1571629.1571631},
  url = {https://doi.org/10.1145/1571629.1571631},
  abstract = {Dynamic information flow analysis (DIFA) was devised to enable the flow of information among variables in an executing program to be monitored and possibly regulated. It is related to techniques like dynamic slicing and dynamic impact analysis. To better understand the basis for DIFA, we conducted an empirical study in which we measured the strength of information flows identified by DIFA, using information theoretic and correlation-based methods. The results indicate that in most cases the occurrence of a chain of dynamic program dependences between two variables does not indicate a measurable information flow between them. We also explored the relationship between the strength of an information flow and the length of the corresponding dependence chain, and we obtained results indicating that no consistent relationship exists between the length of an information flow and its strength. Finally, we investigated whether data dependence and control dependence makes equal or unequal contributions to flow strength. The results indicate that flows due to data dependences alone are stronger, on average, than flows due to control dependences alone. We present the details of our study and consider the implications of the results for applications of DIFA and related techniques.},
  keywords = {correlation,Dynamic information flow analysis,dynamic slicing,entropy,information flow length,information flow strength,information leakage,notion,program dependence}
}

@article{masriPrevalenceCoincidentalCorrectness2014a,
  title = {Prevalence of {{Coincidental Correctness}} and {{Mitigation}} of {{Its Impact}} on {{Fault Localization}}},
  author = {Masri, Wes and Assi, Rawad Abou},
  date = {2014-02},
  journaltitle = {ACM Trans. Softw. Eng. Methodol.},
  volume = {23},
  number = {1},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {1049-331X},
  doi = {10.1145/2559932},
  url = {https://doi.org/10.1145/2559932},
  abstract = {Researchers have argued that for failure to be observed the following three conditions must be met: CR = the defect was reached; CI = the program has transitioned into an infectious state; and CP = the infection has propagated to the output. Coincidental Correctness (CC) arises when the program produces the correct output while condition CR is met but not CP. We recognize two forms of coincidental correctness, weak and strong. In weak CC, CR is met, whereas CI might or might not be met, whereas in strong CC, both CR and CI are met. In this work we first show that CC is prevalent in both of its forms and demonstrate that it is a safety reducing factor for Coverage-Based Fault Localization (CBFL). We then propose two techniques for cleansing test suites from coincidental correctness to enhance CBFL, given that the test cases have already been classified as failing or passing. We evaluated the effectiveness of our techniques by empirically quantifying their accuracy in identifying weak CC tests. The results were promising, for example, the better performing technique, using 105 test suites and statement coverage, exhibited 9\% false negatives, 30\% false positives, and no false negatives nor false positives in 14.3\% of the test suites. Also using 73 test suites and more complex coverage, the numbers were 12\%, 19\%, and 15\%, respectively.},
  keywords = {cluster analysis,Coincidental correctness,coverage-based fault localization,fuzzy sets,notion,strong coincidental correctness,weak coincidental correctness}
}

@article{masudSemanticCorrectnessDependenceBased2021,
  title = {Semantic {{Correctness}} of {{Dependence-Based Slicing}} for {{Interprocedural}}, {{Possibly Nonterminating Programs}}},
  author = {Masud, Abu Naser and Lisper, Björn},
  date = {2021-01},
  journaltitle = {ACM Trans. Program. Lang. Syst.},
  volume = {42},
  number = {4},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {0164-0925},
  doi = {10.1145/3434489},
  url = {https://doi.org/10.1145/3434489},
  abstract = {Existing proofs of correctness for dependence-based slicing methods are limited either to the slicing of intraprocedural programs [2, 39], or the proof is only applicable to a specific slicing method [4, 41]. We contribute a general proof of correctness for dependence-based slicing methods such as Weiser [50, 51], or Binkley et\&nbsp;al. [7, 8], for interprocedural, possibly nonterminating programs. The proof uses well-formed weak and strong control closure relations, which are the interprocedural extensions of the generalised weak/strong control closure provided by Danicic et al. [13], capturing various nonterminating-insensitive and nontermination-sensitive control-dependence relations that have been proposed in the literature. Thus, our proof framework is valid for a whole range of existing control-dependence relations.We have provided a definition of semantically correct (SC) slice. We prove that SC slices agree with Weiser slicing, that deterministic SC slices preserve termination, and that nondeterministic SC slices preserve the nondeterministic behavior of the original programs.},
  keywords = {bisimulation,Correctness of slicing,interprocedural program,nondeterminism,nontermination,notion,semi-equivalence,simulation,static slicing}
}

@article{mathisDetectingInformationFlow2017,
  title = {Detecting Information Flow by Mutating Input Data},
  author = {Mathis, Björn and Avdiienko, Vitalii and Soremekun, Ezekiel O. and Böhme, Marcel and Zeller, Andreas},
  date = {2017},
  journaltitle = {2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  pages = {263--273},
  keywords = {notion}
}

@inproceedings{mathisLearningInputTokens2020,
  title = {Learning {{Input Tokens}} for {{Effective Fuzzing}}},
  booktitle = {Proceedings of the 29th {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Mathis, Björn and Gopinath, Rahul and Zeller, Andreas},
  date = {2020},
  series = {{{ISSTA}} 2020},
  pages = {27--37},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3395363.3397348},
  url = {https://doi.org/10.1145/3395363.3397348},
  abstract = {Modern fuzzing tools like AFL operate at a lexical level: They explore the input space of tested programs one byte after another. For inputs with complex syntactical properties, this is very inefficient, as keywords and other tokens have to be composed one character at a time. Fuzzers thus allow to specify dictionaries listing possible tokens the input can be composed from; such dictionaries speed up fuzzers dramatically. Also, fuzzers make use of dynamic tainting to track input tokens and infer values that are expected in the input validation phase. Unfortunately, such tokens are usually implicitly converted to program specific values which causes a loss of the taints attached to the input data in the lexical phase. In this paper, we present a technique to extend dynamic tainting to not only track explicit data flows but also taint implicitly converted data without suffering from taint explosion. This extension makes it possible to augment existing techniques and automatically infer a set of tokens and seed inputs for the input language of a program given nothing but the source code. Specifically targeting the lexical analysis of an input processor, our lFuzzer test generator systematically explores branches of the lexical analysis, producing a set of tokens that fully cover all decisions seen. The resulting set of tokens can be directly used as a dictionary for fuzzing. Along with the token extraction seed inputs are generated which give further fuzzing processes a head start. In our experiments, the lFuzzer-AFL combination achieves up to 17\% more coverage on complex input formats like json, lisp, tinyC, and JavaScript compared to AFL.},
  isbn = {978-1-4503-8008-9},
  venue = {Virtual Event, USA},
  keywords = {fuzzing,notion,parser,test input generation}
}

@inproceedings{mathisParserDirectedFuzzing2019,
  title = {Parser-{{Directed Fuzzing}}},
  booktitle = {Proceedings of the 40th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Mathis, Björn and Gopinath, Rahul and Mera, Michaël and Kampmann, Alexander and Höschele, Matthias and Zeller, Andreas},
  date = {2019},
  series = {{{PLDI}} 2019},
  pages = {548--560},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3314221.3314651},
  url = {https://doi.org/10.1145/3314221.3314651},
  abstract = {To be effective, software test generation needs to well cover the space of possible inputs. Traditional fuzzing generates large numbers of random inputs, which however are unlikely to contain keywords and other specific inputs of non-trivial input languages. Constraint-based test generation solves conditions of paths leading to uncovered code, but fails on programs with complex input conditions because of path explosion. In this paper, we present a test generation technique specifically directed at input parsers. We systematically produce inputs for the parser and track comparisons made; after every rejection, we satisfy the comparisons leading to rejection. This approach effectively covers the input space: Evaluated on five subjects, from CSV files to JavaScript, our pFuzzer prototype covers more tokens than both random-based and constraint-based approaches, while requiring no symbolic analysis and far fewer tests than random fuzzers.},
  isbn = {978-1-4503-6712-7},
  venue = {Phoenix, AZ, USA},
  keywords = {fuzzing,notion,parsers,security,test generation}
}

@article{mayerCrossLanguageCodeAnalysis2012,
  title = {Cross-{{Language Code Analysis}} and {{Refactoring}}},
  author = {Mayer, Philip and Schroeder, Andreas},
  date = {2012},
  journaltitle = {2012 IEEE 12th International Working Conference on Source Code Analysis and Manipulation},
  pages = {94--103},
  keywords = {notion}
}

@inproceedings{mayerEmpiricalAnalysisUtilization2015,
  title = {An {{Empirical Analysis}} of the {{Utilization}} of {{Multiple Programming Languages}} in {{Open Source Projects}}},
  booktitle = {Proceedings of the 19th {{International Conference}} on {{Evaluation}} and {{Assessment}} in {{Software Engineering}}},
  author = {Mayer, Philip and Bauer, Alexander},
  date = {2015},
  series = {{{EASE}} '15},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2745802.2745805},
  url = {https://doi.org/10.1145/2745802.2745805},
  abstract = {Background: Anecdotal evidence suggests that software applications are usually implemented using a combination of (programming) languages. Aim: We want to provide empirical evidence on the phenomenon of multi-language programming. Methods: We use data mining of 1150 open source projects selected for diversity from a public repository to a) investigate the projects for number and type of languages found and the relative sizes of the languages; b) report on associations between the number of languages found and the size, age, number of contributors, and number of commits of a project using a (Quasi-)Poisson regression model, and c) discuss concrete associations between the general-purpose languages and domain-specific languages found using frequent item set mining. Results: We found a) a mean number of 5 languages per project with a clearly dominant main general-purpose language and 5 often-used DSL types, b) a significant influence of the size, number of commits, and the main language on the number of languages as well as no significant influence of age and number of contributors, and c) three language ecosystems grouped around XML, Shell/Make, and HTML/CSS. Conclusions: Multi-language programming seems to be common in open-source projects and is a factor which must be dealt with in tooling and when assessing development and maintenance of such software systems.},
  isbn = {978-1-4503-3350-4},
  venue = {Nanjing, China},
  keywords = {notion}
}

@book{mazhdrakovMonteCarloMethod2018,
  title = {The {{Monte Carlo}} Method: Engineering Applications},
  author = {Mazhdrakov, Metodi and Benov, Dobriyan and Valkanov, Nikolai},
  date = {2018},
  publisher = {ACMO Academic Press},
  isbn = {978-619-90684-3-4},
  keywords = {notion}
}

@inproceedings{mazouniPolicyTestingMDPFuzz2024,
  title = {Policy {{Testing}} with {{MDPFuzz}} ({{Replicability Study}})},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Mazouni, Quentin and Spieker, Helge and Gotlieb, Arnaud and Acher, Mathieu},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {1567--1578},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3680382},
  url = {https://dl.acm.org/doi/10.1145/3650212.3680382},
  urldate = {2024-09-21},
  abstract = {In recent years, following tremendous achievements in Reinforcement Learning, a great deal of interest has been devoted to ML models for sequential decision-making. Together with these scientific breakthroughs/advances, research has been conducted to develop automated functional testing methods for finding faults in black-box Markov decision processes. Pang et al. (ISSTA 2022) presented a black-box fuzz testing framework called MDPFuzz. The method consists of a fuzzer whose main feature is to use Gaussian Mixture Models (GMMs) to compute coverage of the test inputs as the likelihood to have already observed their results. This guidance through coverage evaluation aims at favoring novelty during testing and fault discovery in the decision model.       Pang et al. evaluated their work with four use cases, by comparing the number of failures found after twelve-hour testing campaigns with or without the guidance of the GMMs (ablation study). In this paper, we verify some of the key findings of the original paper and explore the limits of MDPFuzz through reproduction and replication. We re-implemented the proposed methodology and evaluated our replication in a large-scale study that extends the original four use cases with three new ones. Furthermore, we compare MDPFuzz and its ablated counterpart with a random testing baseline. We also assess the effectiveness of coverage guidance for different parameters, something that has not been done in the original evaluation. Despite this parameter analysis and unlike Pang et al.’s original conclusions, we find that in most cases, the aforementioned ablated Fuzzer outperforms MDPFuzz, and conclude that the coverage model proposed does not lead to finding more faults.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/UIUIZFR9/Mazouni et al. - 2024 - Policy Testing with MDPFuzz (Replicability Study).pdf}
}

@inproceedings{mcallesterConvergenceRateGoodTuring2000,
  title = {On the {{Convergence Rate}} of {{Good-Turing Estimators}}},
  booktitle = {Annual {{Conference Computational Learning Theory}}},
  author = {McAllester, David A. and Schapire, Robert E.},
  date = {2000},
  keywords = {notion}
}

@inproceedings{mccamantQuantitativeInformationFlow2008,
  title = {Quantitative Information Flow as Network Flow Capacity},
  booktitle = {Proceedings of the 29th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {McCamant, Stephen and Ernst, Michael D},
  date = {2008},
  pages = {193--205},
  keywords = {notion}
}

@incollection{mccloskeyCatastrophicInterferenceConnectionist1989,
  title = {Catastrophic {{Interference}} in {{Connectionist Networks}}: {{The Sequential Learning Problem}}},
  author = {McCloskey, Michael and Cohen, Neal J.},
  editor = {Bower, Gordon H.},
  date = {1989},
  series = {Psychology of {{Learning}} and {{Motivation}}},
  volume = {24},
  number = {Supplement C},
  pages = {109--165},
  publisher = {Academic Press},
  issn = {0079-7421},
  doi = {10.1016/S0079-7421(08)60536-8},
  url = {http://www.sciencedirect.com/science/article/pii/S0079742108605368},
  abstract = {Publisher Summary Connectionist networks in which information is stored in weights on connections among simple processing units have attracted considerable interest in cognitive science. Much of the interest centers around two characteristics of these networks. First, the weights on connections between units need not be prewired by the model builder but rather may be established through training in which items to be learned are presented repeatedly to the network and the connection weights are adjusted in small increments according to a learning algorithm. Second, the networks may represent information in a distributed fashion. This chapter discusses the catastrophic interference in connectionist networks. Distributed representations established through the application of learning algorithms have several properties that are claimed to be desirable from the standpoint of modeling human cognition. These properties include content-addressable memory and so-called automatic generalization in which a network trained on a set of items responds correctly to other untrained items within the same domain. New learning may interfere catastrophically with old learning when networks are trained sequentially. The analysis of the causes of interference implies that at least some interference will occur whenever new learning may alter weights involved in representing old learning, and the simulation results demonstrate only that interference is catastrophic in some specific networks.},
  keywords = {notion}
}

@misc{mcminnAVMfOpenSourceFramework2016,
  title = {{{AVMf}}: {{An Open-Source Framework}} and {{Implementation}} of the {{Alternating Variable Method}}},
  author = {McMinn, P. S. and Kapfhammer, G. M.},
  date = {2016-09},
  journaltitle = {International Symposium on Search-Based Software Engineering (SSBSE 2016)},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {9962},
  url = {http://eprints.whiterose.ac.uk/104204/},
  abstract = {The Alternating Variable Method (AVM) has been shown to be a fast and effective local search technique for search-based software engineering. Recent improvements to the AVM have generalized the representations it can optimize and have provably reduced its running time. However, until now, there has been no general, publicly-available implementation of the AVM incorporating all of these developments. We introduce AVMf, an object-oriented Java framework that provides such an implementation. AVMf is available from http://avmframework.org for configuration and use in a wide variety of projects.},
  organization = {Springer International Publishing},
  keywords = {Alternating Variable Method (AVM),notion}
}

@inproceedings{mechtaevAngelixScalableMultiline2016,
  title = {Angelix: {{Scalable Multiline Program Patch Synthesis}} via {{Symbolic Analysis}}},
  booktitle = {2016 {{IEEE}}/{{ACM}} 38th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Mechtaev, S. and Yi, J. and Roychoudhury, A.},
  date = {2016-05},
  pages = {691--701},
  doi = {10.1145/2884781.2884807},
  abstract = {Since debugging is a time-consuming activity, automated program repair tools such as GenProg have garnered interest. A recent study revealed that the majority of GenProg repairs avoid bugs simply by deleting functionality. We found that SPR, a state-of-the-art repair tool proposed in 2015, still deletes functionality in their many "plausible" repairs. Unlike generate-and-validate systems such as GenProg and SPR, semantic analysis based repair techniques synthesize a repair based on semantic information of the program. While such semantics-based repair methods show promise in terms of quality of generated repairs, their scalability has been a concern so far. In this paper, we present Angelix, a novel semantics-based repair method that scales up to programs of similar size as are handled by search-based repair tools such as GenProg and SPR. This shows that Angelix is more scalable than previously proposed semantics based repair methods such as SemFix and DirectFix. Furthermore, our repair method can repair multiple buggy locations that are dependent on each other. Such repairs are hard to achieve using SPR and GenProg. In our experiments, Angelix generated repairs from large-scale real-world software such as wireshark and php, and these generated repairs include multi-location repairs. We also report our experience in automatically repairing the well-known Heartbleed vulnerability.},
  keywords = {Angelix method,Automated program repair,Computer bugs,generate-and-validate systems,Heartbleed vulnerability,Maintenance engineering,multilocation repairs,notion,program debugging,program diagnostics,program repair tools,Scalability,scalable multiline program patch synthesis,semantic analysis,Semantic analysis,semantic information,Semantics,Software,Software engineering,SPR tool,symbolic analysis,Testing}
}

@article{memonTamingGoogleScaleContinuous2017,
  title = {Taming {{Google-Scale Continuous Testing}}},
  author = {Memon, Atif M. and Gao, Zebao and Nguyen, Bao-Ngoc and Dhanda, Sanjeev and Nickell, Eric and Siemborski, Rob and Micco, John},
  date = {2017},
  journaltitle = {2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track (ICSE-SEIP)},
  pages = {233--242},
  keywords = {notion}
}

@inproceedings{mengCalculatingBoundsInformation2011,
  title = {Calculating Bounds on Information Leakage Using Two-Bit Patterns},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 6th {{Workshop}} on {{Programming Languages}} and {{Analysis}} for {{Security}}},
  author = {Meng, Ziyuan and Smith, Geoffrey},
  date = {2011},
  pages = {1--12},
  keywords = {notion}
}

@misc{mesecanHyperGIAutomatedDetection2021,
  title = {{{HyperGI}}: {{Automated Detection}} and {{Repair}} of {{Information Flow Leakage}}},
  author = {Mesecan, Ibrahim and Blackwell, Daniel and Clark, David and Cohen, Myra B. and Petke, Justyna},
  date = {2021},
  keywords = {notion}
}

@inproceedings{messaoudiLogBasedSlicingSystemLevel2021,
  title = {Log-{{Based Slicing}} for {{System-Level Test Cases}}},
  booktitle = {Proceedings of the 30th {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Messaoudi, Salma and Shin, Donghwan and Panichella, Annibale and Bianculli, Domenico and Briand, Lionel C.},
  date = {2021},
  series = {{{ISSTA}} 2021},
  pages = {517--528},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3460319.3464824},
  url = {https://doi.org/10.1145/3460319.3464824},
  abstract = {Regression testing is arguably one of the most important activities in software testing. However, its cost-effectiveness and usefulness can be largely impaired by complex system test cases that are poorly designed (e.g., test cases containing multiple test scenarios combined into a single test case) and that require a large amount of time and resources to run. One way to mitigate this issue is decomposing such system test cases into smaller, separate test cases—each of them with only one test scenario and with its corresponding assertions—so that the execution time of the decomposed test cases is lower than the original test cases, while the test effectiveness of the original test cases is preserved. This decomposition can be achieved with program slicing techniques, since test cases are software programs too. However, existing static and dynamic slicing techniques exhibit limitations when (1) the test cases use external resources, (2) code instrumentation is not a viable option, and (3) test execution is expensive. In this paper, we propose a novel approach, called DS3 (Decomposing System teSt caSe), which automatically decomposes a complex system test case into separate test case slices. The idea is to use test case execution logs, obtained from past regression testing sessions, to identify "hidden" dependencies in the slices generated by static slicing. Since logs include run-time information about the system under test, we can use them to extract access and usage of global resources and refine the slices generated by static slicing. We evaluated DS3 in terms of slicing effectiveness and compared it with a vanilla static slicing tool. We also compared the slices obtained by DS3 with the corresponding original system test cases, in terms of test efficiency and effectiveness. The evaluation results on one proprietary system and one open-source system show that DS3 is able to accurately identify the dependencies related to the usage of global resources, which vanilla static slicing misses. Moreover, the generated test case slices are, on average, 3.56 times faster than original system test cases and they exhibit no significant loss in terms of fault detection effectiveness.},
  isbn = {978-1-4503-8459-9},
  venue = {Virtual, Denmark},
  keywords = {log,notion,program slicing,system level testing}
}

@article{miaoClusteringbasedStrategyIdentify2013,
  title = {A Clustering-Based Strategy to Identify Coincidental Correctness in Fault Localization},
  author = {Miao, Yi and Chen, Zhenyu and Li, Sihan and Zhao, Zhihong and Zhou, Yuming},
  date = {2013},
  journaltitle = {International Journal of Software Engineering and Knowledge Engineering},
  volume = {23},
  number = {05},
  pages = {721--741},
  publisher = {World Scientific},
  keywords = {notion}
}

@inproceedings{miaoIdentifyingCoincidentalCorrectness2012,
  title = {Identifying {{Coincidental Correctness}} for {{Fault Localization}} by {{Clustering Test Cases}}.},
  booktitle = {{{SEKE}}},
  author = {Miao, Yi and Chen, Zhenyu and Li, Sihan and Zhao, Zhihong and Zhou, Yuming},
  date = {2012},
  pages = {267--272},
  keywords = {notion}
}

@article{miller1992estimating,
  title = {Estimating the Probability of Failure When Testing Reveals No Failures},
  author = {Miller, Keith W. and Morell, Larry J. and Noonan, Robert E. and Park, Stephen K. and Nicol, David M. and Murrill, Branson W. and Voas, M},
  date = {1992},
  journaltitle = {IEEE transactions on Software Engineering},
  volume = {18},
  number = {1},
  pages = {33},
  publisher = {IEEE Computer Society},
  keywords = {notion}
}

@article{millerEmpiricalStudyReliability1990,
  title = {An Empirical Study of the Reliability of {{UNIX}} Utilities},
  author = {Miller, Barton P. and Fredriksen, Lars and So, Bryan},
  date = {1990},
  journaltitle = {Communications of the ACM},
  volume = {33},
  pages = {32--44},
  keywords = {notion}
}

@article{millerNoteBiasInformation1955,
  title = {Note on the Bias of Information Estimates},
  author = {Miller, George},
  date = {1955},
  journaltitle = {Information theory in psychology: Problems and methods},
  publisher = {Free Press},
  keywords = {notion}
}

@article{millerUnderstandingResponseOpenSource,
  title = {Understanding the {{Response}} to {{Open-Source Dependency Abandonment}} in the Npm {{Ecosystem}}},
  author = {Miller, Courtney and Jahanshahi, Mahmoud and Mockus, Audris and Vasilescu, Bogdan and Kastner, Christian},
  abstract = {Many developers relying on open-source digital infrastructure expect continuous maintenance, but even the most critical packages can become unmaintained. Despite this, there is little understanding of the prevalence of abandonment of widely-used packages, of subsequent exposure, and of reactions to abandonment in practice, or the factors that influence them. We perform a large-scale quantitative analysis of all widely-used npm packages and find that abandonment is common among them, that abandonment exposes many projects which often do not respond, that responses correlate with other dependency management practices, and that removal is significantly faster when a package’s end-of-life status is explicitly stated. We end with recommendations to both researchers and practitioners who are facing dependency abandonment or are sunsetting packages, such as opportunities for low-effort transparency mechanisms to help exposed projects make better, more informed decisions.},
  langid = {english},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/J87NI67A/Miller et al. - Understanding the Response to Open-Source Dependency Abandonment in the npm Ecosystem.pdf}
}

@misc{miltnerBottomSynthesisRecursiveFunctional2021,
  title = {Bottom-up {{Synthesis}} of {{Recursive Functional Programs}} Using {{Angelic Execution}}},
  author = {Miltner, Anders and Nuñez, Adrian Trejo and Brendel, Ana and Chaudhuri, Swarat and Dillig, Isil},
  date = {2021},
  keywords = {notion}
}

@inproceedings{milutinovicUsesExtremeValue2017,
  title = {On Uses of Extreme Value Theory Fit for Industrial-Quality {{WCET}} Analysis},
  booktitle = {2017 12th {{IEEE International Symposium}} on {{Industrial Embedded Systems}} ({{SIES}})},
  author = {Milutinovic, Suzana and Mezzetti, Enrico and Abella, Jaume and Vardanega, Tullio and Cazorla, Francisco J.},
  date = {2017},
  pages = {1--6},
  doi = {10.1109/SIES.2017.7993402},
  keywords = {notion}
}

@inproceedings{minelliKnowWhatYou2015,
  title = {I Know What You Did Last Summer-an Investigation of How Developers Spend Their Time},
  booktitle = {2015 {{IEEE}} 23rd {{International Conference}} on {{Program Comprehension}}},
  author = {Minelli, Roberto and Mocci, Andrea and Lanza, Michele},
  date = {2015},
  pages = {25--35},
  publisher = {IEEE},
  keywords = {notion}
}

@inproceedings{misherghiHDDHierarchicalDelta2006,
  title = {{{HDD}}: {{Hierarchical Delta Debugging}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Software Engineering}}},
  author = {Misherghi, Ghassan and Su, Zhendong},
  date = {2006},
  series = {{{ICSE}} '06},
  pages = {142--151},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/1134285.1134307},
  url = {http://doi.acm.org/10.1145/1134285.1134307},
  isbn = {1-59593-375-1},
  venue = {Shanghai, China},
  keywords = {Delta Debugging,Hierarchical Delta Debugging (HDD),notion}
}

@book{mitchellIntroductionGeneticAlgorithms1998,
  title = {An Introduction to Genetic Algorithms},
  author = {Mitchell, Melanie},
  date = {1998},
  publisher = {MIT press},
  keywords = {notion}
}

@article{mnihHumanlevelControlDeep2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  date = {2015},
  journaltitle = {Nature},
  volume = {518},
  number = {7540},
  pages = {529},
  publisher = {Nature Publishing Group},
  keywords = {notion}
}

@article{mnihPlayingAtariDeep2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin A.},
  date = {2013},
  journaltitle = {CoRR},
  volume = {abs/1312.5602},
  url = {http://arxiv.org/abs/1312.5602},
  keywords = {Atari,notion,Reinforcement Learning}
}

@article{moddemeijerEstimationEntropyMutual1989,
  title = {On Estimation of Entropy and Mutual Information of Continuous Distributions},
  author = {Moddemeijer, R.},
  date = {1989},
  journaltitle = {Signal Processing},
  volume = {16},
  number = {3},
  pages = {233--248},
  issn = {0165-1684},
  doi = {10.1016/0165-1684(89)90132-1},
  url = {https://www.sciencedirect.com/science/article/pii/0165168489901321},
  abstract = {Mutual information is used in a procedure to estimate time-delays between recordings of electroencephalogram (EEG) signals originating from epileptic animals and patients. We present a simple and reliable histogram-based method to estimate mutual information. The accuracies of this mutual information estimator and of a similar entropy estimator are discussed. The bias and variance calculations presented can also be applied to discrete valued systems. Finally, we present some simulation results, which are compared with earlier work. Zusammenfassung Die Auswertung von Elektroenzephalogrammen bei Epilepsie-Kranken erfordert die Bestimmung von Totzeiten zwischen den einzelnen Aufzeichnungen. Hierzu sind Verfahren geeignet, die am informationstheoretischen Begriff “mittlere Transinformation” anknüpfen. In diesem Beitrag wird eine einfache und zuverlässige Methode beschrieben, die mittlere Transinformation auf der Basis experimenteller Daten zu schätzen. Diskutiert werden Erwartungstreue und Varianz der vorgeschlagenen Schätzfunktionen. Die theoretisch erzielten Ergebnisse werden mit experimentellen Daten verglichen, die an Signalfolgen mit Gauβ-verteilten Amplituden gewonnen wurden. Résumé L'information mutuelle est utilisée dans une procédure pour estimer les retards temporels entre les enregistrements de signaux electroencéphalographique (EEG) provenant des animaux et patients épileptiques. Nous présentons une méthode simple et fiable basée sur l'histogramme pour estimer l'information mutuelle. La précision de cet estimateur d'information mutuelle et celle estimateurs d'entropie sont discutées. Les calculs de biais et de variance présentés peuvent également être appliqués aux systèmes à valeurs discrètes. Finalement, nous présentons quelques résultats de simulations qui sont comparés aux travaux antérieurs.},
  keywords = {bias,delay estimation,entropy,estimator,Mutual information,notion,variance}
}

@inproceedings{monperrusCriticalReviewAutomatic2014,
  title = {A {{Critical Review}} of "{{Automatic Patch Generation Learned}} from {{Human-written Patches}}": {{Essay}} on the {{Problem Statement}} and the {{Evaluation}} of {{Automatic Software Repair}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Software Engineering}}},
  author = {Monperrus, Martin},
  date = {2014},
  series = {{{ICSE}} 2014},
  pages = {234--242},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2568225.2568324},
  url = {http://doi.acm.org/10.1145/2568225.2568324},
  isbn = {978-1-4503-2756-5},
  venue = {Hyderabad, India},
  keywords = {automatic patch generation,automatic program fixing,automatic software repair,Bugs,error recovery,faults,notion}
}

@inproceedings{moraglioGeometricSemanticGenetic2012,
  title = {Geometric {{Semantic Genetic Programming}}},
  booktitle = {Parallel {{Problem Solving}} from {{Nature}} - {{PPSN XII}}},
  author = {Moraglio, Alberto and Krawiec, Krzysztof and Johnson, Colin G.},
  editor = {Coello, Carlos A. Coello and Cutello, Vincenzo and Deb, Kalyanmoy and Forrest, Stephanie and Nicosia, Giuseppe and Pavone, Mario},
  date = {2012},
  pages = {21--31},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  abstract = {Traditional Genetic Programming (GP) searches the space of functions/programs by using search operators that manipulate their syntactic representation, regardless of their actual semantics/behaviour. Recently, semantically aware search operators have been shown to outperform purely syntactic operators. In this work, using a formal geometric view on search operators and representations, we bring the semantic approach to its extreme consequences and introduce a novel form of GP – Geometric Semantic GP (GSGP) – that searches directly the space of the underlying semantics of the programs. This perspective provides new insights on the relation between program syntax and semantics, search operators and fitness landscape, and allows for principled formal design of semantic search operators for different classes of problems. We derive specific forms of GSGP for a number of classic GP domains and experimentally demonstrate their superiority to conventional operators.},
  isbn = {978-3-642-32937-1},
  keywords = {notion}
}

@article{moravcikDeepStackExpertLevelArtificial2017,
  title = {{{DeepStack}}: {{Expert-Level Artificial Intelligence}} in {{No-Limit Poker}}},
  author = {Moravcík, Matej and Schmid, Martin and Burch, Neil and Lisý, Viliam and Morrill, Dustin and Bard, Nolan and Davis, Trevor and Waugh, Kevin and Johanson, Michael and Bowling, Michael H.},
  date = {2017},
  journaltitle = {CoRR},
  volume = {abs/1701.01724},
  url = {http://arxiv.org/abs/1701.01724},
  keywords = {Artificial Intelligence (AI),DeepStack,notion}
}

@inproceedings{morenoUseStackTraces2014,
  title = {On the {{Use}} of {{Stack Traces}} to {{Improve Text Retrieval-Based Bug Localization}}},
  booktitle = {2014 {{IEEE International Conference}} on {{Software Maintenance}} and {{Evolution}}},
  author = {Moreno, L. and Treadway, J. J. and Marcus, A. and Shen, W.},
  date = {2014-09},
  pages = {151--160},
  issn = {1063-6773},
  doi = {10.1109/ICSME.2014.37},
  abstract = {Many bug localization techniques rely on Text Retrieval (TR) models. The most successful approaches have been proven to be the ones combining TR techniques with static analysis, dynamic analysis, and/or software repositories information. Dynamic software analysis and software repositories mining bring a significant overhead, as they require instrumenting and executing the software, and analyzing large amounts of data, respectively. We propose a new static technique, named Lobster (Locating Bugs using Stack Traces and text Retrieval), which is meant to improve TR-based bug localization without the overhead associated with dynamic analysis and repository mining. Specifically, we use the stack traces submitted in a bug report to compute the similarity between their code elements and the source code of a software system. We combine the stack trace based similarity and the textual similarity provided by TR techniques to retrieve code elements relevant to bug reports. We empirically evaluated Lobster using 155 bug reports containing stack traces from 14 open source software systems. We used Lucene, an optimized version of VSM, as baseline of comparison. The results show that, in average, Lobster improves or maintains the effectiveness of Lucene-based bug localization in 82\% of the cases.},
  keywords = {bug localization,bug localization techniques,Conferences,dynamic analysis,improve text retrieval,information retrieval,notion,program debugging,software analysis,Software maintenance,software repositories information,software repositories mining,software system,source code,stack traces,static analysis,text analysis,text retrieval,textual similarity,TR models,TR techniques}
}

@book{morganCounterfactualsCausalInference2015,
  title = {Counterfactuals and {{Causal Inference}}: {{Methods}} and {{Principles}} for {{Social Research}}},
  author = {Morgan, Stephen L. and Winship, Christopher},
  date = {2015},
  edition = {2},
  location = {Cambridge},
  keywords = {notion}
}

@article{motwaniQualityAutomatedProgram2022,
  title = {Quality of {{Automated Program Repair}} on {{Real-World Defects}}},
  author = {Motwani, Manish and Soto, Mauricio and Brun, Yuriy and Just, René and Le Goues, Claire},
  date = {2022-02},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {48},
  number = {2},
  pages = {637--661},
  issn = {1939-3520},
  doi = {10.1109/TSE.2020.2998785},
  url = {https://ieeexplore.ieee.org/abstract/document/9104918},
  urldate = {2024-09-19},
  abstract = {Automated program repair is a promising approach to reducing the costs of manual debugging and increasing software quality. However, recent studies have shown that automated program repair techniques can be prone to producing patches of low quality, overfitting to the set of tests provided to the repair technique, and failing to generalize to the intended specification. This paper rigorously explores this phenomenon on real-world Java programs, analyzing the effectiveness of four well-known repair techniques, GenProg, Par, SimFix, and TrpAutoRepair, on defects made by the projects’ developers during their regular development process. We find that: (1) When applied to real-world Java code, automated program repair techniques produce patches for between 10.6 and 19.0 percent of the defects, which is less frequent than when applied to C code. (2) The produced patches often overfit to the provided test suite, with only between 13.8 and 46.1 percent of the patches passing an independent set of tests. (3) Test suite size has an extremely small but significant effect on the quality of the patches, with larger test suites producing higher-quality patches, though, surprisingly, higher-coverage test suites correlate with lower-quality patches. (4) The number of tests that a buggy program fails has a small but statistically significant positive effect on the quality of the produced patches. (5) Test suite provenance, whether the test suite is written by a human or automatically generated, has a significant effect on the quality of the patches, with developer-written tests typically producing higher-quality patches. And (6) the patches exhibit insufficient diversity to improve quality through some method of combining multiple patches. We develop JaRFly, an open-source framework for implementing techniques for automatic search-based improvement of Java programs. Our study uses JaRFly to faithfully reimplement GenProg and TrpAutoRepair to work on Java code, and makes the first public release of an implementation of Par. Unlike prior work, our study carefully controls for confounding factors and produces a methodology, as well as a dataset of automatically-generated test suites, for objectively evaluating the quality of Java repair techniques on real-world defects.},
  eventtitle = {{{IEEE Transactions}} on {{Software Engineering}}},
  keywords = {Automated program repair,Contracts,Defects4J,Diversity reception,GenProg,Inspection,Java,Maintenance engineering,Manuals,notion,objective quality measure,Par,patch quality,Software quality,TrpAutoRepair},
  file = {/Users/bohrok/Zotero/storage/5IKYCQQQ/Motwani et al. - 2022 - Quality of Automated Program Repair on Real-World Defects.pdf;/Users/bohrok/Zotero/storage/MXHEF8RT/9104918.html}
}

@inproceedings{mudduluruVerifyingDeterminismSequential2021,
  title = {Verifying {{Determinism}} in {{Sequential Programs}}},
  booktitle = {2021 {{IEEE}}/{{ACM}} 43rd {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Mudduluru, Rashmi and Waataja, Jason and Milstein, Suzanne and Ernst, Michael D.},
  date = {2021},
  pages = {37--49},
  doi = {10.1109/ICSE43902.2021.00017},
  keywords = {notion}
}

@misc{mullerFexAssistedIdentification2021,
  title = {Fex: {{Assisted Identification}} of {{Domain Features}} from {{C Programs}}},
  author = {Müller, Patrick and Narasimhan, Krishna and Mezini, Mira},
  date = {2021},
  keywords = {notion}
}

@article{mundEfficientInterproceduralDynamic2006,
  title = {An {{Efficient Interprocedural Dynamic Slicing Method}}},
  author = {Mund, G. B. and Mall, Rajib},
  date = {2006-06},
  journaltitle = {J. Syst. Softw.},
  volume = {79},
  number = {6},
  pages = {791--806},
  publisher = {Elsevier Science Inc.},
  location = {New York, NY, USA},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2005.07.024},
  url = {http://dx.doi.org/10.1016/j.jss.2005.07.024},
  keywords = {Control dependence graph,Control flow graph,Dynamic slicing,notion,Program debugging,Program dependence graph,Program slicing,Static slicing}
}

@inproceedings{muQuantitativeProgramDependence2012,
  title = {Quantitative {{Program Dependence Graphs}}},
  booktitle = {Formal {{Methods}} and {{Software Engineering}}},
  author = {Mu, Chunyan},
  editor = {Aoki, Toshiaki and Taguchi, Kenji},
  date = {2012},
  pages = {103--118},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  abstract = {This paper develops a novel approach that analyses dependencies of programs in a quantitative aspect. We introduce a definition of Quantitative Program Dependence Graph (QPDG) which can be used to model a program's behaviour given spaces of inputs. The programs we consider are in a core while-language. We present the semantics for the purpose of building QPDGs. The QPDG reasons about the program's quantitative uncertainty behaviours based on a probabilistic analysis. It can be used to characterise dependence analysis of programs in a quantitative way. We also provides an optimisation of the QPDG by doing slicing in order to perform a flow analysis, e.g., how input variables at the source node might affect a given output variable at the target node and how much.},
  isbn = {978-3-642-34281-3},
  keywords = {notion}
}

@inproceedings{murphyAutomaticSystemTesting2009,
  title = {Automatic {{System Testing}} of {{Programs Without Test Oracles}}},
  booktitle = {Proceedings of the {{Eighteenth International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Murphy, Christian and Shen, Kuang and Kaiser, Gail},
  date = {2009},
  series = {{{ISSTA}} '09},
  pages = {189--200},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/1572272.1572295},
  url = {http://doi.acm.org/10.1145/1572272.1572295},
  isbn = {978-1-60558-338-9},
  venue = {Chicago, IL, USA},
  keywords = {Metamorphic Testing,notion,Software Testing}
}

@article{murrayBPFormalProofs2018,
  title = {{{BP}}: {{Formal Proofs}}, the {{Fine Print}} and {{Side Effects}}},
  author = {Murray, Toby C. and family=Oorschot, given=Paul C., prefix=van, useprefix=false},
  date = {2018},
  journaltitle = {2018 IEEE Cybersecurity Development (SecDev)},
  pages = {1--10},
  keywords = {notion}
}

@inproceedings{muylaertUntanglingCompositeCommits2018,
  title = {Untangling {{Composite Commits Using Program Slicing}}},
  booktitle = {2018 {{IEEE}} 18th {{International Working Conference}} on {{Source Code Analysis}} and {{Manipulation}} ({{SCAM}})},
  author = {Muylaert, Ward and De Roover, Coen},
  date = {2018},
  pages = {193--202},
  doi = {10.1109/SCAM.2018.00030},
  keywords = {notion}
}

@inproceedings{nagyFullSpeedFuzzingReducing2019,
  title = {Full-{{Speed Fuzzing}}: {{Reducing Fuzzing Overhead}} through {{Coverage-Guided Tracing}}},
  booktitle = {2019 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  author = {Nagy, Stefan and Hicks, Matthew},
  date = {2019},
  pages = {787--802},
  doi = {10.1109/SP.2019.00069},
  keywords = {notion}
}

@inproceedings{naishSpectralDebuggingHow2012,
  title = {Spectral Debugging: {{How}} Much Better Can We Do?},
  booktitle = {{{ACSC}}},
  author = {Naish, Lee and Lee, Hua Jie and Ramamohanarao, Kotagiri},
  date = {2012},
  keywords = {notion}
}

@article{nayrollesBugReproductionApproach2017,
  title = {A Bug Reproduction Approach Based on Directed Model Checking and Crash Traces},
  author = {Nayrolles, Mathieu and Hamou-Lhadj, Abdelwahab and Tahar, Sofiène and Larsson, Alf},
  date = {2017},
  journaltitle = {Journal of Software: Evolution and Process},
  volume = {29},
  number = {3},
  pages = {e1789},
  doi = {10.1002/smr.1789},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.1789},
  abstract = {Abstract Reproducing a bug that caused a system to crash is an important task for uncovering the causes of the crash and providing appropriate fixes. In this paper, we propose a novel crash reproduction approach that combines directed model checking and backward slicing to identify the program statements needed to reproduce a crash. Our approach, named JCHARMING (Java CrasH Automatic Reproduction by directed Model checkING), uses information found in crash traces combined with static program slices to guide a model checking engine in an optimal way. We show that JCHARMING is efficient in reproducing bugs from 10 different open source systems. Overall, JCHARMING is able to reproduce 80\% of the bugs used in this study in an average time of 19 min. Copyright \textbackslash copyright 2016 John Wiley \& Sons, Ltd.},
  keywords = {automatic bug reproduction,dynamic analysis,model checking,notion,software maintenance}
}

@article{nemenmanEntropyInferenceRevisited2001,
  title = {Entropy and Inference, Revisited},
  author = {Nemenman, Ilya and Shafee, Fariel and Bialek, William},
  date = {2001},
  journaltitle = {Advances in neural information processing systems},
  volume = {14},
  keywords = {notion}
}

@inproceedings{newsomeDynamicTaintAnalysis2005,
  title = {Dynamic {{Taint Analysis}} for {{Automatic Detection}}, {{Analysis}}, and {{SignatureGeneration}} of {{Exploits}} on {{Commodity Software}}},
  booktitle = {Network and {{Distributed System Security Symposium}}},
  author = {Newsome, James and Song, Dawn Xiaodong},
  date = {2005},
  url = {https://api.semanticscholar.org/CorpusID:99191},
  keywords = {notion}
}

@inproceedings{newsomeMeasuringChannelCapacity2009,
  title = {Measuring Channel Capacity to Distinguish Undue Influence},
  booktitle = {Proceedings of the {{ACM SIGPLAN Fourth Workshop}} on {{Programming Languages}} and {{Analysis}} for {{Security}}},
  author = {Newsome, James and McCamant, Stephen and Song, Dawn},
  date = {2009},
  pages = {73--85},
  keywords = {notion}
}

@inproceedings{nguyenCrosslanguageProgramSlicing2015,
  title = {Cross-Language {{Program Slicing}} for {{Dynamic Web Applications}}},
  booktitle = {Proceedings of the 2015 10th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Nguyen, Hung Viet and Kästner, Christian and Nguyen, Tien N.},
  date = {2015},
  series = {{{ESEC}}/{{FSE}} 2015},
  pages = {369--380},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2786805.2786872},
  url = {http://doi.acm.org/10.1145/2786805.2786872},
  isbn = {978-1-4503-3675-8},
  venue = {Bergamo, Italy},
  keywords = {cross-language analysis,dynamic web applications,notion,Program slicing}
}

@inproceedings{nguyenSemFixProgramRepair2013,
  title = {{{SemFix}}: {{Program}} Repair via Semantic Analysis},
  booktitle = {2013 35th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Nguyen, H. D. T. and Qi, D. and Roychoudhury, A. and Chandra, S.},
  date = {2013-05},
  pages = {772--781},
  issn = {0270-5257},
  doi = {10.1109/ICSE.2013.6606623},
  keywords = {automated program repair methods,Computer bugs,constraint solving,debugging,Educational institutions,genetic algorithms,Genetic programming,genetic programming based repair,GNU Coreutils,Input variables,Maintenance engineering,notion,program debugging,program synthesis,program testing,repair code complexity,repair expressions,seeded bugs,semantic analysis,Semantics,SemFix,SIR programs,software development project,software engineering,symbolic execution,Syntactics}
}

@inproceedings{nguyenStatisticalSemanticLanguage2013,
  title = {A {{Statistical Semantic Language Model}} for {{Source Code}}},
  booktitle = {Proceedings of the 2013 9th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Nguyen, Tung Thanh and Nguyen, Anh Tuan and Nguyen, Hoan Anh and Nguyen, Tien N.},
  date = {2013},
  series = {{{ESEC}}/{{FSE}} 2013},
  pages = {532--542},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2491411.2491458},
  url = {https://doi.org/10.1145/2491411.2491458},
  abstract = {Recent research has successfully applied the statistical n-gram language model to show that source code exhibits a good level of repetition. The n-gram model is shown to have good predictability in supporting code suggestion and completion. However, the state-of-the-art n-gram approach to capture source code regularities/patterns is based only on the lexical information in a local context of the code units. To improve predictability, we introduce SLAMC, a novel statistical semantic language model for source code. It incorporates semantic information into code tokens and models the regularities/patterns of such semantic annotations, called sememes, rather than their lexemes. It combines the local context in semantic n-grams with the global technical concerns/functionality into an n-gram topic model, together with pairwise associations of program elements. Based on SLAMC, we developed a new code suggestion method, which is empirically evaluated on several projects to have relatively 18-68\% higher accuracy than the state-of-the-art approach.},
  isbn = {978-1-4503-2237-9},
  venue = {Saint Petersburg, Russia},
  keywords = {Code Completion,notion,Statistical Semantic Language Model}
}

@inproceedings{nicolaeRevisitingNeuralProgram2023,
  title = {Revisiting {{Neural Program Smoothing}} for {{Fuzzing}}},
  booktitle = {Proceedings of the 31st {{ACM Joint European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Nicolae, Maria-Irina and Eisele, Max and Zeller, Andreas},
  date = {2023},
  series = {{{ESEC}}/{{FSE}} 2023},
  pages = {133--145},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3611643.3616308},
  url = {https://doi.org/10.1145/3611643.3616308},
  abstract = {Testing with randomly generated inputs (fuzzing) has gained significant traction due to its capacity to expose program vulnerabilities automatically. Fuzz testing campaigns generate large amounts of data, making them ideal for the application of machine learning (ML). Neural program smoothing, a specific family of ML-guided fuzzers, aims to use a neural network as a smooth approximation of the program target for new test case generation. In this paper, we conduct the most extensive evaluation of neural program smoothing (NPS) fuzzers against standard gray-box fuzzers ({$>$}11 CPU years and {$>$}5.5 GPU years), and make the following contributions: We find that the original performance claims for NPS fuzzers do not hold; a gap we relate to fundamental, implementation, and experimental limitations of prior works. We contribute the first in-depth analysis of the contribution of machine learning and gradient-based mutations in NPS. We implement Neuzz++, which shows that addressing the practical limitations of NPS fuzzers improves performance, but that standard gray-box fuzzers almost always surpass NPS-based fuzzers. As a consequence, we propose new guidelines targeted at benchmarking fuzzing based on machine learning, and present MLFuzz, a platform with GPU access for easy and reproducible evaluation of ML-based fuzzers. Neuzz++, MLFuzz, and all our data are public.},
  isbn = {9798400703270},
  venue = {{$<$}conf-loc{$>$}, {$<$}city{$>$}San Francisco{$<$}/city{$>$}, {$<$}state{$>$}CA{$<$}/state{$>$}, {$<$}country{$>$}USA{$<$}/country{$>$}, {$<$}/conf-loc{$>$}},
  keywords = {fuzzing,machine learning,neural networks,neural program smoothing,notion}
}

@misc{nieEvaluationMethodologiesCode2021,
  title = {Evaluation {{Methodologies}} for {{Code Learning Tasks}}},
  author = {Nie, Pengyu and Zhang, Jiyang and Li, Junyi Jessy and Mooney, Raymond J. and Gligoric, Milos},
  date = {2021},
  keywords = {notion}
}

@inproceedings{nilizadehDifFuzzDifferentialFuzzing2019,
  title = {{{DifFuzz}}: {{Differential Fuzzing}} for {{Side-Channel Analysis}}},
  booktitle = {2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Nilizadeh, Shirin and Noller, Yannic and Pasareanu, Corina S.},
  date = {2019},
  pages = {176--187},
  doi = {10.1109/ICSE.2019.00034},
  keywords = {Correlation,dynamic analysis,fuzzing,Fuzzing,Instruments,Java,notion,Performance analysis,side-channel,Time factors,Tools,vulnerability detection}
}

@misc{nollerHowTrustAutogenerated2021,
  title = {How to Trust Auto-Generated Code Patches? {{A}} Developer Survey and Empirical Assessment of Existing Program Repair Tools},
  author = {Noller, Yannic and Shariffdeen, Ridwan and Gao, Xiang and Roychoudhury, Abhik},
  date = {2021},
  keywords = {notion}
}

@thesis{nollerModelCountingString2016,
  type = {mathesis},
  title = {Model Counting of String Constraints for Probabilistic Symbolic Execution},
  author = {Noller, Yannic},
  date = {2016},
  institution = {University of Stuttgart},
  keywords = {notion}
}

@inproceedings{nollerQFuzzQuantitativeFuzzing2021,
  title = {{{QFuzz}}: {{Quantitative Fuzzing}} for {{Side Channels}}},
  booktitle = {Proceedings of the 30th {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Noller, Yannic and Tizpaz-Niari, Saeid},
  date = {2021},
  series = {{{ISSTA}} 2021},
  pages = {257--269},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3460319.3464817},
  url = {https://doi.org/10.1145/3460319.3464817},
  abstract = {Side channels pose a significant threat to the confidentiality of software systems. Such vulnerabilities are challenging to detect and evaluate because they arise from non-functional properties of software such as execution times and require reasoning on multiple execution traces. Recently, noninterference notions have been adapted in static analysis, symbolic execution, and greybox fuzzing techniques. However, noninterference is a strict notion and may reject security even if the strength of information leaks are weak. A quantitative notion of security allows for the relaxation of noninterference and tolerates small (unavoidable) leaks. Despite progress in recent years, the existing quantitative approaches have scalability limitations in practice. In this work, we present QFuzz, a greybox fuzzing technique to quantitatively evaluate the strength of side channels with a focus on min entropy. Min entropy is a measure based on the number of distinguishable observations (partitions) to assess the resulting threat from an attacker who tries to compromise secrets in one try. We develop a novel greybox fuzzing equipped with two partitioning algorithms that try to maximize the number of distinguishable observations and the cost differences between them. We evaluate QFuzz on a large set of benchmarks from existing work and real-world libraries (with a total of 70 subjects). QFuzz compares favorably to three state-of-the-art detection techniques. QFuzz provides quantitative information about leaks beyond the capabilities of all three techniques. Crucially, we compare QFuzz to a state-of-the-art quantification tool and find that QFuzz significantly outperforms the tool in scalability while maintaining similar precision. Overall, we find that our approach scales well for real-world applications and provides useful information to evaluate resulting threats. Additionally, QFuzz identifies a zero-day side-channel vulnerability in a security critical Java library that has since been confirmed and fixed by the developers.},
  isbn = {978-1-4503-8459-9},
  venue = {Virtual, Denmark},
  keywords = {dynamic analysis,fuzzing,notion,quantification,side-channel analysis,vulnerability detection}
}

@inproceedings{ocinneideExperimentalAssessmentSoftware2012,
  title = {Experimental {{Assessment}} of {{Software Metrics Using Automated Refactoring}}},
  booktitle = {Proceedings of the {{ACM-IEEE International Symposium}} on {{Empirical Software Engineering}} and {{Measurement}}},
  author = {Ó Cinnéide, Mel and Tratt, Laurence and Harman, Mark and Counsell, Steve and Hemati Moghadam, Iman},
  date = {2012},
  series = {{{ESEM}} '12},
  pages = {49--58},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2372251.2372260},
  url = {http://doi.acm.org/10.1145/2372251.2372260},
  isbn = {978-1-4503-1056-7},
  venue = {Lund, Sweden},
  keywords = {notion,Refectoring,SBSE,Software Metrics}
}

@article{oeschPTreeProgramming2017,
  title = {P-{{Tree Programming}}},
  author = {Oesch, Chrsitian},
  date = {2017-07},
  keywords = {notion}
}

@inproceedings{ohannessianRareProbabilityEstimation2012,
  title = {Rare {{Probability Estimation}} under {{Regularly Varying Heavy Tails}}},
  booktitle = {Proceedings of the 25th {{Annual Conference}} on {{Learning Theory}}},
  author = {Ohannessian, Mesrob I. and Dahleh, Munther A.},
  editor = {Mannor, Shie and Srebro, Nathan and Williamson, Robert C.},
  date = {2012-06-25},
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {23},
  pages = {21.1--21.24},
  publisher = {PMLR},
  location = {Edinburgh, Scotland},
  url = {https://proceedings.mlr.press/v23/ohannessian12.html},
  abstract = {This paper studies the problem of estimating the probability of symbols that have occurred very rarely, in samples drawn independently from an unknown, possibly infinite, discrete distribution. In particular, we study the multiplicative consistency of estimators, defined as the ratio of the estimate to the true quantity converging to one. We first show that the classical Good-Turing estimator is not universally consistent in this sense, despite enjoying favorable additive properties. We then use Karamata's theory of regular variation to prove that regularly varying heavy tails are sufficient for consistency. At the core of this result is a multiplicative concentration that we establish both by extending the McAllester-Ortiz additive concentration for the missing mass to all rare probabilities and by exploiting regular variation. We also derive a family of estimators which, in addition to being consistent, address some of the shortcomings of the Good-Turing estimator. For example, they perform smoothing implicitly and have the absolute discounting structure of many heuristic algorithms. This also establishes a discrete parallel to extreme value theory, and many of the techniques therein can be adapted to the framework that we set forth.},
  keywords = {notion}
}

@inproceedings{ohEffectivelySamplingHigher2021a,
  title = {Effectively {{Sampling Higher Order Mutants Using Causal Effect}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation Workshops}} ({{ICSTW}})},
  author = {Oh, Saeyoon and Lee, Seongmin and Yoo, Shin},
  date = {2021-04},
  year = {2021},
  pages = {19--24},
  doi = {10.1109/ICSTW52544.2021.00017},
  url = {https://ieeexplore.ieee.org/abstract/document/9440150},
  urldate = {2024-11-18},
  abstract = {Higher Order Mutation (HOM) has been proposed to avoid equivalent mutants and improve the scalability of mutation testing, but generating useful HOMs remain an expensive search problem on its own. We propose a new approach to generate Strongly Subsuming Higher Order Mutants (SSHOM) using a recently introduced Causal Program Dependence Analysis (CPDA). CPDA itself is based on program mutation, and provides quantitative estimation of how often a change of the value of a program element will cause a value change of another program element. Our SSHOM generation approach chooses pairs of program elements using heuristics based on CPDA analysis, performs First Order Mutation to the chosen pairs, and generates an HOM by combining two FOMs.},
  eventtitle = {2021 {{IEEE International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation Workshops}} ({{ICSTW}})},
  keywords = {Benchmark testing,causal inference,causal program dependence analysis,Conferences,Diversity reception,Estimation,higher order mutant,Measurement,mutation testing,Scalability,Software testing},
  file = {/Users/bohrok/Zotero/storage/83FC8YBH/Oh et al. - 2021 - Effectively Sampling Higher Order Mutants Using Causal Effect.pdf;/Users/bohrok/Zotero/storage/Y5TQM4KY/9440150.html}
}

@article{ojdanicMutationTestingEvolving2022,
  title = {Mutation {{Testing}} in {{Evolving Systems}}: {{Studying}} the Relevance of Mutants to Code Evolution},
  author = {Ojdanić, Milo\textbackslash vs and Soremekun, Ezekiel O. and Degiovanni, Renzo and Papadakis, Mike and Traon, Yves Le},
  date = {2022},
  journaltitle = {ACM Transactions on Software Engineering and Methodology},
  keywords = {notion}
}

@article{ojdanicUseCommitrelevantMutants2022,
  title = {On the Use of Commit-Relevant Mutants},
  author = {Ojdanić, Miloš and Ma, Wei and Laurent, Thomas and Chekam, Thierry Titcheu and Ventresque, Anthony and Papadakis, Mike},
  date = {2022-05-30},
  journaltitle = {Empirical Software Engineering},
  volume = {27},
  number = {5},
  pages = {114},
  doi = {10.1007/s10664-022-10138-1},
  url = {https://doi.org/10.1007/s10664-022-10138-1},
  abstract = {Applying mutation testing to test subtle program changes, such as program patches or other small-scale code modifications, requires using mutants that capture the delta of the altered behaviours. To address this issue, we introduce the concept of commit-relevant mutants, which are the mutants that interact with the behaviours of the system affected by a particular commit. Therefore, commit-aware mutation testing, is a test assessment metric tailored to a specific commit. By analysing 83 commits from 25 projects involving 2,253,610 mutants in both C and Java, we identify the commit-relevant mutants and explore their relationship with other categories of mutants. Our results show that commit-relevant mutants represent a small subset of all mutants, which differs from the other classes of mutants (subsuming and hard-to-kill), and that the commit-relevant mutation score is weakly correlated with the traditional mutation score (Kendall/Pearson 0.15-0.4). Moreover, commit-aware mutation analysis provides insights about the testing of a commit, which can be more efficient than the classical mutation analysis; in our experiments, by analysing the same number of mutants, commit-aware mutants have better fault-revelation potential (30\% higher chances of revealing commit-introducing faults) than traditional mutants. We also illustrate a possible application of commit-aware mutation testing as a metric to evaluate test case prioritisation.},
  isbn = {1573-7616},
  keywords = {notion}
}

@article{oneillGrammaticalEvolution2001,
  title = {Grammatical Evolution},
  author = {O'Neill, M. and Ryan, C.},
  date = {2001-08},
  journaltitle = {IEEE Transactions on Evolutionary Computation},
  volume = {5},
  number = {4},
  pages = {349--358},
  issn = {1941-0026},
  doi = {10.1109/4235.942529},
  keywords = {automatic programming,Backus-Naur form,binary genome,Bioinformatics,Biological information theory,complexity,computational complexity,degenerate code,Evolution (biology),evolutionary algorithm,Evolutionary computation,genetic algorithms,Genetic mutations,Genetic programming,Genomics,grammars,grammatical evolution,Law,Legal factors,neural nets,neural networks,notion,Production,variable-length binary string}
}

@article{orlitskyAlwaysGoodTuring2003,
  title = {Always Good Turing: {{Asymptotically}} Optimal Probability Estimation},
  author = {Orlitsky, Alon and Santhanam, Narayana P and Zhang, Junan},
  date = {2003},
  journaltitle = {Science},
  volume = {302},
  number = {5644},
  pages = {427--431},
  publisher = {American Association for the Advancement of Science},
  keywords = {notion}
}

@inproceedings{orlitskyCompetitiveDistributionEstimation2015,
  title = {Competitive {{Distribution Estimation}}: {{Why}} Is {{Good-Turing Good}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Orlitsky, Alon and Suresh, Ananda Theertha},
  editor = {Cortes, C. and Lawrence, N. and Lee, D. and Sugiyama, M. and Garnett, R.},
  date = {2015},
  volume = {28},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2015/file/d759175de8ea5b1d9a2660e45554894f-Paper.pdf},
  keywords = {notion}
}

@article{orlitskyOptimalPredictionNumber2016,
  title = {Optimal Prediction of the Number of Unseen Species},
  author = {Orlitsky, Alon and Suresh, Ananda Theertha and Wu, Yihong},
  date = {2016},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {113},
  number = {47},
  pages = {13283--13288},
  doi = {10.1073/pnas.1607774113},
  url = {https://www.pnas.org/doi/abs/10.1073/pnas.1607774113},
  abstract = {Estimating the number of unseen species is an important problem in many scientific endeavors. Its most popular formulation, introduced by Fisher et al. [Fisher RA, Corbet AS, Williams CB (1943) J Animal Ecol 12(1):42−58], uses n samples to predict the number U of hitherto unseen species that would be observed if t⋅n new samples were collected. Of considerable interest is the largest ratio t between the number of new and existing samples for which U can be accurately predicted. In seminal works, Good and Toulmin [Good I, Toulmin G (1956) Biometrika 43(102):45−63] constructed an intriguing estimator that predicts U for all t≤1. Subsequently, Efron and Thisted [Efron B, Thisted R (1976) Biometrika 63(3):435−447] proposed a modification that empirically predicts U even for some t\&gt;1, but without provable guarantees. We derive a class of estimators that provably predict U all of the way up to t∝log⁡n. We also show that this range is the best possible and that the estimator's mean-square error is near optimal for any t. Our approach yields a provable guarantee for the Efron−Thisted estimator and, in addition, a variant with stronger theoretical and experimental performance than existing methodologies on a variety of synthetic and real datasets. The estimators are simple, linear, computationally efficient, and scalable to massive datasets. Their performance guarantees hold uniformly for all distributions, and apply to all four standard sampling models commonly used across various scientific disciplines: multinomial, Poisson, hypergeometric, and Bernoulli product.},
  keywords = {notion}
}

@inproceedings{ottensteinProgramDependenceGraph1984,
  title = {The Program Dependence Graph in a Software Development Environment},
  booktitle = {{{SDE}} 1},
  author = {Ottenstein, Karl J. and Ottenstein, Linda M.},
  date = {1984},
  keywords = {notion}
}

@inproceedings{oyaBackDrawingBoard2017,
  title = {Back to the {{Drawing Board}}: {{Revisiting}} the {{Design}} of {{Optimal Location Privacy-preserving Mechanisms}}},
  booktitle = {Proceedings of the 2017 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Oya, Simon and Troncoso, Carmela and Pérez-González, Fernando},
  date = {2017},
  series = {{{CCS}} '17},
  pages = {1959--1972},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3133956.3134004},
  url = {https://doi.org/10.1145/3133956.3134004},
  abstract = {In the last years we have witnessed the appearance of a variety of strategies to design optimal location privacy-preserving mechanisms, in terms of maximizing the adversary's expected error with respect to the users' whereabouts. In this work, we take a closer look at the defenses created by these strategies and show that, even though they are indeed optimal in terms of adversary's correctness, not all of them offer the same protection when looking at other dimensions of privacy. To avoid "bad" choices, we argue that the search for optimal mechanisms must be guided by complementary criteria. We provide two example auxiliary metrics that help in this regard: the conditional entropy, that captures an information-theoretic aspect of the problem; and the worst-case quality loss, that ensures that the output of the mechanism always provides a minimum utility to the users. We describe a new mechanism that maximizes the conditional entropy and is optimal in terms of average adversary error, and compare its performance with previously proposed optimal mechanisms using two real datasets. Our empirical results confirm that no mechanism fares well on every privacy criteria simultaneously, making apparent the need for considering multiple privacy dimensions to have a good understanding of the privacy protection a mechanism provides.},
  isbn = {978-1-4503-4946-8},
  venue = {Dallas, Texas, USA},
  keywords = {location privacy,mechanism design,mechanism evaluation,notion,quantifying privacy}
}

@article{padhyeFuzzFactoryDomainspecificFuzzing2019,
  title = {{{FuzzFactory}}: Domain-Specific Fuzzing with Waypoints},
  shorttitle = {{{FuzzFactory}}},
  author = {Padhye, Rohan and Lemieux, Caroline and Sen, Koushik and Simon, Laurent and Vijayakumar, Hayawardh},
  date = {2019-10-10},
  journaltitle = {Proceedings of the ACM on Programming Languages},
  volume = {3},
  pages = {1--29},
  publisher = {Association for Computing Machinery},
  doi = {10.1145/3360600},
  url = {https://dl.acm.org/doi/10.1145/3360600},
  urldate = {2024-09-19},
  abstract = {Coverage-guided fuzz testing has gained prominence as a highly effective method of finding security vulnerabilities such as buffer overflows in programs that parse binary data. Recently, researchers have introduced various specializations to the coverage-guided fuzzing algorithm for different domain-specific testing goals, such as finding performance bottlenecks, generating valid inputs, handling magic-byte comparisons, etc. Each such solution can require non-trivial implementation effort and produces a distinct variant of a fuzzing tool. We observe that many of these domain-specific solutions follow a common solution pattern. In this paper, we present FuzzFactory, a framework for developing domain-specific fuzzing applications without requiring changes to mutation and search heuristics. FuzzFactory allows users to specify the collection of dynamic domain-specific feedback during test execution, as well as how such feedback should be aggregated. FuzzFactory uses this information to selectively save intermediate inputs, called waypoints, to augment coverage-guided fuzzing. Such waypoints always make progress towards domain-specific multi-dimensional objectives. We instantiate six domain-specific fuzzing applications using FuzzFactory: three re-implementations of prior work and three novel solutions, and evaluate their effectiveness on benchmarks from Google's fuzzer test suite. We also show how multiple domains can be composed to perform better than the sum of their parts. For example, we combine domain-specific feedback about strict equality comparisons and dynamic memory allocations, to enable the automatic generation of LZ4 bombs and PNG bombs.},
  issue = {OOPSLA},
  keywords = {domain-specific fuzzing,frameworks,fuzz testing,notion,waypoints},
  file = {/Users/bohrok/Zotero/storage/AZVYHII4/Padhye et al. - 2019 - FuzzFactory domain-specific fuzzing with waypoints.pdf}
}

@inproceedings{padhyeSemanticFuzzingZest2019,
  title = {Semantic Fuzzing with Zest},
  booktitle = {Proceedings of the 28th {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Padhye, Rohan and Lemieux, Caroline and Sen, Koushik and Papadakis, Mike and Le Traon, Yves},
  date = {2019-07-10},
  series = {{{ISSTA}} 2019},
  pages = {329--340},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3293882.3330576},
  url = {https://dl.acm.org/doi/10.1145/3293882.3330576},
  urldate = {2024-09-19},
  abstract = {Programs expecting structured inputs often consist of both a syntactic analysis stage, which parses raw input, and a semantic analysis stage, which conducts checks on the parsed input and executes the core logic of the program. Generator-based testing tools in the lineage of QuickCheck are a promising way to generate random syntactically valid test inputs for these programs. We present Zest, a technique which automatically guides QuickCheck-like random input generators to better explore the semantic analysis stage of test programs. Zest converts random-input generators into deterministic parametric input generators. We present the key insight that mutations in the untyped parameter domain map to structural mutations in the input domain. Zest leverages program feedback in the form of code coverage and input validity to perform feedback-directed parameter search. We evaluate Zest against AFL and QuickCheck on five Java programs: Maven, Ant, BCEL, Closure, and Rhino. Zest covers 1.03x-2.81x as many branches within the benchmarks' semantic analysis stages as baseline techniques. Further, we find 10 new bugs in the semantic analysis stages of these benchmarks. Zest is the most effective technique in finding these bugs reliably and quickly, requiring at most 10 minutes on average to find each bug.},
  isbn = {978-1-4503-6224-5},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/8FXPUNNW/Padhye et al. - 2019 - Semantic fuzzing with zest.pdf}
}

@online{padhyeSoftwareEngineeringMethods2024a,
  title = {Software {{Engineering Methods For AI-Driven Deductive Legal Reasoning}}},
  author = {Padhye, Rohan},
  date = {2024-06-27},
  eprint = {2404.09868},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2404.09868},
  url = {http://arxiv.org/abs/2404.09868},
  urldate = {2024-09-19},
  abstract = {The recent proliferation of generative artificial intelligence (AI) technologies such as pre-trained large language models (LLMs) has opened up new frontiers in computational law. An exciting area of development is the use of AI to automate the deductive rule-based reasoning inherent in statutory and contract law. This paper argues that such automated deductive legal reasoning can now be viewed from the lens of software engineering, treating LLMs as interpreters of natural-language programs with natural-language inputs. We show how it is possible to apply principled software engineering techniques to enhance AI-driven legal reasoning of complex statutes and to unlock new applications in automated meta-reasoning such as mutation-guided example generation and metamorphic property-based testing.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Software Engineering,notion},
  file = {/Users/bohrok/Zotero/storage/UZ6ZMS6S/Padhye - 2024 - Software Engineering Methods For AI-Driven Deductive Legal Reasoning.pdf;/Users/bohrok/Zotero/storage/NTX4JCVN/2404.html}
}

@inproceedings{painskyConvergenceGuaranteesGoodTuring2022,
  title = {Convergence {{Guarantees}} for the {{Good-Turing Estimator}}},
  author = {Painsky, Amichai},
  date = {2022},
  keywords = {notion}
}

@article{painskyGeneralizedGoodTuringImproves2021,
  title = {Generalized {{Good-Turing Improves Missing Mass Estimation}}},
  author = {Painsky, Amichai},
  date = {2021},
  journaltitle = {Journal of the American Statistical Association},
  keywords = {notion}
}

@article{painskyGeneralizedGoodTuringImproves2023,
  title = {Generalized {{Good-Turing Improves Missing Mass Estimation}}},
  author = {Painsky, Amichai},
  date = {2023-07-03},
  journaltitle = {Journal of the American Statistical Association},
  volume = {118},
  number = {543},
  pages = {1890--1899},
  publisher = {ASA Website},
  issn = {0162-1459},
  doi = {10.1080/01621459.2021.2020658},
  url = {https://doi.org/10.1080/01621459.2021.2020658},
  urldate = {2024-09-30},
  abstract = {Consider a finite sample from an unknown distribution over a countable alphabet. The missing mass refers to the probability of symbols that do not appear in the sample. Estimating the missing mass is a basic problem in statistics and related fields, which dates back to the early work of Laplace, and the more recent seminal contribution of Good and Turing. In this article, we introduce a generalized Good-Turing (GT) framework for missing mass estimation. We derive an upper-bound for the risk (in terms of mean squared error) and minimize it over the parameters of our framework. Our analysis distinguishes between two setups, depending on the (unknown) alphabet size. When the alphabet size is bounded from above, our risk-bound demonstrates a significant improvement compared to currently known results (which are typically oblivious to the alphabet size). Based on this bound, we introduce a numerically obtained estimator that improves upon GT. When the alphabet size holds no restrictions, we apply our suggested risk-bound and introduce a closed-form estimator that again improves GT performance guarantees. Our suggested framework is easy to apply and does not require additional modeling assumptions. This makes it a favorable choice for practical applications. Supplementary materials for this article are available online.},
  keywords = {Categorical data analysis,Frequency of frequencies,Minimax estimation,notion,Rule of succession},
  file = {/Users/bohrok/Zotero/storage/M6AKB7I8/Painsky - 2023 - Generalized Good-Turing Improves Missing Mass Estimation.pdf}
}

@inproceedings{paixaoCROPLinkingCode2018,
  title = {{{CROP}}: {{Linking Code Reviews}} to {{Source Code Changes}}},
  booktitle = {2018 {{IEEE}}/{{ACM}} 15th {{International Conference}} on {{Mining Software Repositories}} ({{MSR}})},
  author = {Paixao, M. and Krinke, J. and Han, D. and Harman, M.},
  date = {2018-05},
  pages = {46--49},
  issn = {2574-3848},
  keywords = {Agriculture,Code Review,code review datasets,Code Review Open Platform,code review repository,configuration management,CROP,Data mining,History,Metadata,notion,Open source software,open source software development,Platform,public domain software,Repository,Servers,Software Change Analysis,software engineering,software reviews,source code,source code (software)}
}

@article{paixaoImpactCodeReview2021,
  title = {The {{Impact}} of {{Code Review}} on {{Architectural Changes}}},
  author = {Paixao, Matheus and Krinke, Jens and Han, DongGyun and Ragkhitwetsagul, Chaiyong and Harman, Mark},
  date = {2021},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {47},
  number = {5},
  pages = {1041--1059},
  doi = {10.1109/TSE.2019.2912113},
  keywords = {notion}
}

@inproceedings{palikarevaShadowDoubtTesting2016,
  title = {Shadow of a {{Doubt}}: {{Testing}} for {{Divergences Between Software Versions}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Software Engineering}}},
  author = {Palikareva, Hristina and Kuchta, Tomasz and Cadar, Cristian},
  date = {2016},
  series = {{{ICSE}} '16},
  pages = {1181--1192},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2884781.2884845},
  url = {http://doi.acm.org/10.1145/2884781.2884845},
  isbn = {978-1-4503-3900-1},
  venue = {Austin, Texas},
  keywords = {Cross-version Checks,notion,Regression Bugs,Software Testing,Symbolic Execution}
}

@inproceedings{panCanProgramSynthesis2021,
  title = {Can {{Program Synthesis}} Be {{Used}} to {{Learn Merge Conflict Resolutions}}? {{An Empirical Analysis}}},
  booktitle = {2021 {{IEEE}}/{{ACM}} 43rd {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Pan, Rangeet and Le, Vu and Nagappan, Nachiappan and Gulwani, Sumit and Lahiri, Shuvendu and Kaufman, Mike},
  date = {2021},
  pages = {785--796},
  publisher = {IEEE},
  keywords = {notion}
}

@article{pandaACCoNovelApproach2015,
  title = {{{ACCo}}: A Novel Approach to Measure Cohesion Using Hierarchical Slicing of {{Java}} Programs},
  author = {Panda, S. and Mohapatra, D. P.},
  date = {2015},
  journaltitle = {Innovations in Systems and Software Engineering},
  volume = {11},
  number = {4},
  pages = {243--260},
  doi = {10.1007/s11334-015-0252-8},
  url = {https://doi.org/10.1007/s11334-015-0252-8},
  abstract = {Maintainability of program parts refers to the ease with which these parts can be modified. Many existing metrics of internal quality attributes such as cohesion, coupling, etc. have been used in this regard. Cohesiveness among program parts is considered as a strong measure for the maintainability of object-oriented components and to predict the probability of being erroneous. Our objective is to propose a novel graph-based cohesion metric to measure the maintainability of different program parts in an object-oriented program and predict their fault proneness. We compute the cohesion of the sliced component as a measure to predict its correctness and preciseness. In addition, we wish to theoretically validate the proposed technique against the existing guidelines of cohesion measurement and compare it with some existing techniques. We propose a new cohesion metric named affected component cohesion (ACCo) to measure the maintainability of different program parts and predict their fault proneness. This metric is based on the hierarchical decomposition slice of an object-oriented program that comprises all the affected program parts. The slices are obtained with respect to some modification made to the program under consideration. It is essential to consider all possible dependence relationships along with the control and data dependences that exist between different program parts of an object-oriented program for a better program comprehension. To represent these dependences, we construct a suitable intermediate graph for an object-oriented program. Then, we compute the slice of the object-oriented program using the intermediate graph to extract the affected program parts. These extracted affected program parts are represented as nodes in the proposed affected slice graph. The critical and sub-critical nodes that require thorough testing are determined by estimating their cohesion measure. The theoretical validation signifies that ACCo satisfies all the existing properties for cohesion measurement. The results obtained are more precise and comparable with other existing approaches. ACCo is a more precise and practical technique to measure the inter-relatedness of affected program parts in an object-oriented program. The discussion on possible threats to its validity demonstrates the scope for improvement of this approach.},
  isbn = {1614-5054},
  keywords = {notion}
}

@inproceedings{panDecomposingConvolutionalNeural2022,
  title = {Decomposing {{Convolutional Neural Networks}} into {{Reusable}} and {{Replaceable Modules}}},
  booktitle = {International {{Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Pan, Rangeet and Rajan, Hridesh},
  date = {2022},
  keywords = {notion}
}

@inproceedings{panDecomposingDeepNeural2020,
  title = {On {{Decomposing}} a {{Deep Neural Network}} into {{Modules}}},
  booktitle = {Proceedings of the 28th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Pan, Rangeet and Rajan, Hridesh},
  date = {2020},
  series = {{{ESEC}}/{{FSE}} 2020},
  pages = {889--900},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3368089.3409668},
  url = {https://doi.org/10.1145/3368089.3409668},
  abstract = {Deep learning is being incorporated in many modern software systems. Deep learning approaches train a deep neural network (DNN) model using training examples, and then use the DNN model for prediction. While the structure of a DNN model as layers is observable, the model is treated in its entirety as a monolithic component. To change the logic implemented by the model, e.g. to add/remove logic that recognizes inputs belonging to a certain class, or to replace the logic with an alternative, the training examples need to be changed and the DNN needs to be retrained using the new set of examples. We argue that decomposing a DNN into DNN modules— akin to decomposing a monolithic software code into modules—can bring the benefits of modularity to deep learning. In this work, we develop a methodology for decomposing DNNs for multi-class problems into DNN modules. For four canonical problems, namely MNIST, EMNIST, FMNIST, and KMNIST, we demonstrate that such decomposition enables reuse of DNN modules to create different DNNs, enables replacement of one DNN module in a DNN with another without needing to retrain. The DNN models formed by composing DNN modules are at least as good as traditional monolithic DNNs in terms of test accuracy for our problems.},
  isbn = {978-1-4503-7043-1},
  venue = {Virtual Event, USA},
  keywords = {decomposing,deep neural networks,modularity,modules,notion}
}

@inproceedings{panEDEFuzzWebAPI2024,
  title = {{{EDEFuzz}}: {{A Web API Fuzzer}} for {{Excessive Data Exposures}}},
  booktitle = {Proceedings of the {{IEEE}}/{{ACM}} 46th {{International Conference}} on {{Software Engineering}}},
  author = {Pan, Lianglu and Cohney, Shaanan and Murray, Toby and Pham, Van-Thuan},
  date = {2024},
  series = {{{ICSE}} '24},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3597503.3608133},
  url = {https://doi.org/10.1145/3597503.3608133},
  abstract = {APIs often transmit far more data to client applications than they need, and in the context of web applications, often do so over public channels. This issue, termed Excessive Data Exposure (EDE), was OWASP's third most significant API vulnerability of 2019. However, there are few automated tools—either in research or industry—to effectively find and remediate such issues. This is unsurprising as the problem lacks an explicit test oracle: the vulnerability does not manifest through explicit abnormal behaviours (e.g., program crashes or memory access violations).In this work, we develop a metamorphic relation to tackle that challenge and build the first fuzzing tool—that we call EDEFuzz—to systematically detect EDEs. EDEFuzz can significantly reduce false negatives that occur during manual inspection and ad-hoc text-matching techniques, the current most-used approaches.We tested EDEFuzz against the sixty-nine applicable targets from the Alexa Top-200 and found 33,365 potential leaks—illustrating our tool's broad applicability and scalability. In a more-tightly controlled experiment of eight popular websites in Australia, EDEFuzz achieved a high true positive rate of 98.65\% with minimal configuration, illustrating our tool's accuracy and efficiency.},
  isbn = {9798400702174},
  venue = {{$<$}conf-loc{$>$}, {$<$}city{$>$}Lisbon{$<$}/city{$>$}, {$<$}country{$>$}Portugal{$<$}/country{$>$}, {$<$}/conf-loc{$>$}},
  keywords = {notion}
}

@inproceedings{pangMDPFuzzTestingModels2022,
  title = {{{MDPFuzz}}: Testing Models Solving {{Markov}} Decision Processes},
  shorttitle = {{{MDPFuzz}}},
  booktitle = {Proceedings of the 31st {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Pang, Qi and Yuan, Yuanyuan and Wang, Shuai},
  date = {2022-07-18},
  series = {{{ISSTA}} 2022},
  pages = {378--390},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3533767.3534388},
  url = {https://dl.acm.org/doi/10.1145/3533767.3534388},
  urldate = {2024-09-21},
  abstract = {The Markov decision process (MDP) provides a mathematical frame- work for modeling sequential decision-making problems, many of which are crucial to security and safety, such as autonomous driving and robot control. The rapid development of artificial intelligence research has created efficient methods for solving MDPs, such as deep neural networks (DNNs), reinforcement learning (RL), and imitation learning (IL). However, these popular models solving MDPs are neither thoroughly tested nor rigorously reliable.   We present MDPFuzz, the first blackbox fuzz testing framework for models solving MDPs. MDPFuzz forms testing oracles by checking whether the target model enters abnormal and dangerous states. During fuzzing, MDPFuzz decides which mutated state to retain by measuring if it can reduce cumulative rewards or form a new state sequence. We design efficient techniques to quantify the “freshness” of a state sequence using Gaussian mixture models (GMMs) and dynamic expectation-maximization (DynEM). We also prioritize states with high potential of revealing crashes by estimating the local sensitivity of target models over states.   MDPFuzz is evaluated on five state-of-the-art models for solving MDPs, including supervised DNN, RL, IL, and multi-agent RL. Our evaluation includes scenarios of autonomous driving, aircraft collision avoidance, and two games that are often used to benchmark RL. During a 12-hour run, we find over 80 crash-triggering state sequences on each model. We show inspiring findings that crash-triggering states, though they look normal, induce distinct neuron activation patterns compared with normal states. We further develop an abnormal behavior detector to harden all the evaluated models and repair them with the findings of MDPFuzz to significantly enhance their robustness without sacrificing accuracy.},
  isbn = {978-1-4503-9379-9},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/GFHNWSHD/Pang et al. - 2022 - MDPFuzz testing models solving Markov decision processes.pdf}
}

@inproceedings{panichellaImpactTestCase2016,
  title = {The {{Impact}} of {{Test Case Summaries}} on {{Bug Fixing Performance}}: {{An Empirical Investigation}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Software Engineering}}},
  author = {Panichella, Sebastiano and Panichella, Annibale and Beller, Moritz and Zaidman, Andy and Gall, Harald C.},
  date = {2016},
  series = {{{ICSE}} '16},
  pages = {547--558},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2884781.2884847},
  url = {http://doi.acm.org/10.1145/2884781.2884847},
  isbn = {978-1-4503-3900-1},
  venue = {Austin, Texas},
  keywords = {notion,Software Testing,Test Case}
}

@inproceedings{panichellaReformulatingBranchCoverage2015,
  title = {Reformulating {{Branch Coverage}} as a {{Many-Objective Optimization Problem}}},
  booktitle = {2015 {{IEEE}} 8th {{International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}} ({{ICST}})},
  author = {Panichella, A. and Kifetew, F. M. and Tonella, P.},
  date = {2015-04},
  pages = {1--10},
  issn = {2159-4848},
  doi = {10.1109/ICST.2015.7102604},
  abstract = {Test data generation has been extensively investigated as a search problem, where the search goal is to maximize the number of covered program elements (e.g., branches). Recently, the whole suite approach, which combines the fitness functions of single branches into an aggregate, test suite-level fitness, has been demonstrated to be superior to the traditional single-branch at a time approach. In this paper, we propose to consider branch coverage directly as a many-objective optimization problem, instead of aggregating multiple objectives into a single value, as in the whole suite approach. Since programs may have hundreds of branches (objectives), traditional many-objective algorithms that are designed for numerical optimization problems with less than 15 objectives are not applicable. Hence, we introduce a novel highly scalable many-objective genetic algorithm, called MOSA (Many-Objective Sorting Algorithm), suitably defined for the many- objective branch coverage problem. Results achieved on 64 Java classes indicate that the proposed many-objective algorithm is significantly more effective and more efficient than the whole suite approach. In particular, effectiveness (coverage) was significantly improved in 66\% of the subjects and efficiency (search budget consumed) was improved in 62\% of the subjects on which effectiveness remains the same.},
  keywords = {Branch Coverage,Evolutionary Testing,Multi-objective Optimization,notion}
}

@article{paninskiEstimationEntropyMutual2003,
  title = {Estimation of {{Entropy}} and {{Mutual Information}}},
  author = {Paninski, Liam},
  date = {2003},
  journaltitle = {Neural Computation},
  volume = {15},
  pages = {1191--1253},
  keywords = {notion}
}

@article{papadakisMetallaxisFLMutationBasedFault2015,
  title = {Metallaxis-{{FL}}: {{Mutation-Based Fault Localization}}},
  author = {Papadakis, Mike and Le Traon, Yves},
  date = {2015-08},
  journaltitle = {Softw. Test. Verif. Reliab.},
  volume = {25},
  number = {5–7},
  pages = {605--628},
  publisher = {{John Wiley and Sons Ltd.}},
  location = {GBR},
  issn = {0960-0833},
  doi = {10.1002/stvr.1509},
  url = {https://doi.org/10.1002/stvr.1509},
  abstract = {Fault localization methods seek to identify faulty program statements based on the information provided by the failing and passing test executions. Spectrum-based methods are among the most popular ones and assist programmers by assigning suspiciousness values on program statements according to their probability of being faulty. This paper proposes Metallaxis, a fault localization approach based on mutation analysis. The innovative part of Metallaxis is that it uses mutants and links them with the faulty program places. Thus, mutants that are killed mostly by failing tests provide a good indication about the location of a fault. Experimentation using Metallaxis suggests that it is significantly more effective than statement-based approaches. This is true even in the case where mutation cost-reduction techniques, such as mutant sampling, are facilitated. Additionally, results from a controlled experiment show that the use of mutation as a testing technique provides benefits to the fault localization process. Therefore, fault localization is significantly improved by using mutation-based tests instead of block-based or branch-based test suites. Finally, evidence in support of the methods' scalability is also given. Copyright \textbackslash copyright 2013 John Wiley \&amp; Sons, Ltd.},
  keywords = {debugging,fault localization,mutation analysis,notion}
}

@incollection{papadakisMutationTestingAdvances2019,
  title = {Mutation Testing Advances: An Analysis and Survey},
  booktitle = {Advances in {{Computers}}},
  author = {Papadakis, Mike and Kintis, Marinos and Zhang, Jie and Jia, Yue and Le Traon, Yves and Harman, Mark},
  date = {2019},
  volume = {112},
  pages = {275--378},
  publisher = {Elsevier},
  keywords = {notion}
}

@inproceedings{papadakisThreatsValidityMutationbased2016,
  title = {Threats to the {{Validity}} of {{Mutation-based Test Assessment}}},
  booktitle = {Proceedings of the 25th {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Papadakis, Mike and Henard, Christopher and Harman, Mark and Jia, Yue and Le Traon, Yves},
  date = {2016},
  series = {{{ISSTA}} 2016},
  pages = {354--365},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2931037.2931040},
  url = {http://doi.acm.org/10.1145/2931037.2931040},
  isbn = {978-1-4503-4390-9},
  venue = {Saarbr\&\#252;cken, Germany},
  keywords = {Mutation Testing,notion,Test Assessment}
}

@inproceedings{papadakisUsingMutantsLocate2012,
  title = {Using {{Mutants}} to {{Locate}} “{{Unknown}}” {{Faults}}},
  booktitle = {2016 {{IEEE International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}} ({{ICST}})},
  author = {Papadakis, Mike and Traon, Yves Le},
  date = {2012},
  pages = {691--700},
  keywords = {Fault Localization,Mutation Testing,notion}
}

@inproceedings{parkAutomaticModelingOpaque2019,
  title = {Automatic {{Modeling}} of {{Opaque Code}} for {{JavaScript Static Analysis}}},
  booktitle = {Fundamental {{Approaches}} to {{Software Engineering}}},
  author = {Park, Joonyoung and Jordan, Alexander and Ryu, Sukyoung},
  editor = {Hähnle, Reiner and family=Aalst, given=Wil, prefix=van der, useprefix=true},
  date = {2019},
  pages = {43--60},
  publisher = {Springer International Publishing},
  location = {Cham},
  abstract = {Static program analysis often encounters problems in analyzing library code. Most real-world programs use library functions intensively, and library functions are usually written in different languages. For example, static analysis of JavaScript programs requires analysis of the standard built-in library implemented in host environments. A common approach to analyze such opaque code is for analysis developers to build models that provide the semantics of the code. Models can be built either manually, which is time consuming and error prone, or automatically, which may limit application to different languages or analyzers. In this paper, we present a novel mechanism to support automatic modeling of opaque code, which is applicable to various languages and analyzers. For a given static analysis, our approach automatically computes analysis results of opaque code via dynamic testing during static analysis. By using testing techniques, the mechanism does not guarantee sound over-approximation of program behaviors in general. However, it is fully automatic, is scalable in terms of the size of opaque code, and provides more precise results than conventional over-approximation approaches. Our evaluation shows that although not all functionalities in opaque code can (or should) be modeled automatically using our technique, a large number of JavaScript built-in functions are approximated soundly yet more precisely than existing manual models.},
  isbn = {978-3-030-16722-6},
  keywords = {notion}
}

@inproceedings{parkDebuggingNonDeadlockConcurrency2013,
  title = {Debugging {{Non-Deadlock Concurrency Bugs}}},
  booktitle = {Proceedings of the 2013 {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Park, Sangmin},
  date = {2013},
  series = {{{ISSTA}} 2013},
  pages = {358--361},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2483760.2492395},
  url = {https://doi.org/10.1145/2483760.2492395},
  abstract = {Concurrency bugs are difficult to debug because of nondeterministic behaviors of concurrent programs. Existing fault-localization techniques for non-deadlock concurrency bugs do not provide the comprehensive information to identify and understand the bugs. Existing automatic fault-fix techniques for concurrency bugs do not provide confidence that the fault has been fixed. To address the limitations of existing techniques, this research will develop techniques that will assist developers in several aspects of debugging, from finding the locations of concurrency bugs to providing confidence after the fix. The expected contributions of this research include a fault-localization technique that locates concurrency bugs, a fault-comprehension technique that assists understanding of bugs, a fix-confidence technique that provides confidence of fixes, and a framework that implements the techniques and that is used to perform empirical studies to evaluate the techniques.},
  isbn = {978-1-4503-2159-4},
  venue = {Lugano, Switzerland},
  keywords = {Concurrency,Debugging,notion}
}

@inproceedings{parkFalconFaultLocalization2010,
  title = {Falcon: {{Fault Localization}} in {{Concurrent Programs}}},
  booktitle = {Proceedings of the 32nd {{ACM}}/{{IEEE International Conference}} on {{Software Engineering}} - {{Volume}} 1},
  author = {Park, Sangmin and Vuduc, Richard W. and Harrold, Mary Jean},
  date = {2010},
  series = {{{ICSE}} '10},
  pages = {245--254},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1806799.1806838},
  url = {https://doi.org/10.1145/1806799.1806838},
  abstract = {Concurrency fault are difficult to find because they usually occur under specific thread interleavings. Fault-detection tools in this area find data-access patterns among thread interleavings, but they report benign patterns as well as actual faulty patterns. Traditional fault-localization techniques have been successful in identifying faults in sequential, deterministic programs, but they cannot detect faulty data-access patterns among threads. This paper presents a new dynamic fault-localization technique that can pinpoint faulty data-access patterns in multi-threaded concurrent programs. The technique monitors memory-access sequences among threads, detects data-access patterns associated with a program's pass/fail results, and reports dataaccess patterns with suspiciousness scores. The paper also presents the description of a prototype implementation of the technique in Java, and the results of an empirical study we performed with the prototype on several Java benchmarks. The empirical study shows that the technique can effectively and efficiently localize the faults for our subjects.},
  isbn = {978-1-60558-719-6},
  venue = {Cape Town, South Africa},
  keywords = {atomicity violation,concurrency,debugging,fault localization,notion,order violation}
}

@misc{parkGenerativeAgentsInteractive2023,
  title = {Generative {{Agents}}: {{Interactive Simulacra}} of {{Human Behavior}}},
  author = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
  date = {2023},
  keywords = {notion}
}

@inproceedings{parkJISETJavaScriptIRbased2020,
  title = {{{JISET}}: {{JavaScript IR-based Semantics Extraction Toolchain}}},
  booktitle = {2020 35th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Park, J. and Park, J. and An, S. and Ryu, S.},
  date = {2020},
  pages = {647--658},
  keywords = {notion}
}

@inproceedings{parninAreAutomatedDebugging2011,
  title = {Are {{Automated Debugging Techniques Actually Helping Programmers}}?},
  booktitle = {2011 {{International Symposium}} on {{Software Testing}} and {{Analysis}}, {{ISSTA}} 2011 - {{Proceedings}}},
  author = {Parnin, Chris and Orso, Alessandro},
  date = {2011-01},
  doi = {10.1145/2001420.2001445},
  keywords = {notion}
}

@inproceedings{partachiFlexemeUntanglingCommits2020,
  title = {Flexeme: {{Untangling Commits Using Lexical Flows}}},
  author = {family=Pârtachi, given=PP, given-i=PP and family=Dash, given=SK, given-i=SK and Allamanis, M and family=Barr, given=ET, given-i=ET},
  date = {2020},
  publisher = {Association for Computing Machinery (ACM)},
  keywords = {notion,Tangled changes}
}

@inproceedings{pasareanuSymbolicPathFinderSymbolic2010,
  title = {Symbolic {{PathFinder}}: {{Symbolic Execution}} of {{Java Bytecode}}},
  booktitle = {Proceedings of the {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Păsăreanu, Corina S. and Rungta, Neha},
  date = {2010},
  series = {{{ASE}} '10},
  pages = {179--180},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1858996.1859035},
  url = {https://doi.org/10.1145/1858996.1859035},
  abstract = {Symbolic Pathfinder (SPF) combines symbolic execution with model checking and constraint solving for automated test case generation and error detection in Java programs with unspecified inputs. In this tool, programs are executed on symbolic inputs representing multiple concrete inputs. Values of variables are represented as constraints generated from the analysis of Java bytecode. The constraints are solved using off-the shelf solvers to generate test inputs guaranteed to achieve complex coverage criteria. SPF has been used successfully at NASA, in academia, and in industry.},
  isbn = {978-1-4503-0116-9},
  venue = {Antwerp, Belgium},
  keywords = {automated test case generation,notion,program analysis}
}

@unpublished{paulusGradientEstimationStochastic2020,
  title = {Gradient Estimation with Stochastic Softmax Tricks},
  author = {Paulus, Max B and Choi, Dami and Tarlow, Daniel and Krause, Andreas and Maddison, Chris J},
  date = {2020},
  eprint = {2006.08063},
  eprinttype = {arXiv},
  keywords = {notion}
}

@inproceedings{paveseAutomatedReliabilityEstimation2013,
  title = {Automated {{Reliability Estimation}} over {{Partial Systematic Explorations}}},
  booktitle = {Proceedings of the 2013 {{International Conference}} on {{Software Engineering}}},
  author = {Pavese, Esteban and Braberman, Víctor and Uchitel, Sebastian},
  date = {2013},
  series = {{{ICSE}} '13},
  pages = {602--611},
  publisher = {IEEE Press},
  location = {San Francisco, CA, USA},
  abstract = {Model-based reliability estimation of software systems can provide useful insights early in the development process. However, computational complexity of estimating reliability metrics such as mean time to first failure (MTTF) can be prohibitive both in time, space and precision. In this paper we present an alternative to exhaustive model exploration-as in probabilistic model checking-and partial random exploration–as in statistical model checking. Our hypothesis is that a (carefully crafted) partial systematic exploration of a system model can provide better bounds for reliability metrics at lower computation cost. We present a novel automated technique for reliability estimation that combines simulation, invariant inference and probabilistic model checking. Simulation produces a probabilistically relevant set of traces from which a state invariant is inferred. The invariant characterises a partial model which is then exhaustively explored using probabilistic model checking. We report on experiments that suggest that reliability estimation using this technique can be more effective than (full model) probabilistic and statistical model checking for system models with rare failures.},
  isbn = {978-1-4673-3076-3},
  keywords = {notion}
}

@book{pearlCausality2009,
  title = {Causality},
  author = {Pearl, Judea},
  date = {2009},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9780511803161},
  keywords = {notion}
}

@article{pearlConfoundingEquivalenceCausal2014,
  title = {Confounding {{Equivalence}} in {{Causal Inference}}},
  author = {Pearl, Judea and Paz, Azaria},
  date = {2014},
  journaltitle = {Journal of Causal Inference},
  keywords = {notion}
}

@inproceedings{pearlDirectIndirectEffects2001,
  title = {Direct and {{Indirect Effects}}},
  booktitle = {Proceedings of the {{Seventeenth Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Pearl, Judea},
  date = {2001},
  series = {{{UAI}}'01},
  pages = {411--420},
  publisher = {Morgan Kaufmann Publishers Inc.},
  location = {San Francisco, CA, USA},
  isbn = {1-55860-800-1},
  venue = {Seattle, Washington},
  keywords = {notion}
}

@inproceedings{pearlTheoreticalImpedimentsMachine2018,
  title = {Theoretical {{Impediments}} to {{Machine Learning With Seven Sparks}} from the {{Causal Revolution}}},
  booktitle = {{{WSDM}} '18: {{Proceedings}} of the {{Eleventh ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  author = {Pearl, Judea},
  date = {2018-02},
  pages = {3--3},
  doi = {10.1145/3159652.3176182},
  isbn = {978-1-4503-5581-0},
  keywords = {notion}
}

@inproceedings{pearsonEvaluatingImprovingFault2017,
  title = {Evaluating and {{Improving Fault Localization}}},
  booktitle = {2017 {{IEEE}}/{{ACM}} 39th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Pearson, Spencer and Campos, José and Just, René and Fraser, Gordon and Abreu, Rui and Ernst, Michael D. and Pang, Deric and Keller, Benjamin},
  date = {2017},
  pages = {609--620},
  doi = {10.1109/ICSE.2017.62},
  keywords = {notion}
}

@inproceedings{peiDeepXploreAutomatedWhitebox2017,
  title = {{{DeepXplore}}: {{Automated Whitebox Testing}} of {{Deep Learning Systems}}},
  booktitle = {Proceedings of the 26th {{Symposium}} on {{Operating Systems Principles}}},
  author = {Pei, Kexin and Cao, Yinzhi and Yang, Junfeng and Jana, Suman},
  date = {2017},
  series = {{{SOSP}} '17},
  pages = {1--18},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/3132747.3132785},
  url = {http://doi.acm.org/10.1145/3132747.3132785},
  isbn = {978-1-4503-5085-3},
  venue = {Shanghai, China},
  keywords = {Deep learning testing,differential testing,notion,whitebox testing}
}

@article{pelletUsingMarkovBlankets2008,
  title = {Using {{Markov Blankets}} for {{Causal Structure Learning}}},
  author = {Pellet, Jean-Philippe and Elisseeff, André},
  date = {2008},
  journaltitle = {Journal of Machine Learning Research},
  volume = {9},
  number = {43},
  pages = {1295--1342},
  url = {http://jmlr.org/papers/v9/pellet08a.html},
  keywords = {notion}
}

@misc{pengHowCouldNeural2021,
  title = {How Could {{Neural Networks}} Understand {{Programs}}?},
  author = {Peng, Dinglan and Zheng, Shuxin and Li, Yatao and Ke, Guolin and He, Di and Liu, Tie-Yan},
  date = {2021},
  keywords = {notion}
}

@article{pengTFuzzFuzzingProgram2018,
  title = {T-{{Fuzz}}: {{Fuzzing}} by {{Program Transformation}}},
  author = {Peng, Hui and Shoshitaishvili, Yan and Payer, Mathias},
  date = {2018},
  journaltitle = {2018 IEEE Symposium on Security and Privacy (SP)},
  pages = {697--710},
  url = {https://api.semanticscholar.org/CorpusID:4662297},
  keywords = {notion}
}

@inproceedings{pereraDefectPredictionGuided2020,
  title = {Defect Prediction Guided Search-Based Software Testing},
  booktitle = {Proceedings of the 35th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Perera, Anjana and Aleti, Aldeida and Böhme, Marcel and Turhan, Burak},
  date = {2020},
  pages = {448--460},
  keywords = {notion}
}

@article{perez-ortizTighterRiskCertificates2021a,
  title = {Tighter {{Risk Certificates}} for {{Neural Networks}}},
  author = {Pérez-Ortiz, María and Rivasplata, Omar and Shawe-Taylor, John and Szepesvári, Csaba},
  date = {2021},
  journaltitle = {Journal of Machine Learning Research},
  volume = {22},
  number = {227},
  pages = {1--40},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v22/20-879.html},
  urldate = {2024-10-02},
  abstract = {This paper presents an empirical study regarding training probabilistic neural networks using training objectives derived from PAC-Bayes bounds. In the context of probabilistic neural networks, the output of training is a probability distribution over network weights. We present two training objectives, used here for the first time in connection with training neural networks. These two training objectives are derived from tight PAC-Bayes bounds. We also re-implement a previously used training objective based on a classical PAC-Bayes bound, to compare the properties of the predictors learned using the different training objectives. We compute risk certificates for the learnt predictors, based on part of the data used to learn the predictors. We further experiment with different types of priors on the weights (both data-free and data-dependent priors) and neural network architectures. Our experiments on MNIST and CIFAR-10 show that our training methods produce competitive test set errors and non-vacuous risk bounds with much tighter values than previous results in the literature, showing promise not only to guide the learning algorithm through bounding the risk but also for model selection. These observations suggest that the methods studied here might be good candidates for self-certified learning, in the sense of using the whole data set for learning a predictor and certifying its risk on any unseen data (from the same distribution as the training data) potentially without the need for holding out test data.},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/ZPXA7XQX/Pérez-Ortiz et al. - 2021 - Tighter Risk Certificates for Neural Networks.pdf;/Users/bohrok/Zotero/storage/PPIHRMCD/PBB.html}
}

@unpublished{perezExploitingRedundantTest2013,
  title = {Exploiting {{Redundant Test Cases}} in {{Fault Localisation}}: {{Good}} or {{Bad}}?},
  author = {Perez, Alexandre and Cardoso, Nuno and Campos, Jos ́e and Abreu, Rui},
  date = {2013-11},
  keywords = {notion}
}

@article{perezFramingProgramComprehension2016,
  title = {Framing Program Comprehension as Fault Localization},
  author = {Perez, Alexandre and Abreu, Rui},
  date = {2016},
  journaltitle = {Journal of Software: Evolution and Process},
  volume = {28},
  number = {10},
  pages = {840--862},
  publisher = {Wiley Online Library},
  keywords = {notion}
}

@article{perezTheoreticalEmpiricalAnalysis2021,
  title = {A {{Theoretical}} and {{Empirical Analysis}} of {{Program Spectra Diagnosability}}},
  author = {Perez, Alexandre and Abreu, Rui and family=Deursen, given=Arie, prefix=van, useprefix=false},
  date = {2021},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {47},
  pages = {412--431},
  keywords = {notion}
}

@misc{petersCausalInferenceUsing2015,
  title = {Causal Inference Using Invariant Prediction: Identification and Confidence Intervals},
  author = {Peters, Jonas and Bühlmann, Peter and Meinshausen, Nicolai},
  date = {2015},
  keywords = {notion}
}

@book{petersElementsCausalInference2017,
  title = {Elements of Causal Inference: Foundations and Learning Algorithms},
  author = {Peters, Jonas and Janzing, Dominik and Schölkopf, Bernhard},
  date = {2017},
  publisher = {MIT press},
  keywords = {notion}
}

@article{petkeGeneticImprovementSoftware2018,
  title = {Genetic {{Improvement}} of {{Software}}: {{A Comprehensive Survey}}},
  author = {Petke, J. and Haraldsson, S. O. and Harman, M. and Langdon, W. B. and White, D. R. and Woodward, J. R.},
  date = {2018-06},
  journaltitle = {IEEE Transactions on Evolutionary Computation},
  volume = {22},
  number = {3},
  pages = {415--432},
  issn = {1089-778X},
  doi = {10.1109/TEVC.2017.2693219},
  keywords = {computer science,core publications,dramatic performance improvements,evolutionary algorithms,genetic algorithms,Genetic improvement (GI),genetic programming,Genetic programming,GI,History,notion,Software,software engineering,Software engineering,Software genetic improvement,software maintenance,software repair,Software testing,survey,system functionality}
}

@article{petkePracticalCombinatorialInteraction2015,
  title = {Practical {{Combinatorial Interaction Testing}}: {{Empirical Findings}} on {{Efficiency}} and {{Early Fault Detection}}},
  author = {Petke, Justyna and Cohen, Myra B. and Harman, Mark and Yoo, Shin},
  date = {2015-09},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {41},
  number = {9},
  pages = {901--924},
  keywords = {Combinatorial Interaction Testing (CIT),notion,Software Testing}
}

@inproceedings{petkeSoftwareRobustnessSurvey2021,
  title = {Software {{Robustness}}: {{A Survey}}, a {{Theory}}, and {{Prospects}}},
  booktitle = {Proceedings of the 29th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Petke, Justyna and Clark, David and Langdon, William B.},
  date = {2021},
  series = {{{ESEC}}/{{FSE}} 2021},
  pages = {1475--1478},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3468264.3473133},
  url = {https://doi.org/10.1145/3468264.3473133},
  abstract = {If a software execution is disrupted, witnessing the execution at a later point may see evidence of the disruption or not. If not, we say the disruption failed to propagate. One name for this phenomenon is software robustness but it appears in different contexts in software engineering with different names. Contexts include testing, security, reliability, and automated code improvement or repair. Names include coincidental correctness, correctness attraction, transient error reliability. As witnessed, it is a dynamic phenomenon but any explanation with predictive power must necessarily take a static view. As a dynamic/static phenomenon it is convenient to take a statistical view of it which we do by way of information theory. We theorise that for failed disruption propagation to occur, a necessary condition is that the code region where the disruption occurs is composed with or succeeded by a subsequent code region that suffers entropy loss over all executions. The higher is the entropy loss, the higher the likelihood that disruption in the first region fails to propagate to the downstream observation point. We survey different research silos that address this phenomenon and explain how the theory might be exploited in software engineering.},
  isbn = {978-1-4503-8562-6},
  venue = {Athens, Greece},
  keywords = {Anti-fragile,Correctness Attraction,Failed Disruption Propagation,Genetic Improvement,Information Theory,notion,Software Robustness}
}

@article{petkeSpecialisingSoftwareDifferent2018,
  title = {Specialising {{Software}} for {{Different Downstream Applications Using Genetic Improvement}} and {{Code Transplantation}}},
  author = {Petke, J. and Harman, M. and Langdon, W. B. and Weimer, W.},
  date = {2018},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {44},
  number = {6},
  pages = {574--594},
  doi = {10.1109/TSE.2017.2702606},
  keywords = {Genetic Improvement (GI),notion,transplantation}
}

@inproceedings{phamAFLNetGreyboxFuzzer2020,
  title = {{{AFLNet}}: A Greybox Fuzzer for Network Protocols},
  booktitle = {2020 {{IEEE}} 13th {{International Conference}} on {{Software Testing}}, {{Validation}} and {{Verification}} ({{ICST}})},
  author = {Pham, Van-Thuan and Böhme, Marcel and Roychoudhury, Abhik},
  date = {2020},
  pages = {460--465},
  publisher = {IEEE},
  keywords = {notion}
}

@inproceedings{phamCertifiedContinualLearning2024,
  title = {Certified {{Continual Learning}} for {{Neural Network Regression}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Pham, Long H. and Sun, Jun},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {806--818},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3680322},
  url = {https://dl.acm.org/doi/10.1145/3650212.3680322},
  urldate = {2024-09-21},
  abstract = {On the one hand, there has been considerable progress on neural network verification in recent years, which makes certifying neural networks a possibility. On the other hand, neural networks in practice are often re-trained over time to cope with new data distribution or for solving different tasks (a.k.a. continual learning). Once re-trained, the verified correctness of the neural network is likely broken, particularly in the presence of the phenomenon known as catastrophic forgetting. In this work, we propose an approach called certified continual learning which improves existing continual learning methods by preserving, as long as possible, the established correctness properties of a verified network. Our approach is evaluated with multiple neural networks and on two different continual learning methods. The results show that our approach is efficient and the trained models preserve their certified correctness and often maintain high utility.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/PYUH4BGZ/Pham and Sun - 2024 - Certified Continual Learning for Neural Network Regression.pdf}
}

@inproceedings{phamCertifiedContinualLearning2024a,
  title = {Certified {{Continual Learning}} for {{Neural Network Regression}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Pham, Long H. and Sun, Jun},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {806--818},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3680322},
  url = {https://dl.acm.org/doi/10.1145/3650212.3680322},
  urldate = {2024-09-21},
  abstract = {On the one hand, there has been considerable progress on neural network verification in recent years, which makes certifying neural networks a possibility. On the other hand, neural networks in practice are often re-trained over time to cope with new data distribution or for solving different tasks (a.k.a. continual learning). Once re-trained, the verified correctness of the neural network is likely broken, particularly in the presence of the phenomenon known as catastrophic forgetting. In this work, we propose an approach called certified continual learning which improves existing continual learning methods by preserving, as long as possible, the established correctness properties of a verified network. Our approach is evaluated with multiple neural networks and on two different continual learning methods. The results show that our approach is efficient and the trained models preserve their certified correctness and often maintain high utility.},
  isbn = {9798400706127}
}

@article{phamSmartGreyboxFuzzing2021,
  title = {Smart {{Greybox Fuzzing}}},
  author = {Pham, Van-Thuan and Böhme, Marcel and Santosa, Andrew E. and Căciulescu, Alexandru R. and Roychoudhury, Abhik},
  date = {2021},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {47},
  number = {9},
  pages = {1980--1997},
  doi = {10.1109/TSE.2019.2941681},
  keywords = {notion}
}

@article{phanSymbolicQuantitativeInformation2012,
  title = {Symbolic {{Quantitative Information Flow}}},
  author = {Phan, Quoc-Sang and Malacaria, Pasquale and Tkachuk, Oksana and Păsăreanu, Corina S.},
  date = {2012-11},
  journaltitle = {SIGSOFT Softw. Eng. Notes},
  volume = {37},
  number = {6},
  pages = {1--5},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {0163-5948},
  doi = {10.1145/2382756.2382791},
  url = {https://doi.org/10.1145/2382756.2382791},
  abstract = {Quantitative Information Flow (QIF) is a powerful approach to quantify leaks of confidential information in a software system. Here we present a novel method that precisely quanties information leaks. In order to mitigate the state-space explosion problem, we propose a symbolic representation of data, and a general SMT-based framework to explore systematically the state space. Symbolic Execution fits well with our framework, so we implement a method of QIF analysis employing Symbolic Execution. We develop our method as a prototype tool that can perform QIF analysis for a software system developed in Java. The tool is built on top of Java Pathfinder, an open source model checking platform, and it is the first tool in the field to support information-theoretic QIF analysis.},
  keywords = {algorithms,notion,security,verification}
}

@inproceedings{phanSynthesisAdaptiveSideChannel2017,
  title = {Synthesis of {{Adaptive Side-Channel Attacks}}},
  booktitle = {2017 {{IEEE}} 30th {{Computer Security Foundations Symposium}} ({{CSF}})},
  author = {Phan, Quoc-Sang and Bang, Lucas and Pasareanu, Corina S. and Malacaria, Pasquale and Bultan, Tevfik},
  date = {2017},
  pages = {328--342},
  doi = {10.1109/CSF.2017.8},
  keywords = {notion}
}

@article{pierrotLearningCompositionalNeural2019,
  title = {Learning {{Compositional Neural Programs}} with {{Recursive Tree Search}} and {{Planning}}},
  author = {Pierrot, Thomas and Ligner, Guillaume and Reed, Scott E. and Sigaud, Olivier and Perrin, Nicolas and Laterre, Alexandre and Kas, David and Beguir, Karim and family=Freitas, given=Nando, prefix=de, useprefix=false},
  date = {2019},
  journaltitle = {CoRR},
  volume = {abs/1905.12941},
  eprint = {1905.12941},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/1905.12941},
  keywords = {notion}
}

@inproceedings{podgurskiCounterFaultValueBasedFault2020,
  title = {{{CounterFault}}: {{Value-Based Fault Localization}} by {{Modeling}} and {{Predicting Counterfactual Outcomes}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Software Maintenance}} and {{Evolution}} ({{ICSME}})},
  author = {Podgurski, A. and Küçük, Y.},
  date = {2020},
  pages = {382--393},
  doi = {10.1109/ICSME46990.2020.00044},
  keywords = {notion}
}

@book{poliFieldGuideGenetic2008,
  title = {A {{Field Guide}} to {{Genetic Programming}}},
  author = {Poli, Riccardo and Langdon, William and Mcphee, Nicholas},
  date = {2008-01},
  isbn = {978-1-4092-0073-4},
  keywords = {notion}
}

@article{politowskiSurveyVideoGame2021,
  title = {A {{Survey}} of {{Video Game Testing}}},
  author = {Politowski, Cristiano and Petrillo, Fábio and Gu'eh'eneuc, Yann-Gael},
  date = {2021},
  journaltitle = {2021 IEEE/ACM International Conference on Automation of Software Test (AST)},
  pages = {90--99},
  keywords = {notion}
}

@unpublished{poluGenerativeLanguageModeling2020,
  title = {Generative {{Language Modeling}} for {{Automated Theorem Proving}}},
  author = {Polu, Stanislas and Sutskever, Ilya},
  date = {2020},
  eprint = {2009.03393},
  eprinttype = {arXiv},
  keywords = {notion}
}

@article{polyanskiyDualizingCamsMethod2019,
  title = {Dualizing {{Le Cam}}'s Method, with Applications to Estimating the Unseens},
  author = {Polyanskiy, Yury and Wu, Yihong},
  date = {2019},
  journaltitle = {ArXiv},
  volume = {abs/1902.05616},
  keywords = {notion}
}

@inproceedings{pooleCyclicCausalModels2013,
  title = {Cyclic Causal Models with Discrete Variables: {{Markov}} Chain Equilibrium Semantics and Sample Ordering},
  booktitle = {Twenty-{{Third International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Poole, David and Crowley, Mark},
  date = {2013},
  keywords = {notion}
}

@article{pooveyBenchmarkCharacterizationEEMBC2009,
  title = {A {{Benchmark Characterization}} of the {{EEMBC Benchmark Suite}}},
  author = {Poovey, Jason A. and Conte, Thomas M. and Levy, Markus and Gal-On, Shay},
  date = {2009},
  journaltitle = {IEEE Micro},
  volume = {29},
  number = {5},
  pages = {18--29},
  doi = {10.1109/MM.2009.74},
  keywords = {notion}
}

@article{praditwongSoftwareModuleClustering2011,
  title = {Software {{Module Clustering}} as a {{Multi-Objective Search Problem}}},
  author = {Praditwong, K. and Harman, M. and Yao, X.},
  date = {2011-03},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {37},
  number = {2},
  pages = {264--282},
  issn = {0098-5589},
  doi = {10.1109/TSE.2010.26},
  abstract = {Software module clustering is the problem of automatically organizing software units into modules to improve program structure. There has been a great deal of recent interest in search-based formulations of this problem in which module boundaries are identified by automated search, guided by a fitness function that captures the twin objectives of high cohesion and low coupling in a single-objective fitness function. This paper introduces two novel multi-objective formulations of the software module clustering problem, in which several different objectives (including cohesion and coupling) are represented separately. In order to evaluate the effectiveness of the multi-objective approach, a set of experiments was performed on 17 real-world module clustering problems. The results of this empirical study provide strong evidence to support the claim that the multi-objective approach produces significantly better solutions than the existing single-objective approach.},
  keywords = {evolutionary computation.,module clustering,multi-objective optimization,multi-objective search problem,notion,optimisation,pattern clustering,program structure,SBSE,search problems,software engineering,software module clustering}
}

@article{pritzelNeuralEpisodicControl2017,
  title = {Neural {{Episodic Control}}},
  author = {Pritzel, Alexander and Uria, Benigno and Srinivasan, Sriram and Badia, Adrià Puigdomènech and Vinyals, Oriol and Hassabis, Demis and Wierstra, Daan and Blundell, Charles},
  date = {2017},
  journaltitle = {CoRR},
  volume = {abs/1703.01988},
  url = {http://arxiv.org/abs/1703.01988},
  keywords = {notion}
}

@inproceedings{qiModularizingTrainingNew2024,
  title = {Modularizing While {{Training}}: {{A New Paradigm}} for {{Modularizing DNN Models}}},
  booktitle = {Proceedings of the {{IEEE}}/{{ACM}} 46th {{International Conference}} on {{Software Engineering}}},
  author = {Qi, Binhang and Sun, Hailong and Zhang, Hongyu and Zhao, Ruobing and Gao, Xiang},
  date = {2024},
  series = {{{ICSE}} '24},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3597503.3608135},
  url = {https://doi.org/10.1145/3597503.3608135},
  abstract = {Deep neural network (DNN) models have become increasingly crucial components of intelligent software systems. However, training a DNN model is typically expensive in terms of both time and computational resources. To address this issue, recent research has focused on reusing existing DNN models - borrowing the concept of software reuse in software engineering. However, reusing an entire model could cause extra overhead or inherit the weaknesses from the undesired functionalities. Hence, existing work proposes to decompose an already trained model into modules, i.e., modularizing-after-training, to enable module reuse. Since the trained models are not built for modularization, modularizing-after-training may incur huge overhead and model accuracy loss. In this paper, we propose a novel approach that incorporates modularization into the model training process, i.e., modularizing-while-training (MwT). We train a model to be structurally modular through two loss functions that optimize intra-module cohesion and inter-module coupling. We have implemented the proposed approach for modularizing Convolutional Neural Network (CNN) models. The evaluation results on representative models demonstrate that MwT outperforms the existing state-of-the-art modularizing-after-training approach. Specifically, the accuracy loss caused by MwT is only 1.13 percentage points, which is less than that of the existing approach. The kernel retention rate of the modules generated by MwT is only 14.58\%, with a reduction of 74.31\% over the existing approach. Furthermore, the total time cost required for training and modularizing is only 108 minutes, which is half the time required by the existing approach. Our work demonstrates that MwT is a new and more effective paradigm for realizing DNN model modularization, offering a fresh perspective on achieving model reuse.},
  isbn = {9798400702174},
  venue = {{$<$}conf-loc{$>$}, {$<$}city{$>$}Lisbon{$<$}/city{$>$}, {$<$}country{$>$}Portugal{$<$}/country{$>$}, {$<$}/conf-loc{$>$}},
  keywords = {convolutional neural network,DNN modularization,model reuse,modular training,notion}
}

@article{quenouilleNotesBiasEstimation1956,
  title = {Notes on Bias in Estimation},
  author = {Quenouille, Maurice H},
  date = {1956},
  journaltitle = {Biometrika},
  volume = {43},
  number = {3/4},
  pages = {353--360},
  publisher = {JSTOR},
  keywords = {notion}
}

@article{ragkhitwetsagulComparisonCodeSimilarity2018,
  title = {A Comparison of Code Similarity Analysers},
  author = {Ragkhitwetsagul, Chaiyong and Krinke, Jens and Clark, David},
  date = {2018-08-01},
  journaltitle = {Empirical Software Engineering},
  volume = {23},
  number = {4},
  pages = {2464--2519},
  issn = {1573-7616},
  doi = {10.1007/s10664-017-9564-7},
  url = {https://doi.org/10.1007/s10664-017-9564-7},
  abstract = {Copying and pasting of source code is a common activity in software engineering. Often, the code is not copied as it is and it may be modified for various purposes; e.g. refactoring, bug fixing, or even software plagiarism. These code modifications could affect the performance of code similarity analysers including code clone and plagiarism detectors to some certain degree. We are interested in two types of code modification in this study: pervasive modifications, i.e. transformations that may have a global effect, and local modifications, i.e. code changes that are contained in a single method or code block. We evaluate 30 code similarity detection techniques and tools using five experimental scenarios for Java source code. These are (1) pervasively modified code, created with tools for source code and bytecode obfuscation, and boiler-plate code, (2) source code normalisation through compilation and decompilation using different decompilers, (3) reuse of optimal configurations over different data sets, (4) tool evaluation using ranked-based measures, and (5) local + global code modifications. Our experimental results show that in the presence of pervasive modifications, some of the general textual similarity measures can offer similar performance to specialised code similarity tools, whilst in the presence of boiler-plate code, highly specialised source code similarity detection techniques and tools outperform textual similarity measures. Our study strongly validates the use of compilation/decompilation as a normalisation technique. Its use reduced false classifications to zero for three of the tools. Moreover, we demonstrate that optimal configurations are very sensitive to a specific data set. After directly applying optimal configurations derived from one data set to another, the tools perform poorly on the new data set. The code similarity analysers are thoroughly evaluated not only based on several well-known pair-based and query-based error measures but also on each specific type of pervasive code modification. This broad, thorough study is the largest in existence and potentially an invaluable guide for future users of similarity detection in source code.},
  keywords = {notion}
}

@inproceedings{rahmanCausalDeepLearning2024,
  title = {Towards {{Causal Deep Learning}} for {{Vulnerability Detection}}},
  booktitle = {Proceedings of the {{IEEE}}/{{ACM}} 46th {{International Conference}} on {{Software Engineering}}},
  author = {Rahman, Md Mahbubur and Ceka, Ira and Mao, Chengzhi and Chakraborty, Saikat and Ray, Baishakhi and Le, Wei},
  date = {2024},
  series = {{{ICSE}} '24},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3597503.3639170},
  url = {https://doi.org/10.1145/3597503.3639170},
  abstract = {Deep learning vulnerability detection has shown promising results in recent years. However, an important challenge that still blocks it from being very useful in practice is that the model is not robust under perturbation and it cannot generalize well over the out-of-distribution (OOD) data, e.g., applying a trained model to unseen projects in real world. We hypothesize that this is because the model learned non-robust features, e.g., variable names, that have spurious correlations with labels. When the perturbed and OOD datasets no longer have the same spurious features, the model prediction fails. To address the challenge, in this paper, we introduced causality into deep learning vulnerability detection. Our approach CausalVul consists of two phases. First, we designed novel perturbations to discover spurious features that the model may use to make predictions. Second, we applied the causal learning algorithms, specifically, do-calculus, on top of existing deep learning models to systematically remove the use of spurious features and thus promote causal based prediction. Our results show that CausalVul consistently improved the model accuracy, robustness and OOD performance for all the state-of-the-art models and datasets we experimented. To the best of our knowledge, this is the first work that introduces do calculus based causal learning to software engineering models and shows it's indeed useful for improving the model accuracy, robustness and generalization. Our replication package is located at https://figshare.com/s/0ffda320dcb96c249ef2.},
  isbn = {9798400702174},
  venue = {{$<$}conf-loc{$>$}, {$<$}city{$>$}Lisbon{$<$}/city{$>$}, {$<$}country{$>$}Portugal{$<$}/country{$>$}, {$<$}/conf-loc{$>$}},
  keywords = {causality,notion,spurious features,vulnerability detection}
}

@inproceedings{rajaramanMinimaxRiskMissing2017,
  title = {Minimax Risk for Missing Mass Estimation},
  booktitle = {2017 {{IEEE International Symposium}} on {{Information Theory}} ({{ISIT}})},
  author = {Rajaraman, Nikhilesh and Thangaraj, Andrew and Suresh, Ananda Theertha},
  date = {2017},
  pages = {3025--3029},
  publisher = {IEEE},
  keywords = {notion}
}

@inproceedings{ramalingamDataFlowFrequency1996,
  title = {Data {{Flow Frequency Analysis}}},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 1996 {{Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Ramalingam, G.},
  date = {1996},
  series = {{{PLDI}} '96},
  pages = {267--277},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/231379.231433},
  url = {https://doi.org/10.1145/231379.231433},
  abstract = {Conventional dataflow analysis computes information about what facts may or will not hold during the execution of a program. Sometimes it is useful, for program optimization, to know how often or with what probability a fact holds true during program execution. In this paper, we provide a precise formulation of this problem for a large class of dataflow problems — the class of finite bi-distributive subset problems. We show how it can be reduced to a generalization of the standard dataflow analysis problem, one that requires a sum-over-all-paths quantity instead of the usual meet-overall-paths quantity. We show that Kildall's result expressing the meet-over-all-paths value as a maximal-fixed-point carries over to the generalized setting. We then outline ways to adapt the standard dataflow analysis algorithms to solve this generalized problem, both in the intraprocedural and the interprocedural case.},
  isbn = {0-89791-795-2},
  venue = {Philadelphia, Pennsylvania, USA},
  keywords = {notion}
}

@article{ramalingamDataFlowFrequency1996a,
  title = {Data {{Flow Frequency Analysis}}},
  author = {Ramalingam, G.},
  date = {1996-05},
  journaltitle = {SIGPLAN Not.},
  volume = {31},
  number = {5},
  pages = {267--277},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {0362-1340},
  doi = {10.1145/249069.231433},
  url = {https://doi.org/10.1145/249069.231433},
  abstract = {Conventional dataflow analysis computes information about what facts may or will not hold during the execution of a program. Sometimes it is useful, for program optimization, to know how often or with what probability a fact holds true during program execution. In this paper, we provide a precise formulation of this problem for a large class of dataflow problems — the class of finite bi-distributive subset problems. We show how it can be reduced to a generalization of the standard dataflow analysis problem, one that requires a sum-over-all-paths quantity instead of the usual meet-overall-paths quantity. We show that Kildall's result expressing the meet-over-all-paths value as a maximal-fixed-point carries over to the generalized setting. We then outline ways to adapt the standard dataflow analysis algorithms to solve this generalized problem, both in the intraprocedural and the interprocedural case.},
  keywords = {notion}
}

@article{ramalingamUndecidabilityAliasing1994,
  title = {The Undecidability of Aliasing},
  author = {Ramalingam, G.},
  date = {1994},
  journaltitle = {ACM Trans. Program. Lang. Syst.},
  volume = {16},
  pages = {1467--1471},
  keywords = {notion}
}

@article{ranganathSlicingConcurrentJava2007,
  title = {Slicing Concurrent {{Java}} Programs Using {{Indus}} and {{Kaveri}}},
  author = {Ranganath, Venkatesh Prasad and Hatcliff, John},
  date = {2007-10-01},
  journaltitle = {International Journal on Software Tools for Technology Transfer},
  volume = {9},
  number = {5},
  pages = {489--504},
  issn = {1433-2787},
  doi = {10.1007/s10009-007-0043-0},
  url = {http://dx.doi.org/10.1007/s10009-007-0043-0},
  abstract = {Program slicing is a program analysis and transformation technique that has been successfully used in a wide range of applications including program comprehension, debugging, maintenance, testing, and verification. However, there are only few fully featured implementations of program slicing that are available for industrial applications or academic research. In particular, very little tool support exists for slicing programs written in modern object-oriented languages such as Java, C\#, or C++. In this paper, we present Indus—a robust framework for analyzing and slicing concurrent Java programs, and Kaveri—a feature-rich Eclipse-based GUI front end for Indus slicing. For Indus, we describe the underlying tool architecture, analysis components, and program dependence capabilities required for slicing. In addition, we present a collection of advanced features useful for effective slicing of Java programs including calling-context sensitive slicing, scoped slicing, control slicing, and chopping. For Kaveri, we discuss the design goals and basic capabilities of the graphical facilities integrated into a Java development environment to present the slicing information. This paper is an extended version of a tool demonstration paper presented at the International Conference on Fundamental Aspects of Software Engineering (FASE 2005). Thus, the paper highlights tool capabilities and engineering issues and refers the reader to other papers for technical details.},
  keywords = {Indus,Java,Kaveri,notion,Program Slicing}
}

@article{rantanenDiscoveringCausalGraphs2020,
  title = {Discovering Causal Graphs with Cycles and Latent Confounders: An Exact Branch-and-Bound Approach},
  author = {Rantanen, Kari and Hyttinen, Antti and Järvisalo, Matti},
  date = {2020},
  journaltitle = {International Journal of Approximate Reasoning},
  volume = {117},
  pages = {29--49},
  publisher = {Elsevier},
  keywords = {causal discovery,notion}
}

@inproceedings{rantanenLearningOptimalCausal2018,
  title = {Learning Optimal Causal Graphs with Exact Search},
  booktitle = {International {{Conference}} on {{Probabilistic Graphical Models}}},
  author = {Rantanen, Kari and Hyttinen, Antti and Järvisalo, Matti},
  date = {2018},
  pages = {344--355},
  publisher = {PMLR},
  keywords = {causal discovery,notion}
}

@inproceedings{rantanenLearningOptimalCyclic2020,
  title = {Learning {{Optimal Cyclic Causal Graphs}} from {{Interventional Data}}},
  author = {Rantanen, Kari and Hyttinen, Antti and Järvisalo, Matti},
  date = {2020},
  keywords = {causal discovery,notion}
}

@inproceedings{raoImpactsTestSuites2013,
  title = {Impacts of {{Test Suite}}'s {{Class Imbalance}} on {{Spectrum-Based Fault Localization Techniques}}},
  booktitle = {2013 13th {{International Conference}} on {{Quality Software}}},
  author = {Rao, P. and Zheng, Z. and Chen, T. Y. and Wang, N. and Cai, K.},
  date = {2013},
  pages = {260--267},
  doi = {10.1109/QSIC.2013.18},
  keywords = {notion}
}

@inproceedings{raoRetrievalSoftwareLibraries2011,
  title = {Retrieval from {{Software Libraries}} for {{Bug Localization}}: {{A Comparative Study}} of {{Generic}} and {{Composite Text Models}}},
  booktitle = {Proceedings of the 8th {{Working Conference}} on {{Mining Software Repositories}}},
  author = {Rao, Shivani and Kak, Avinash},
  date = {2011},
  series = {{{MSR}} '11},
  pages = {43--52},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/1985441.1985451},
  url = {http://doi.acm.org/10.1145/1985441.1985451},
  isbn = {978-1-4503-0574-7},
  venue = {Waikiki, Honolulu, HI, USA},
  keywords = {bug localization,information retrieval,latent dirichlet allocation,latent semantic analysis,notion,software engineering}
}

@inproceedings{rathTraceabilityWildAutomatically2018,
  title = {Traceability in the {{Wild}}: {{Automatically Augmenting Incomplete Trace Links}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Software Engineering}}},
  author = {Rath, Michael and Rendall, Jacob and Guo, Jin L. C. and Cleland-Huang, Jane and Mäder, Patrick},
  date = {2018},
  series = {{{ICSE}} '18},
  pages = {834--845},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3180155.3180207},
  url = {https://doi.org/10.1145/3180155.3180207},
  abstract = {Software and systems traceability is widely accepted as an essential element for supporting many software development tasks. Today's version control systems provide inbuilt features that allow developers to tag each commit with one or more issue ID, thereby providing the building blocks from which project-wide traceability can be established between feature requests, bug fixes, commits, source code, and specific developers. However, our analysis of six open source projects showed that on average only 60\% of the commits were linked to specific issues. Without these fundamental links the entire set of project-wide links will be incomplete, and therefore not trustworthy. In this paper we address the fundamental problem of missing links between commits and issues. Our approach leverages a combination of process and text-related features characterizing issues and code changes to train a classifier to identify missing issue tags in commit messages, thereby generating the missing links. We conducted a series of experiments to evaluate our approach against six open source projects and showed that it was able to effectively recommend links for tagging issues at an average of 96\% recall and 33\% precision. In a related task for augmenting a set of existing trace links, the classifier returned precision at levels greater than 89\% in all projects and recall of 50\%.},
  isbn = {978-1-4503-5638-1},
  venue = {Gothenburg, Sweden},
  keywords = {link recovery,machine learning,notion,open source,traceability}
}

@inproceedings{rawadabouassiSubstateProfilingEnhanced2020,
  title = {Substate {{Profiling}} for {{Enhanced Fault Detection}} and {{Localization}}: {{An Empirical Study}}},
  booktitle = {{{IEEE}} 13th {{International Conference}} on {{Software Testing}}, {{Validation}} and {{Verification}} ({{ICST}})},
  author = {Rawad Abou Assi and Wes Masri and Trad, Chadi},
  date = {2020},
  publisher = {IEEE},
  keywords = {notion}
}

@inproceedings{rawatVUzzerApplicationawareEvolutionary2017,
  title = {{{VUzzer}}: {{Application-aware Evolutionary Fuzzing}}.},
  booktitle = {{{NDSS}}},
  author = {Rawat, Sanjay and Jain, Vivek and Kumar, Ashish and Cojocar, Lucian and Giuffrida, Cristiano and Bos, Herbert},
  date = {2017},
  volume = {17},
  pages = {1--14},
  keywords = {notion}
}

@inproceedings{rayNaturalnessBuggyCode2016,
  title = {On the "{{Naturalness}}" of {{Buggy Code}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Software Engineering}}},
  author = {Ray, Baishakhi and Hellendoorn, Vincent and Godhane, Saheel and Tu, Zhaopeng and Bacchelli, Alberto and Devanbu, Premkumar},
  date = {2016},
  series = {{{ICSE}} '16},
  pages = {428--439},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2884781.2884848},
  url = {https://doi.org/10.1145/2884781.2884848},
  abstract = {Real software, the kind working programmers produce by the kLOC to solve real-world problems, tends to be "natural", like speech or natural language; it tends to be highly repetitive and predictable. Researchers have captured this naturalness of software through statistical models and used them to good effect in suggestion engines, porting tools, coding standards checkers, and idiom miners. This suggests that code that appears improbable, or surprising, to a good statistical language model is "unnatural" in some sense, and thus possibly suspicious. In this paper, we investigate this hypothesis. We consider a large corpus of bug fix commits (ca. 7,139), from 10 different Java projects, and focus on its language statistics, evaluating the naturalness of buggy code and the corresponding fixes. We find that code with bugs tends to be more entropic (i.e. unnatural), becoming less so as bugs are fixed. Ordering files for inspection by their average entropy yields cost-effectiveness scores comparable to popular defect prediction methods. At a finer granularity, focusing on highly entropic lines is similar in cost-effectiveness to some well-known static bug finders (PMD, FindBugs) and ordering warnings from these bug finders using an entropy measure improves the cost-effectiveness of inspecting code implicated in warnings. This suggests that entropy may be a valid, simple way to complement the effectiveness of PMD or FindBugs, and that search-based bug-fixing methods may benefit from using entropy both for fault-localization and searching for fixes.},
  isbn = {978-1-4503-3900-1},
  venue = {Austin, Texas},
  keywords = {notion}
}

@report{rebertSecureDesignGoogles2024,
  title = {Secure by {{Design}}: {{Google}}'s {{Perspective}} on {{Memory Safety}}},
  author = {Rebert, Alex and Kern, Christoph},
  date = {2024},
  institution = {Google Security Engineering},
  keywords = {notion}
}

@article{rebuffiICaRLIncrementalClassifier2016,
  title = {{{iCaRL}}: {{Incremental Classifier}} and {{Representation Learning}}},
  author = {Rebuffi, Sylvestre-Alvise and Kolesnikov, Alexander and Lampert, Christoph H.},
  date = {2016},
  journaltitle = {CoRR},
  volume = {abs/1611.07725},
  url = {http://arxiv.org/abs/1611.07725},
  keywords = {notion}
}

@inproceedings{reedNeuralProgrammerInterpreters2016,
  title = {Neural {{Programmer-Interpreters}}},
  booktitle = {4th {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2016, {{San Juan}}, {{Puerto Rico}}, {{May}} 2-4, 2016, {{Conference Track Proceedings}}},
  author = {Reed, Scott E. and family=Freitas, given=Nando, prefix=de, useprefix=false},
  editor = {Bengio, Yoshua and LeCun, Yann},
  date = {2016},
  url = {http://arxiv.org/abs/1511.06279},
  keywords = {notion}
}

@inproceedings{regehrTestcaseReductionCompiler2012,
  title = {Test-Case {{Reduction}} for {{C Compiler Bugs}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Regehr, John and Chen, Yang and Cuoq, Pascal and Eide, Eric and Ellison, Chucky and Yang, Xuejun},
  date = {2012},
  series = {{{PLDI}} '12},
  pages = {335--346},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2254064.2254104},
  url = {http://doi.acm.org/10.1145/2254064.2254104},
  isbn = {978-1-4503-1205-9},
  venue = {Beijing, China},
  keywords = {automated testing,bug reporting,compiler defect,compiler testing,notion,random testing,test-case minimization}
}

@inproceedings{renRootCauseLocalization2019,
  title = {Root {{Cause Localization}} for {{Unreproducible Builds}} via {{Causality Analysis Over System Call Tracing}}},
  booktitle = {2019 34th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Ren, Z. and Liu, C. and Xiao, X. and Jiang, H. and Xie, T.},
  date = {2019-11},
  pages = {527--538},
  issn = {1938-4300},
  doi = {10.1109/ASE.2019.00056},
  keywords = {localization,notion,system call tracing,unreproducible builds}
}

@article{repsProgramAnalysisGraph1997,
  title = {Program Analysis via Graph Reachability},
  author = {Reps, T.},
  date = {1997},
  journaltitle = {Inf. Softw. Technol.},
  volume = {40},
  pages = {701--726},
  keywords = {notion}
}

@article{richardsonCharacterizationMarkovEquivalence1997,
  title = {A Characterization of {{Markov}} Equivalence for Directed Cyclic Graphs},
  author = {Richardson, Thomas},
  date = {1997},
  journaltitle = {International Journal of Approximate Reasoning},
  volume = {17},
  number = {2},
  pages = {107--162},
  issn = {0888-613X},
  doi = {10.1016/S0888-613X(97)00020-0},
  url = {http://www.sciencedirect.com/science/article/pii/S0888613X97000200},
  abstract = {The concept of d-separation (Pearl, 1988) was originally defined for acyclic directed graphs, but there is a natural extension of the concept to directed graphs with cycles. When exactly the same set of d-separation relations hold in two directed graphs, no matter whether respectively cyclic or acyclic, we say that they are markov equivalent. In other words, when two directed cyclic graphs are Markov equivalent, the set of distributions that satisfy a natural extension of the global directed Markov condition (Lauritzen et al., 1990) is exactly the same for each graph. There is an obvious exponential (in the number of vertices) time algorithm for deciding Markov equivalence of two directed cyclic graphs: simply check all of the d-separation relations in each graph. In this paper I prove a theorem that gives necessary and sufficient conditions for two directed cyclic graphs to be Markov equivalent, where each of the conditions can be checked in polynomial time. Hence, the theorem can be easily adapted into a polynomial time algorithm for deciding the Markov equivalence of two directed cyclic graphs (Richardson, 1996).},
  keywords = {conditional independence,d-separation,DAG,DCG,directed cyclic graphical model,Markov equivalence,non-recursive structural equation model,notion,SEM}
}

@inproceedings{richardsonDiscoveryAlgorithmDirected1996,
  title = {A {{Discovery Algorithm}} for {{Directed Cyclic Graphs}}},
  booktitle = {Proceedings of the {{Twelfth International Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Richardson, Thomas},
  date = {1996},
  series = {{{UAI}}'96},
  pages = {454--461},
  publisher = {Morgan Kaufmann Publishers Inc.},
  location = {San Francisco, CA, USA},
  isbn = {1-55860-412-X},
  venue = {Portland, OR},
  keywords = {notion}
}

@article{rieckSallyToolEmbedding2012,
  title = {Sally: {{A Tool}} for {{Embedding Strings}} in {{Vector Spaces}}},
  author = {Rieck, Konrad and Wressnegger, Christian and Bikadorov, Alexander},
  date = {2012-11},
  journaltitle = {Journal of Machine Learning Research (JMLR)},
  volume = {13},
  pages = {3247--3251},
  keywords = {notion}
}

@inproceedings{rivasplataPACBayesAnalysisUsual2020,
  title = {{{PAC-Bayes Analysis Beyond}} the {{Usual Bounds}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Rivasplata, Omar and Kuzborskij, Ilja and Szepesvari, Csaba and Shawe-Taylor, John},
  date = {2020},
  volume = {33},
  pages = {16833--16845},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2020/hash/c3992e9a68c5ae12bd18488bc579b30d-Abstract.html},
  urldate = {2024-09-20},
  abstract = {We focus on a stochastic learning model where the learner observes a finite set of training examples and the output of the learning process is a data-dependent distribution over a space of hypotheses. The learned data-dependent distribution is then used to make randomized predictions, and the high-level theme addressed here is guaranteeing the quality of predictions on examples that were not seen during training, i.e. generalization. In this setting the unknown quantity of interest is the expected risk of the data-dependent randomized predictor, for which upper bounds can be derived via a PAC-Bayes analysis, leading to PAC-Bayes bounds.},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/RGHQXRSF/Rivasplata et al. - 2020 - PAC-Bayes Analysis Beyond the Usual Bounds.pdf}
}

@article{robillardEmpiricalStudyConcept2007,
  title = {An Empirical Study of the Concept Assignment Problem},
  author = {Robillard, Martin P and Shepherd, David and Hill, Emily and Vijay-Shanker, K and Pollock, Lori},
  date = {2007},
  journaltitle = {School of Computer Science, McGill University, Tech. Rep. SOCS-TR-2007.3},
  keywords = {notion}
}

@article{robillardTopologyAnalysisSoftware2008,
  title = {Topology {{Analysis}} of {{Software Dependencies}}},
  author = {Robillard, Martin P.},
  date = {2008-08},
  journaltitle = {ACM Trans. Softw. Eng. Methodol.},
  volume = {17},
  number = {4},
  pages = {18:1--18:36},
  publisher = {ACM},
  location = {New York, NY, USA},
  issn = {1049-331X},
  doi = {10.1145/13487689.13487691},
  url = {http://doi.acm.org/10.1145/13487689.13487691},
  keywords = {feature location,notion,program understanding,separation of concerns,software change,Software evolution,software navigation,static analysis}
}

@inproceedings{rodriguez-cancioImagesCodeLossy2018,
  title = {Images of {{Code}}: {{Lossy Compression}} for {{Native Instructions}}},
  booktitle = {{{NIER}} Track at {{ICSE}}},
  author = {Rodriguez-Cancio, Marcelino and Baudry, Benoit and White, Jules},
  date = {2018},
  keywords = {notion}
}

@inproceedings{romanelliEstimatingGLeakageMachine2020,
  title = {Estimating G-{{Leakage}} via {{Machine Learning}}},
  booktitle = {Proceedings of the 2020 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Romanelli, Marco and Chatzikokolakis, Konstantinos and Palamidessi, Catuscia and Piantanida, Pablo},
  date = {2020},
  series = {{{CCS}} '20},
  pages = {697--716},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3372297.3423363},
  url = {https://doi.org/10.1145/3372297.3423363},
  abstract = {This paper considers the problem of estimating the information leakage of a system in the black-box scenario, i.e. when the system's internals are unknown to the learner, or too complicated to analyze, and the only available information are pairs of input-output data samples, obtained by submitting queries to the system or provided by a third party. The frequentist approach relies on counting the frequencies to estimate the input-output conditional probabilities, however this method is not accurate when the domain of possible outputs is large. To overcome this difficulty, the estimation of the Bayes error of the ideal classifier was recently investigated using Machine Learning (ML) models, and it has been shown to be more accurate thanks to the ability of those models to learn the input-output correspondence. However, the Bayes vulnerability is only suitable to describe one-try attacks. A more general and flexible measure of leakage is the g-vulnerability, which encompasses several different types of adversaries, with different goals and capabilities. We propose a novel approach to perform black-box estimation of the g-vulnerability using ML which does not require to estimate the conditional probabilities and is suitable for a large class of ML algorithms. First, we formally show the learnability for all data distributions. Then, we evaluate the performance via various experiments using k-Nearest Neighbors and Neural Networks. Our approach outperform the frequentist one when the observables domain is large.},
  isbn = {978-1-4503-7089-9},
  venue = {Virtual Event, USA},
  keywords = {g-vulnerability estimation,machine learning,neural networks,notion}
}

@inproceedings{romanoEmpiricalAnalysisUIbased2021,
  title = {An {{Empirical Analysis}} of {{UI-based Flaky Tests}}},
  booktitle = {2021 {{IEEE}}/{{ACM}} 43rd {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Romano, Alan and Song, Zihe and Grandhi, Sampath and Yang, Wei and Wang, Weihang},
  date = {2021},
  pages = {1585--1597},
  doi = {10.1109/ICSE43902.2021.00141},
  keywords = {notion}
}

@article{rosenbaumCentralRolePropensity1983,
  title = {The Central Role of the Propensity Score in Observational Studies for Causal Effects},
  author = {ROSENBAUM, PAUL R. and RUBIN, DONALD B.},
  date = {1983-04},
  journaltitle = {Biometrika},
  volume = {70},
  number = {1},
  pages = {41--55},
  issn = {0006-3444},
  doi = {10.1093/biomet/70.1.41},
  url = {https://doi.org/10.1093/biomet/70.1.41},
  abstract = {The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subclassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a two- dimensional plot.},
  keywords = {notion}
}

@inproceedings{rosenbergSpectrumBasedLogDiagnosis2020,
  title = {Spectrum-{{Based Log Diagnosis}}},
  booktitle = {Proceedings of the 14th {{ACM}} / {{IEEE International Symposium}} on {{Empirical Software Engineering}} and {{Measurement}} ({{ESEM}})},
  author = {Rosenberg, Carl Martin and Moonen, Leon},
  date = {2020},
  series = {{{ESEM}} '20},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3382494.3410684},
  url = {https://doi.org/10.1145/3382494.3410684},
  abstract = {Background: Continuous Engineering practices are increasingly adopted in modern software development. However, a frequently reported need is for more effective methods to analyze the massive amounts of data resulting from the numerous build and test runs. Aims: We present and evaluate Spectrum-Based Log Diagnosis (SBLD), a method to help developers quickly diagnose problems found in complex integration and deployment runs. Inspired by Spectrum-Based Fault Localization, SBLD leverages the differences in event occurrences between logs for failing and passing runs, to highlight events that are stronger associated with failing runs.Method: Using data provided by Cisco Norway, we empirically investigate the following questions: (i) How well does SBLD reduce the effort needed to identify all failure-relevant events in the log for a failing run? (ii) How is the performance of SBLD affected by available data? (iii) How does SBLD compare to searching for simple textual patterns that often occur in failure-relevant events? We answer (i) and (ii) using summary statistics and heatmap visualizations, and for (iii) we compare three configurations of SBLD (with resp. minimum, median and maximum data) against a textual search using Wilcoxon signed-rank tests and the Vargha-Delaney measure of stochastic superiority.Results: Our evaluation shows that (i) SBLD achieves a significant effort reduction for the dataset used, (ii) SBLD benefits from additional logs for passing runs in general, and it benefits from additional logs for failing runs when there is a proportional amount of logs for passing runs in the data. Finally, (iii) SBLD and textual search are roughly equally effective at effort-reduction, while textual search has slightly better recall. We investigate the cause, and discuss how it is due to characteristics of a specific part of our data.Conclusions: We conclude that SBLD shows promise as a method for diagnosing failing runs, that its performance is positively affected by additional data, but that it does not outperform textual search on the dataset considered. Future work includes investigating SBLD's generalizability on additional datasets.},
  isbn = {978-1-4503-7580-1},
  venue = {Bari, Italy},
  keywords = {continuous engineering,failure diagnosis,log analysis,log mining,notion}
}

@article{rothermelPrioritizingTestCases2001,
  title = {Prioritizing Test Cases for Regression Testing},
  author = {Rothermel, G. and Untch, R. H. and {Chengyun Chu} and Harrold, M. J.},
  date = {2001},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {27},
  number = {10},
  pages = {929--948},
  doi = {10.1109/32.962562},
  keywords = {notion}
}

@inproceedings{roySpreadsheetTestingPractice2017,
  title = {Spreadsheet Testing in Practice},
  booktitle = {2017 {{IEEE}} 24th {{International Conference}} on {{Software Analysis}}, {{Evolution}} and {{Reengineering}} ({{SANER}})},
  author = {Roy, Sohon and Hermans, Felienne and family=Deursen, given=Arie, prefix=van, useprefix=true},
  date = {2017},
  pages = {338--348},
  doi = {10.1109/SANER.2017.7884634},
  keywords = {notion}
}

@inproceedings{ryderIncrementalDataFlow1983,
  title = {Incremental Data Flow Analysis},
  booktitle = {{{ACM-SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Ryder, Barbara G.},
  date = {1983},
  url = {https://api.semanticscholar.org/CorpusID:14841441},
  keywords = {notion}
}

@inproceedings{sadowskiModernCodeReview2018,
  title = {Modern {{Code Review}}: {{A Case Study}} at {{Google}}},
  booktitle = {International {{Conference}} on {{Software Engineering}}, {{Software Engineering}} in {{Practice}} Track ({{ICSE SEIP}})},
  author = {Sadowski, Caitlin and Söderberg, Emma and Church, Luke and Sipko, Michal and Bacchelli, Alberto},
  date = {2018},
  keywords = {notion}
}

@article{sahaAttackSynthesisStrings2019,
  title = {Attack {{Synthesis}} for {{Strings}} Using {{Meta-Heuristics}}},
  author = {Saha, Seemanta and Kadron, Ismet Burak and Eiers, William and Bang, Lucas and Bultan, Tevfik},
  date = {2019},
  journaltitle = {ACM SIGSOFT Softw. Eng. Notes},
  volume = {43},
  pages = {56},
  url = {https://api.semanticscholar.org/CorpusID:57757299},
  keywords = {notion}
}

@inproceedings{sahaElixirEffectiveObjectoriented2017,
  title = {Elixir: {{Effective}} Object-Oriented Program Repair},
  booktitle = {2017 32nd {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Saha, R. K. and Lyu, Y. and Yoshida, H. and Prasad, M. R.},
  date = {2017-10},
  pages = {648--659},
  doi = {10.1109/ASE.2017.8115675},
  keywords = {-validate repair technique,bug report,called ELIXIR,Computer bugs,Concrete,concrete repairs,correctly repaired bugs,dataset Bugs.jar,expressive repair-expressions,generate- validate repair technique,HD-Repair,Java,learning (artificial intelligence),machine-learnt model,Maintenance engineering,notion,Object oriented modeling,object-oriented program repair,object-oriented programming,OO-program bugs,PAR,popular Defects4J dataset,potential repair location,program context,program debugging,repair space,Software,state-of-the-art repair techniques,synthesizing patches,Tools}
}

@inproceedings{sahaImprovingBugLocalization2013,
  title = {Improving Bug Localization Using Structured Information Retrieval},
  booktitle = {2013 28th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Saha, R. K. and Lease, M. and Khurshid, S. and Perry, D. E.},
  date = {2013-11},
  pages = {345--355},
  doi = {10.1109/ASE.2013.6693093},
  abstract = {Locating bugs is important, difficult, and expensive, particularly for large-scale systems. To address this, natural language information retrieval techniques are increasingly being used to suggest potential faulty source files given bug reports. While these techniques are very scalable, in practice their effectiveness remains low in accurately localizing bugs to a small number of files. Our key insight is that structured information retrieval based on code constructs, such as class and method names, enables more accurate bug localization. We present BLUiR, which embodies this insight, requires only the source code and bug reports, and takes advantage of bug similarity data if available. We build BLUiR on a proven, open source IR toolkit that anyone can use. Our work provides a thorough grounding of IR-based bug localization research in fundamental IR theoretical and empirical knowledge and practice. We evaluate BLUiR on four open source projects with approximately 3,400 bugs. Results show that BLUiR matches or outperforms a current state-of-the-art tool across applications considered, even when BLUiR does not use bug similarity data used by the other tool.},
  keywords = {Accuracy,BLUiR,bug localization,Bug localization,bug reports,bug similarity data,code constructs,Computer bugs,Indexing,information retrieval,Information retrieval,Java,large-scale systems,Mathematical model,Measurement,natural language information retrieval,natural language processing,notion,open source IR toolkit,program debugging,public domain software,search,source code,structured information retrieval}
}

@article{sahaObtainingInformationLeakage2023,
  title = {Obtaining {{Information Leakage Bounds}} via {{Approximate Model Counting}}},
  author = {Saha, Seemanta and Ghentiyala, Surendra and Lu, Shihua and Bang, Lucas and Bultan, Tevfik},
  date = {2023-06},
  journaltitle = {Proc. ACM Program. Lang.},
  volume = {7},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3591281},
  url = {https://doi.org/10.1145/3591281},
  abstract = {Information leaks are a significant problem in modern software systems. In recent years, information theoretic concepts, such as Shannon entropy, have been applied to quantifying information leaks in programs. One recent approach is to use symbolic execution together with model counting constraints solvers in order to quantify information leakage. There are at least two reasons for unsoundness in quantifying information leakage using this approach: 1) Symbolic execution may not be able to explore all execution paths, 2) Model counting constraints solvers may not be able to provide an exact count. We present a sound symbolic quantitative information flow analysis that bounds the information leakage both for the cases where the program behavior is not fully explored and the model counting constraint solver is unable to provide a precise model count but provides an upper and a lower bound. We implemented our approach as an extension to KLEE for computing sound bounds for information leakage in C programs.},
  issue = {PLDI},
  keywords = {Information Leakage,Model Counting,notion,Optimization,Quantitative Program Analysis,Symbolic Quantitative Information Flow Analysis}
}

@inproceedings{sahaPReachHeuristicProbabilistic2022,
  title = {{{PReach}}: {{A Heuristic}} for {{Probabilistic Reachability}} to {{Identify Hard}} to {{Reach Statements}}},
  booktitle = {International {{Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Saha, Seemanta and Downing, Mara and Brennan, Tegan and Bultan, Tevfik},
  date = {2022},
  doi = {10.1145/3510003.3510227},
  keywords = {notion}
}

@software{sahaPReachProbabilisticReachability2022,
  title = {{{PReach}}: {{A}} Probabilistic Reachability Analyzer to Identify Hard to Reach Program Statements},
  author = {Saha, Seemanta},
  date = {2022-01},
  doi = {10.5281/zenodo.5915206},
  url = {https://doi.org/10.5281/zenodo.5915206},
  organization = {Zenodo},
  version = {1.0.0},
  keywords = {notion}
}

@inproceedings{sahaRarePathGuided2023,
  title = {Rare {{Path Guided Fuzzing}}},
  booktitle = {Proceedings of the 32nd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Saha, Seemanta and Sarker, Laboni and Shafiuzzaman, Md and Shou, Chaofan and Li, Albert and Sankaran, Ganesh and Bultan, Tevfik},
  date = {2023},
  series = {{{ISSTA}} 2023},
  pages = {1295--1306},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3597926.3598136},
  url = {https://doi.org/10.1145/3597926.3598136},
  abstract = {Starting with a random initial seed, fuzzers search for inputs that trigger bugs or vulnerabilities. However, fuzzers often fail to generate inputs for program paths guarded by restrictive branch conditions. In this paper, we show that by first identifying rare-paths in programs (i.e., program paths with path constraints that are unlikely to be satisfied by random input generation), and then, generating inputs/seeds that trigger rare-paths, one can improve the coverage of fuzzing tools. In particular, we present techniques 1) that identify rare paths using quantitative symbolic analysis, and 2) generate inputs that can explore these rare paths using path-guided concolic execution. We provide these inputs as initial seed sets to three state of the art fuzzers. Our experimental evaluation on a set of programs shows that the fuzzers achieve better coverage with the rare-path based seed set compared to a random initial seed.},
  isbn = {9798400702211},
  venue = {Seattle, WA, USA},
  keywords = {Concolic execution,Control flow analysis,Fuzz testing,Model counting,notion,Probabilistic analysis}
}

@inproceedings{sakibVariationsExtensionsInformation2023,
  title = {Variations and {{Extensions}} of {{Information Leakage Metrics}} with {{Applications}} to {{Privacy Problems}} with {{Imperfect Statistical Information}}},
  booktitle = {2023 {{IEEE}} 36th {{Computer Security Foundations Symposium}} ({{CSF}})},
  author = {Sakib, S. and Amariucai, G. T. and Guan, Y.},
  date = {2023-07},
  pages = {407--422},
  publisher = {IEEE Computer Society},
  location = {Los Alamitos, CA, USA},
  doi = {10.1109/CSF57540.2023.00007},
  url = {https://doi.ieeecomputersociety.org/10.1109/CSF57540.2023.00007},
  abstract = {The conventional information leakage metrics assume that an adversary has complete knowledge of the distribution of the mechanism used to disclose information correlated with the sensitive attributes of a system. The only uncertainty arises from the specific realizations that are drawn from this distribution. This assumption does not hold in various practical scenarios where an adversary usually lacks complete information about the joint statistics of the private, utility, and the disclosed data. As a result, the typical information leakage metrics fail to measure the leakage appropriately. In this paper, we introduce multiple new versions of the traditional information-theoretic leakage metrics, that aptly represent information leakage for an adversary who lacks complete knowledge of the joint data statistics, and we provide insights into the potential uses of each. We experiment on a real-world dataset to further demonstrate how the introduced leakage metrics compare with the conventional notions of leakage. Finally, we show how privacy-utility optimization problems can be formulated in this context, such that their solutions result in the optimal information disclosure mechanisms, for various applications.},
  keywords = {computer security,differential privacy,information theory,measurement,notion,optimization,privacy,uncertainty}
}

@article{saltonVectorSpaceModel1975,
  title = {A {{Vector Space Model}} for {{Automatic Indexing}}},
  author = {Salton, G. and Wong, A. and Yang, C. S.},
  date = {1975-11},
  journaltitle = {Commun. ACM},
  volume = {18},
  number = {11},
  pages = {613--620},
  publisher = {ACM},
  location = {New York, NY, USA},
  issn = {0001-0782},
  doi = {10.1145/361219.361220},
  url = {http://doi.acm.org/10.1145/361219.361220},
  keywords = {Automated Indexing,notion,Vector Space Model(VSM)}
}

@inproceedings{samhiCallGraphSoundness2024,
  title = {Call {{Graph Soundness}} in {{Android Static Analysis}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Samhi, Jordan and Just, René and Bissyandé, Tegawendé F. and Ernst, Michael D. and Klein, Jacques},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {945--957},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3680333},
  url = {https://dl.acm.org/doi/10.1145/3650212.3680333},
  urldate = {2024-09-21},
  abstract = {Static analysis is sound in theory, but an implementation may unsoundly fail to analyze all of a program's code. Any such omission is a serious threat to the validity of the tool's output. Our work is the first to measure the prevalence of these omissions. Previously, researchers and analysts did not know what is missed by static analysis, what sort of code is missed, or the reasons behind these omissions. To address this gap, we ran 13static analysis tools and a dynamic analysis on 1000 Android apps. Any method in the dynamic analysis but not in a static analysis is an unsoundness.       Our findings include the following. (1) Apps built around external frameworks challenge static analyzers. On average, the 13 static analysis tools failed to capture 61\% of the dynamically-executed methods. (2) A high level of precision in call graph construction is a synonym for a high level of unsoundness. (3) No existing approach significantly improves static analysis soundness. This includes those specifically tailored for a given mechanism, such as DroidRA to address reflection. It also includes systematic approaches, such as EdgeMiner, capturing all callbacks in the Android framework systematically. (4) Modeling entry point methods challenges call graph construction which jeopardizes soundness.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/NCUQP28R/Samhi et al. - 2024 - Call Graph Soundness in Android Static Analysis.pdf}
}

@inproceedings{samhiCallGraphSoundness2024a,
  title = {Call {{Graph Soundness}} in {{Android Static Analysis}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Samhi, Jordan and Just, René and Bissyandé, Tegawendé F. and Ernst, Michael D. and Klein, Jacques},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {945--957},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3680333},
  url = {https://dl.acm.org/doi/10.1145/3650212.3680333},
  urldate = {2024-09-21},
  abstract = {Static analysis is sound in theory, but an implementation may unsoundly fail to analyze all of a program's code. Any such omission is a serious threat to the validity of the tool's output. Our work is the first to measure the prevalence of these omissions. Previously, researchers and analysts did not know what is missed by static analysis, what sort of code is missed, or the reasons behind these omissions. To address this gap, we ran 13static analysis tools and a dynamic analysis on 1000 Android apps. Any method in the dynamic analysis but not in a static analysis is an unsoundness.       Our findings include the following. (1) Apps built around external frameworks challenge static analyzers. On average, the 13 static analysis tools failed to capture 61\% of the dynamically-executed methods. (2) A high level of precision in call graph construction is a synonym for a high level of unsoundness. (3) No existing approach significantly improves static analysis soundness. This includes those specifically tailored for a given mechanism, such as DroidRA to address reflection. It also includes systematic approaches, such as EdgeMiner, capturing all callbacks in the Android framework systematically. (4) Modeling entry point methods challenges call graph construction which jeopardizes soundness.},
  isbn = {9798400706127}
}

@inproceedings{samhiCallGraphSoundness2024d,
  title = {Call {{Graph Soundness}} in {{Android Static Analysis}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Samhi, Jordan and Just, René and Bissyandé, Tegawendé F. and Ernst, Michael D. and Klein, Jacques},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {945--957},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3680333},
  url = {https://dl.acm.org/doi/10.1145/3650212.3680333},
  urldate = {2024-09-21},
  abstract = {Static analysis is sound in theory, but an implementation may unsoundly fail to analyze all of a program's code. Any such omission is a serious threat to the validity of the tool's output. Our work is the first to measure the prevalence of these omissions. Previously, researchers and analysts did not know what is missed by static analysis, what sort of code is missed, or the reasons behind these omissions. To address this gap, we ran 13static analysis tools and a dynamic analysis on 1000 Android apps. Any method in the dynamic analysis but not in a static analysis is an unsoundness.       Our findings include the following. (1) Apps built around external frameworks challenge static analyzers. On average, the 13 static analysis tools failed to capture 61\% of the dynamically-executed methods. (2) A high level of precision in call graph construction is a synonym for a high level of unsoundness. (3) No existing approach significantly improves static analysis soundness. This includes those specifically tailored for a given mechanism, such as DroidRA to address reflection. It also includes systematic approaches, such as EdgeMiner, capturing all callbacks in the Android framework systematically. (4) Modeling entry point methods challenges call graph construction which jeopardizes soundness.},
  isbn = {9798400706127}
}

@unpublished{sanchezDiffusionModelsCausal2022,
  title = {Diffusion {{Models}} for {{Causal Discovery}} via {{Topological Ordering}}},
  author = {Sanchez, Pedro and Liu, Xiao and O'Neil, Alison Q and Tsaftaris, Sotirios A},
  date = {2022},
  eprint = {2210.06201},
  eprinttype = {arXiv},
  keywords = {notion}
}

@inproceedings{santelicesQuantitativeProgramSlicing2013,
  title = {Quantitative {{Program Slicing}}: {{Separating Statements}} by {{Relevance}}},
  booktitle = {Proceedings of the 2013 {{International Conference}} on {{Software Engineering}}},
  author = {Santelices, Raul and Zhang, Yiji and Jiang, Siyuan and Cai, Haipeng and Zhang, Ying-Jie},
  date = {2013},
  series = {{{ICSE}} '13},
  pages = {1269--1272},
  publisher = {IEEE Press},
  location = {Piscataway, NJ, USA},
  url = {http://dl.acm.org/citation.cfm?id=2486788.2486981},
  isbn = {978-1-4673-3076-3},
  venue = {San Francisco, CA, USA},
  keywords = {notion,Program Slicing}
}

@inproceedings{santinelliSustainabilityExtremeValue2014,
  title = {On the {{Sustainability}} of the {{Extreme Value Theory}} for {{WCET Estimation}}},
  booktitle = {{{WCET}}},
  author = {Santinelli, Luca and Morio, Jérôme and Dufour, Guillaume and Jacquemart-Tomi, Damien},
  date = {2014},
  keywords = {notion}
}

@article{schaulPrioritizedExperienceReplay2015,
  title = {Prioritized {{Experience Replay}}},
  author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  date = {2015},
  journaltitle = {CoRR},
  volume = {abs/1511.05952},
  url = {http://arxiv.org/abs/1511.05952},
  keywords = {notion}
}

@misc{schmidtModelingDiscreteInterventional2012,
  title = {Modeling {{Discrete Interventional Data}} Using {{Directed Cyclic Graphical Models}}},
  author = {Schmidt, Mark and Murphy, Kevin},
  date = {2012},
  keywords = {notion}
}

@misc{schoelkopfCausalAnticausalLearning2012,
  title = {On {{Causal}} and {{Anticausal Learning}}},
  author = {Schoelkopf, Bernhard and Janzing, Dominik and Peters, Jonas and Sgouritsa, Eleni and Zhang, Kun and Mooij, Joris},
  date = {2012},
  keywords = {notion}
}

@unpublished{scholkopfCausalityMachineLearning2019,
  title = {Causality for Machine Learning},
  author = {Schölkopf, Bernhard},
  date = {2019},
  eprint = {1911.10500},
  eprinttype = {arXiv},
  keywords = {notion}
}

@article{scholkopfCausalRepresentationLearning2021,
  title = {Toward {{Causal Representation Learning}}},
  author = {Schölkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
  date = {2021},
  journaltitle = {Proceedings of the IEEE},
  volume = {109},
  number = {5},
  pages = {612--634},
  doi = {10.1109/JPROC.2021.3058954},
  keywords = {notion}
}

@inproceedings{schrettnerImpactAnalysisPresence2012,
  title = {Impact {{Analysis}} in the {{Presence}} of {{Dependence Clusters Using Static Execute}} after in {{WebKit}}},
  booktitle = {2012 {{IEEE}} 12th {{International Working Conference}} on {{Source Code Analysis}} and {{Manipulation}}},
  author = {Schrettner, L. and Jász, J. and Gergely, T. and Beszédes, Á and Gyimóthy, T.},
  date = {2012-09},
  pages = {24--33},
  issn = {null},
  doi = {10.1109/SCAM.2012.22},
  keywords = {Algorithm design and analysis,Approximation methods,Change impact analysis,code dependence,dependence clusters,Dependence clusters,impact analysis,Internet,notion,Regression testing,SEA,Sea measurements,Software algorithms,software quality,software quality assurance,software system,Software systems,Source code analysis,static execute after,Static Execute After,Testing,WebKit}
}

@inproceedings{schroterStackTracesHelp2010,
  title = {Do Stack Traces Help Developers Fix Bugs?},
  booktitle = {2010 7th {{IEEE Working Conference}} on {{Mining Software Repositories}} ({{MSR}} 2010)},
  author = {Schroter, A. and Schröter, A. and Bettenburg, N. and Premraj, R.},
  date = {2010-05},
  pages = {118--121},
  issn = {2160-1852},
  doi = {10.1109/MSR.2010.5463280},
  abstract = {A widely shared belief in the software engineering community is that stack traces are much sought after by developers to support them in debugging. But limited empirical evidence is available to confirm the value of stack traces to developers. In this paper, we seek to provide such evidence by conducting an empirical study on the usage of stack traces by developers from the ECLIPSE project. Our results provide strong evidence to this effect and also throws light on some of the patterns in bug fixing using stack traces. We expect the findings of our study to further emphasize the importance of adding stack traces to bug reports and that in the future, software vendors will provide more support in their products to help general users make such information available when filing bug reports.},
  keywords = {bug fixing,bug tracking,collaboration,Collaboration,Computer bugs,Computer crashes,debugging,Documentation,ECLIPSE project,empirical study,Java,Needles,notion,Open source software,program debugging,Programming profession,Software debugging,software engineering,Software engineering,stack traces}
}

@inproceedings{scottUnderstandingSimpleAsynchronous2015,
  title = {Understanding {{Simple Asynchronous Evolutionary Algorithms}}},
  booktitle = {Proceedings of the 2015 {{ACM Conference}} on {{Foundations}} of {{Genetic Algorithms XIII}}},
  author = {Scott, Eric O. and De Jong, Kenneth A.},
  date = {2015},
  series = {{{FOGA}} '15},
  pages = {85--98},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2725494.2725509},
  url = {http://doi.acm.org/10.1145/2725494.2725509},
  isbn = {978-1-4503-3434-1},
  venue = {Aberystwyth, United Kingdom},
  keywords = {asynchronous algorithms,evolutionary algorithms,notion,parallel algorithms}
}

@article{scutariLearningBayesianNetworks2019,
  title = {Learning {{Bayesian}} Networks from Big Data with Greedy Search: Computational Complexity and Efficient Implementation},
  author = {Scutari, Marco and Vitolo, Claudia and Tucker, Allan},
  date = {2019-09-01},
  journaltitle = {Statistics and Computing},
  volume = {29},
  number = {5},
  pages = {1095--1108},
  doi = {10.1007/s11222-019-09857-1},
  url = {https://doi.org/10.1007/s11222-019-09857-1},
  abstract = {Learning the structure of Bayesian networks from data is known to be a computationally challenging, NP-hard problem. The literature has long investigated how to perform structure learning from data containing large numbers of variables, following a general interest in high-dimensional applications (“small n, large p”) in systems biology and genetics. More recently, data sets with large numbers of observations (the so-called “big data”) have become increasingly common; and these data sets are not necessarily high-dimensional, sometimes having only a few tens of variables depending on the application. We revisit the computational complexity of Bayesian network structure learning in this setting, showing that the common choice of measuring it with the number of estimated local distributions leads to unrealistic time complexity estimates for the most common class of score-based algorithms, greedy search. We then derive more accurate expressions under common distributional assumptions. These expressions suggest that the speed of Bayesian network learning can be improved by taking advantage of the availability of closed-form estimators for local distributions with few parents. Furthermore, we find that using predictive instead of in-sample goodness-of-fit scores improves speed; and we confirm that it improves the accuracy of network reconstruction as well, as previously observed by Chickering and Heckerman (Stat Comput 10: 55–62, 2000). We demonstrate these results on large real-world environmental and epidemiological data; and on reference data sets available from public repositories.},
  isbn = {1573-1375},
  keywords = {notion}
}

@inproceedings{senStatisticalModelChecking2004,
  title = {Statistical {{Model Checking}} of {{Black-Box Probabilistic Systems}}},
  booktitle = {International {{Conference}} on {{Computer Aided Verification}}},
  author = {Sen, Koushik and Viswanathan, Mahesh and Agha, Gul A.},
  date = {2004},
  keywords = {notion}
}

@inproceedings{serebryanyAddressSanitizerFastAddress2012,
  title = {{{AddressSanitizer}}: {{A Fast Address Sanity Checker}}},
  booktitle = {{{USENIX Annual Technical Conference}}},
  author = {Serebryany, Kostya and Bruening, Derek and Potapenko, Alexander and Vyukov, Dmitriy},
  date = {2012},
  keywords = {notion}
}

@misc{serebryanyGWPASanSamplingBasedDetection2023,
  title = {{{GWP-ASan}}: {{Sampling-Based Detection}} of {{Memory-Safety Bugs}} in {{Production}}},
  author = {Serebryany, Kostya and Kennelly, Chris and Phillips, Mitch and Denton, Matt and Elver, Marco and Potapenko, Alexander and Morehouse, Matt and Tsyrklevich, Vlad and Holler, Christian and Lettner, Julian and Kilzer, David and Brandt, Lander},
  date = {2023},
  keywords = {notion}
}

@inproceedings{shahMC2RigorousEfficient2022,
  title = {{{MC2}}: {{Rigorous}} and {{Efficient Directed Greybox Fuzzing}}},
  booktitle = {Proceedings of the 2022 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Shah, Abhishek and She, Dongdong and Sadhu, Samanway and Singal, Krish and Coffman, Peter and Jana, Suman},
  date = {2022},
  series = {{{CCS}} '22},
  pages = {2595--2609},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3548606.3560648},
  url = {https://doi.org/10.1145/3548606.3560648},
  abstract = {Directed greybox fuzzing is a popular technique for targeted software testing that seeks to find inputs that reach a set of target sites in a program. Most existing directed greybox fuzzers do not provide any theoretical analysis of their performance or optimality.In this paper, we introduce a complexity-theoretic framework to pose directed greybox fuzzing as an oracle-guided search problem where some feedback about the input space (e.g., how close an input is to the target sites) is received by querying an oracle. Our framework assumes that each oracle query can return arbitrary content with a large but constant amount of information. Therefore, we use the number of oracle queries required by a fuzzing algorithm to find a target-reaching input as the performance metric. Using our framework, we design a randomized directed greybox fuzzing algorithm that makes a logarithmic (wrt. the number of all possible inputs) number of queries in expectation to find a target-reaching input. We further prove that the number of oracle queries required by our algorithm is optimal, i.e., no fuzzing algorithm can improve (i.e., minimize) the query count by more than a constant factor.We implement our approach in MC\textasciicircum 2 and outperform state-of-the-art directed greybox fuzzers on challenging benchmarks (Magma and Fuzzer Test Suite) by up to two orders of magnitude (i.e., 134times) on average. MC\textasciicircum 2 also found 15 previously undiscovered bugs that other state-of-the-art directed greybox fuzzers failed to find.},
  isbn = {978-1-4503-9450-5},
  venue = {Los Angeles, CA, USA},
  keywords = {automated vulnerability detection,execution complexity,greybox fuzzing,monte carlo counting,noisy binary search,notion}
}

@inproceedings{shahoorPreservingReactivenessUnderstanding2024,
  title = {Preserving {{Reactiveness}}: {{Understanding}} and {{Improving}} the {{Debugging Practice}} of {{Blocking-Call Bugs}}},
  shorttitle = {Preserving {{Reactiveness}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Shahoor, Arooba and Yi, Jooyong and Kim, Dongsun},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {768--780},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3680319},
  url = {https://dl.acm.org/doi/10.1145/3650212.3680319},
  urldate = {2024-09-21},
  abstract = {Reactive programming reacts to data items as they occur, rather than waiting for them to complete. This programming paradigm is widely used in asynchronous and event-driven scenarios, such as web applications, microservices, real-time data processing, IoT, interactive UIs, and big data. When done right, it can offer greater responsiveness without extra resource usage. However, this also requires a thorough understanding of asynchronous and non-blocking coding, posing a learning curve for developers new to this style of programming. In this work, we analyze issues reported in reactive applications and explore their corresponding fixes. Our investigation results reveal that (1) developers often do not fix or ignore reactiveness bugs as compared to other bug types, and (2) this tendency is most pronounced for blocking-call bugs -- bugs that block the execution of the program to wait for the operations (typically I/O operations) to finish, wasting CPU and memory resources. To improve the debugging practice of such blocking bugs, we develop a pattern-based proactive program repair technique and obtain 30 patches, which we submit to the developers. In addition, we hypothesize that the low patch acceptance rate for reactiveness bugs is due to the difficulty of assessing the patches. This is in contrast to functionality bugs, where the correctness of the patches can be assessed by running test cases. To assess our hypothesis, we split our patches into two groups: one with performance improvement evidence and the other without. It turns out that the patches are more likely to be accepted when submitted with performance improvement evidence.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/5Z4YU6KS/Shahoor et al. - 2024 - Preserving Reactiveness Understanding and Improving the Debugging Practice of Blocking-Call Bugs.pdf}
}

@article{shalev-shwartzFailuresGradientBasedDeep2017,
  title = {Failures of {{Gradient-Based Deep Learning}}},
  author = {Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  date = {2017-03},
  url = {https://arxiv.org/abs/1703.07950},
  abstract = {In recent years, Deep Learning has become the go-to solution for a broad range of applications, often outperforming state-of-the-art. However, it is important, for both theoreticians and practitioners, to gain a deeper understanding of the difficulties and limitations associated with common approaches and algorithms. We describe four types of simple problems, for which the gradient-based algorithms commonly used in deep learning either fail or suffer from significant difficulties. We illustrate the failures through practical experiments, and provide theoretical insights explaining their source, and how they might be remedied.},
  keywords = {Deep Learning,notion}
}

@article{shannonMathematicalTheoryCommunication1948,
  title = {A Mathematical Theory of Communication},
  author = {Shannon, Claude Elwood},
  date = {1948},
  journaltitle = {The Bell system technical journal},
  volume = {27},
  number = {3},
  pages = {379--423},
  publisher = {Nokia Bell Labs},
  keywords = {notion}
}

@inproceedings{shariffdeenAutomatedPatchBackporting2021,
  title = {Automated {{Patch Backporting}} in {{Linux}} ({{Experience Paper}})},
  booktitle = {Proceedings of the 30th {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Shariffdeen, Ridwan and Gao, Xiang and Duck, Gregory J. and Tan, Shin Hwei and Lawall, Julia and Roychoudhury, Abhik},
  date = {2021},
  series = {{{ISSTA}} 2021},
  pages = {633--645},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3460319.3464821},
  url = {https://doi.org/10.1145/3460319.3464821},
  abstract = {Whenever a bug or vulnerability is detected in the Linux kernel, the kernel developers will endeavour to fix it by introducing a patch into the mainline version of the Linux kernel source tree. However, many users run older “stable” versions of Linux, meaning that the patch should also be “backported” to one or more of these older kernel versions. This process is error-prone and there is usually along delay in publishing the backported patch. Based on an empirical study, we show that around 8\% of all commits submitted to Linux mainline are backported to older versions,but often more than one month elapses before the backport is available. Hence, we propose a patch backporting technique that can automatically transfer patches from the mainline version of Linux into older stable versions. Our approach first synthesizes a partial transformation rule based on a Linux mainline patch. This rule can then be generalized by analysing the alignment between the mainline and target versions. The generalized rule is then applied to the target version to produce a backported patch. We have implemented our transformation technique in a tool called FixMorph and evaluated it on 350 Linux mainline patches. FixMorph correctly backports 75.1\% of them. Compared to existing techniques, FixMorph improves both the precision and recall in backporting patches. Apart from automation of software maintenance tasks, patch backporting helps in reducing the exposure to known security vulnerabilities in stable versions of the Linux kernel.},
  isbn = {978-1-4503-8459-9},
  venue = {Virtual, Denmark},
  keywords = {Linux Kernel,notion,Patch Backporting,Program Transformation}
}

@article{shariffdeenAutomatedPatchTransplantation2021,
  title = {Automated {{Patch Transplantation}}},
  author = {Shariffdeen, Ridwan Salihin and Tan, Shin Hwei and Gao, Mingyuan and Roychoudhury, Abhik},
  date = {2021-12},
  journaltitle = {ACM Trans. Softw. Eng. Methodol.},
  volume = {30},
  number = {1},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {1049-331X},
  doi = {10.1145/3412376},
  url = {https://doi.org/10.1145/3412376},
  abstract = {Automated program repair is an emerging area that attempts to patch software errors and vulnerabilities. In this article, we formulate and study a problem related to automated repair, namely automated patch transplantation. A patch for an error in a donor program is automatically adapted and inserted into a “similar” target program. We observe that despite standard procedures for vulnerability disclosures and publishing of patches, many un-patched occurrences remain in the wild. One of the main reasons is the fact that various implementations of the same functionality may exist and, hence, published patches need to be modified and adapted. In this article, we therefore propose and implement a workflow for transplanting patches. Our approach centers on identifying patch insertion points, as well as namespaces translation across programs via symbolic execution. Experimental results to eliminate five classes of errors highlight our ability to fix recurring vulnerabilities across various programs through transplantation. We report that in 20 of 24 fixing tasks involving eight application subjects mostly involving file processing programs, we successfully transplanted the patch and validated the transplantation through differential testing. Since the publication of patches make an un-patched implementation more vulnerable, our proposed techniques should serve a long-standing need in practice.},
  keywords = {code transplantation,dynamic program analysis,notion,patch transplantation,Program repair}
}

@inproceedings{shariffdeenConcolicProgramRepair2021,
  title = {Concolic {{Program Repair}}},
  booktitle = {Proceedings of the 42nd {{ACM SIGPLAN International Conference}} on {{Programming Language Design}} and {{Implementation}} ({{PLDI}} '21)},
  author = {Shariffdeen, Ridwan Salihin and Noller, Yannic and Grunske, Lars and Roychoudhury, Abhik},
  date = {2021},
  keywords = {notion}
}

@inproceedings{shenEnhancingROSSystem2024,
  title = {Enhancing {{ROS System Fuzzing}} through {{Callback Tracing}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Shen, Yuheng and Liu, Jianzhong and Xu, Yiru and Sun, Hao and Wang, Mingzhe and Guan, Nan and Shi, Heyuan and Jiang, Yu},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {76--87},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3652111},
  url = {https://dl.acm.org/doi/10.1145/3650212.3652111},
  urldate = {2024-09-21},
  abstract = {The Robot Operating System 2 (ROS) is the de-facto standard for robotic software development, with a wide application in diverse safety-critical domains. There are many efforts in testing that seek to deliver a more secure ROS codebase. However, existing testing methods are often inadequate to capture the complex and stateful behaviors inherent to ROS deployments, resulting in limited test- ing effectiveness. In this paper, we propose R2D2, a ROS system fuzzer that leverages ROS’s runtime states as guidance to increase fuzzing effectiveness and efficiency. Unlike traditional fuzzers, R2D2 employs a systematic instrumentation strategy that captures the system’s runtime behaviors and profiles the current system state in real-time. This approach provides a more in-depth understanding of system behaviors, thereby facilitating a more insightful explo- ration of ROS’s extensive state space. For evaluation, we applied it to four well-known ROS applications. Our evaluation shows that R2D2 achieves an improvement of 3.91× and 2.56× in code coverage compared to state-of-the-art ROS fuzzers, including Ros2Fuzz and RoboFuzz, while also uncovering 39 previously unknown vulnera- bilities, with 6 fixed in both ROS runtime and ROS applications. For its runtime overhead, R2D2 maintains an average execution and memory usage overhead with 10.4\% and 1.0\% in respect, making R2D2 effective in ROS testing.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/4UZBUG6U/Shen et al. - 2024 - Enhancing ROS System Fuzzing through Callback Tracing.pdf}
}

@inproceedings{sheNeuzzEfficientFuzzing2019,
  title = {Neuzz: {{Efficient}} Fuzzing with Neural Program Smoothing},
  booktitle = {2019 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  author = {She, Dongdong and Pei, Kexin and Epstein, Dave and Yang, Junfeng and Ray, Baishakhi and Jana, Suman},
  date = {2019},
  pages = {803--817},
  publisher = {IEEE},
  keywords = {notion}
}

@article{shenPredictingNumberNew2003,
  title = {Predicting the Number of New Species in Further Taxonomic Sampling},
  author = {Shen, Tsung-Jen and Chao, Anne and Lin, Chih-Feng},
  date = {2003},
  journaltitle = {Ecology},
  volume = {84},
  number = {3},
  pages = {798--804},
  publisher = {Wiley Online Library},
  keywords = {notion}
}

@inproceedings{shenSmartCommitGraphbasedInteractive2021,
  title = {{{SmartCommit}}: A Graph-Based Interactive Assistant for Activity-Oriented Commits},
  booktitle = {Proceedings of the 29th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Shen, Bo and Zhang, Wei and Kästner, Christian and Zhao, Haiyan and Wei, Zhao and Liang, Guangtai and Jin, Zhi},
  date = {2021},
  pages = {379--390},
  keywords = {notion}
}

@inproceedings{shiBetterNotTogether2024,
  title = {Better {{Not Together}}: {{Staged Solving}} for {{Context-Free Language Reachability}}},
  shorttitle = {Better {{Not Together}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Shi, Chenghang and Li, Haofeng and Lu, Jie and Li, Lian},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {1112--1123},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3680346},
  url = {https://dl.acm.org/doi/10.1145/3650212.3680346},
  urldate = {2024-09-21},
  abstract = {Context-free language reachability (CFL-reachability) is a fundamental formulation for program analysis with many applications. CFL-reachability analysis is computationally expensive, with a slightly subcubic time complexity concerning the number of nodes in the input graph.             This paper proposes staged solving: a new perspective on solving CFL-reachability. Our key observation is that the context-free grammar (CFG) of a CFL-based program analysis can be decomposed into (1) a smaller CFG, L, for matching parentheses, such as procedure calls/returns, field stores/loads, and (2) a regular grammar, R, capturing control/data flows. Instead of solving these two parts monolithically (as in standard algorithms), staged solving solves L-reachability and R-reachability in two distinct stages. In practice, L-reachability, though still context-free, involves only a small subset of edges, while R-reachability can be computed efficiently with close to quadratic complexity relative to the node size of the input graph. We implement our staged CFL-reachability solver, STG, and evaluate it using two clients: context-sensitive value-flow analysis and field-sensitive alias analysis. The empirical results demonstrate that STG achieves speedups of 861.59x and 4.1x for value-flow analysis and alias analysis on average, respectively, over the standard subcubic algorithm. Moreover, we also showcase that staged solving can help to significantly improve the performance of two state-of-the-art solvers, POCR and PEARL, by 74.82x (1.78x) and 37.66x (1.7x) for value-flow (alias) analysis, respectively.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/WJSY9KKQ/Shi et al. - 2024 - Better Not Together Staged Solving for Context-Free Language Reachability.pdf}
}

@misc{shiCASTEnhancingCode2021,
  title = {{{CAST}}: {{Enhancing Code Summarization}} with {{Hierarchical Splitting}} and {{Reconstruction}} of {{Abstract Syntax Trees}}},
  author = {Shi, Ensheng and Wang, Yanlin and Du, Lun and Zhang, Hongyu and Han, Shi and Zhang, Dongmei and Sun, Hongbin},
  date = {2021},
  keywords = {notion}
}

@article{shiDetectingAssumptionsDeterministic2016,
  title = {Detecting {{Assumptions}} on {{Deterministic Implementations}} of {{Non-deterministic Specifications}}},
  author = {Shi, August and Gyori, Alex and Legunsen, Owolabi and Marinov, Darko},
  date = {2016},
  journaltitle = {2016 IEEE International Conference on Software Testing, Verification and Validation (ICST)},
  pages = {80--90},
  keywords = {notion}
}

@thesis{shinExtendingBoundariesRegression2009,
  type = {phdthesis},
  title = {Extending the {{Boundaries}} in {{Regression Testing}}: {{Complexity}}, {{Latency}}, and {{Expertise}}},
  author = {Shin, Yoo},
  date = {2009},
  institution = {PhD thesis, King's College London},
  keywords = {notion}
}

@online{shinRetrievalAugmentedTestGeneration2024,
  title = {Retrieval-{{Augmented Test Generation}}: {{How Far Are We}}?},
  shorttitle = {Retrieval-{{Augmented Test Generation}}},
  author = {Shin, Jiho and Aleithan, Reem and Hemmati, Hadi and Wang, Song},
  date = {2024-09-19},
  eprint = {2409.12682},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2409.12682},
  url = {http://arxiv.org/abs/2409.12682},
  urldate = {2024-09-21},
  abstract = {Retrieval Augmented Generation (RAG) has shown notable advancements in software engineering tasks. Despite its potential, RAG's application in unit test generation remains under-explored. To bridge this gap, we take the initiative to investigate the efficacy of RAG-based LLMs in test generation. As RAGs can leverage various knowledge sources to enhance their performance, we also explore the impact of different sources of RAGs' knowledge bases on unit test generation to provide insights into their practical benefits and limitations. Specifically, we examine RAG built upon three types of domain knowledge: 1) API documentation, 2) GitHub issues, and 3) StackOverflow Q\&As. Each source offers essential knowledge for creating tests from different perspectives, i.e., API documentations provide official API usage guidelines, GitHub issues offer resolutions of issues related to the APIs from the library developers, and StackOverflow Q\&As present community-driven solutions and best practices. For our experiment, we focus on five widely used and typical Python-based machine learning (ML) projects, i.e., TensorFlow, PyTorch, Scikit-learn, Google JAX, and XGBoost to build, train, and deploy complex neural networks efficiently. We conducted experiments using the top 10\% most widely used APIs across these projects, involving a total of 188 APIs. We investigate the effectiveness of four state-of-the-art LLMs (open and closed-sourced), i.e., GPT-3.5-Turbo, GPT-4o, Mistral MoE 8x22B, and Llamma 3.1 405B. Additionally, we compare three prompting strategies in generating unit test cases for the experimental APIs, i.e., zero-shot, a Basic RAG, and an API-level RAG on the three external sources. Finally, we compare the cost of different sources of knowledge used for the RAG.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering,notion},
  file = {/Users/bohrok/Zotero/storage/KT76RE4C/Shin et al. - 2024 - Retrieval-Augmented Test Generation How Far Are We.pdf;/Users/bohrok/Zotero/storage/BNKX4WPP/2409.html}
}

@inproceedings{shokriProtectingLocationPrivacy2012,
  title = {Protecting Location Privacy: Optimal Strategy against Localization Attacks},
  booktitle = {Proceedings of the 2012 {{ACM Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Shokri, Reza and Theodorakopoulos, George and Troncoso, Carmela and Hubaux, Jean-Pierre and Le Boudec, Jean-Yves},
  date = {2012},
  series = {{{CCS}} '12},
  pages = {617--627},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2382196.2382261},
  url = {https://doi.org/10.1145/2382196.2382261},
  abstract = {The mainstream approach to protecting the location-privacy of mobile users in location-based services (LBSs) is to alter the users' actual locations in order to reduce the location information exposed to the service provider. The location obfuscation algorithm behind an effective location-privacy preserving mechanism (LPPM) must consider three fundamental elements: the privacy requirements of the users, the adversary's knowledge and capabilities, and the maximal tolerated service quality degradation stemming from the obfuscation of true locations. We propose the first methodology, to the best of our knowledge, that enables a designer to find the optimal LPPM for a LBS given each user's service quality constraints against an adversary implementing the optimal inference algorithm. Such LPPM is the one that maximizes the expected distortion (error) that the optimal adversary incurs in reconstructing the actual location of a user, while fulfilling the user's service-quality requirement. We formalize the mutual optimization of user-adversary objectives (location privacy vs. correctness of localization) by using the framework of Stackelberg Bayesian games. In such setting, we develop two linear programs that output the best LPPM strategy and its corresponding optimal inference attack. Our optimal user-centric LPPM can be easily integrated in the users' mobile devices they use to access LBSs. We validate the efficacy of our game theoretic method against real location traces. Our evaluation confirms that the optimal LPPM strategy is superior to a straightforward obfuscation method, and that the optimal localization attack performs better compared to a Bayesian inference attack.},
  isbn = {978-1-4503-1651-4},
  venue = {Raleigh, North Carolina, USA},
  keywords = {location inference attacks,location privacy,location-based services,notion,optimal defense strategy,privacy protection,service quality,stackelberg bayesian games}
}

@inproceedings{shpitserValidityCovariateAdjustment2010,
  title = {On the {{Validity}} of {{Covariate Adjustment}} for {{Estimating Causal Effects}}},
  booktitle = {Proceedings of the {{Twenty-Sixth Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Shpitser, Ilya and VanderWeele, Tyler and Robins, James M.},
  date = {2010},
  series = {{{UAI}}'10},
  pages = {527--536},
  publisher = {AUAI Press},
  location = {Arlington, Virginia, USA},
  abstract = {Identifying effects of actions (treatments) on outcome variables from observational data and causal assumptions is a fundamental problem in causal inference. This identification is made difficult by the presence of con-founders which can be related to both treatment and outcome variables. Confounders are often handled, both in theory and in practice, by adjusting for covariates, in other words considering outcomes conditioned on treatment and covariate values, weighed by probability of observing those covariate values. In this paper, we give a complete graphical criterion for covariate adjustment, which we term the adjustment criterion, and derive some interesting corollaries of the completeness of this criterion.},
  isbn = {978-0-9749039-6-5},
  venue = {Catalina Island, CA},
  keywords = {notion}
}

@inproceedings{shuMFLMethodLevelFault2013,
  title = {{{MFL}}: {{Method-Level Fault Localization}} with {{Causal Inference}}},
  booktitle = {2013 {{IEEE Sixth International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}}},
  author = {Shu, G. and Sun, B. and Podgurski, A. and Cao, F.},
  date = {2013-03},
  pages = {124--133},
  issn = {2159-4848},
  doi = {10.1109/ICST.2013.31},
  keywords = {Algorithm design and analysis,causal graph,causal inference,causal inference techniques,conditional exchangeability,confounder selection,confounder selection algorithm,confounding bias,Debugging,dynamic call graph,dynamic data dependences,Equations,Heuristic algorithms,Inference algorithms,method-level fault localization,MFL,notion,observational data,positivity,positivity properties,program statements,Random variables,Software,software developers,software fault tolerance,statement-level causal SFL,statistical analysis,statistical debugging,statistical fault localization,test suites}
}

@thesis{shuStatisticalEstimationSoftware2014,
  type = {phdthesis},
  title = {Statistical Estimation of Software Reliability and Failure-Causing Effect},
  author = {Shu, Gang},
  date = {2014},
  institution = {Case Western Reserve University},
  keywords = {notion}
}

@inproceedings{sidiroglou-douskosAutomaticErrorElimination2015,
  title = {Automatic {{Error Elimination}} by {{Horizontal Code Transfer}} across {{Multiple Applications}}},
  booktitle = {Proceedings of the 36th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Sidiroglou-Douskos, Stelios and Lahtinen, Eric and Long, Fan and Rinard, Martin},
  date = {2015},
  series = {{{PLDI}} '15},
  pages = {43--54},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2737924.2737988},
  url = {https://doi.org/10.1145/2737924.2737988},
  abstract = {We present Code Phage (CP), a system for automatically transferring correct code from donor applications into recipient applications that process the same inputs to successfully eliminate errors in the recipient. Experimental results using seven donor applications to eliminate ten errors in seven recipient applications highlight the ability of CP to transfer code across applications to eliminate out of bounds access, integer overflow, and divide by zero errors. Because CP works with binary donors with no need for source code or symbolic information, it supports a wide range of use cases. To the best of our knowledge, CP is the first system to automatically transfer code across multiple applications.},
  isbn = {978-1-4503-3468-6},
  venue = {Portland, OR, USA},
  keywords = {automatic code transfer,data structure translation,notion,program repair}
}

@inproceedings{sidiroglou-douskosCodeCarbonCopy2017,
  title = {{{CodeCarbonCopy}}},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Sidiroglou-Douskos, Stelios and Lahtinen, Eric and Eden, Anthony and Long, Fan and Rinard, Martin},
  date = {2017},
  series = {{{ESEC}}/{{FSE}} 2017},
  pages = {95--105},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3106237.3106269},
  url = {https://doi.org/10.1145/3106237.3106269},
  abstract = {We present CodeCarbonCopy (CCC), a system for transferring code from a donor application into a recipient application. CCC starts with functionality identified by the developer to transfer into an insertion point (again identified by the developer) in the recipient. CCC uses paired executions of the donor and recipient on the same input file to obtain a translation between the data representation and name space of the recipient and the data representation and name space of the donor. It also implements a static analysis that identifies and removes irrelevant functionality useful in the donor but not in the recipient. We evaluate CCC on eight transfers between six applications. Our results show that CCC can successfully transfer donor functionality into recipient applications.},
  isbn = {978-1-4503-5105-8},
  venue = {Paderborn, Germany},
  keywords = {Automatic Code Transfer,notion}
}

@inproceedings{sidiroglou-douskosManagingPerformanceVs2011,
  title = {Managing {{Performance}} vs. {{Accuracy Trade-offs}} with {{Loop Perforation}}},
  booktitle = {Proceedings of the 19th {{ACM SIGSOFT Symposium}} and the 13th {{European Conference}} on {{Foundations}} of {{Software Engineering}}},
  author = {Sidiroglou-Douskos, Stelios and Misailovic, Sasa and Hoffmann, Henry and Rinard, Martin},
  date = {2011},
  series = {{{ESEC}}/{{FSE}} '11},
  pages = {124--134},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2025113.2025133},
  url = {http://doi.acm.org/10.1145/2025113.2025133},
  isbn = {978-1-4503-0443-6},
  venue = {Szeged, Hungary},
  keywords = {Loop Perforation,notion}
}

@unpublished{siebertApplicationsStatisticalCausal2022,
  title = {Applications of Statistical Causal Inference in Software Engineering},
  author = {Siebert, Julien},
  date = {2022},
  eprint = {2211.11482},
  eprinttype = {arXiv},
  keywords = {notion}
}

@inproceedings{silvaWhyWeRefactor2016,
  title = {Why {{We Refactor}}? {{Confessions}} of {{GitHub Contributors}}},
  booktitle = {Proceedings of the 2016 24th {{ACM SIGSOFT International Symposium}} on {{Foundations}} of {{Software Engineering}}},
  author = {Silva, Danilo and Tsantalis, Nikolaos and Valente, Marco Tulio},
  date = {2016},
  series = {{{FSE}} 2016},
  pages = {858--870},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2950290.2950305},
  url = {http://doi.acm.org/10.1145/2950290.2950305},
  isbn = {978-1-4503-4218-6},
  venue = {Seattle, WA, USA},
  keywords = {code smells,GitHub,notion,Refactoring,software evolution}
}

@article{simonyanDeepConvolutionalNetworks2013,
  title = {Deep {{Inside Convolutional Networks}}: {{Visualising Image Classification Models}} and {{Saliency Maps}}},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  date = {2013},
  journaltitle = {CoRR},
  volume = {abs/1312.6034},
  url = {http://arxiv.org/abs/1312.6034},
  keywords = {notion}
}

@article{singhConstructionBayesianNetwork1995,
  title = {Construction of {{Bayesian}} Network Structures from Data: {{A}} Brief Survey and an Efficient Algorithm},
  author = {Singh, Moninder and Valtorta, Marco},
  date = {1995},
  journaltitle = {International Journal of Approximate Reasoning},
  volume = {12},
  number = {2},
  pages = {111--131},
  issn = {0888-613X},
  doi = {10.1016/0888-613X(94)00016-V},
  url = {https://www.sciencedirect.com/science/article/pii/0888613X9400016V},
  abstract = {Previous algorithms for the recovery of Bayesian belief network structures from data have been either highly dependent on conditional independence (CI) tests, or have required on ordering on the nodes to be supplied by the user. We present an algorithm that integrates these two approaches: CI tests are used to generate an ordering on the nodes from the database, which is then used to recover the underlying Bayesian network structure using a non-CI-test-based method. Results of the evaluation of the algorithm on a number of databases (e.g., alarm, led, and soybean) are presented. We also discuss some algorithm performance issues and open problems.},
  keywords = {Bayesian networks,conditional independence,notion,probabilistic model construction,probabilistic networks}
}

@inproceedings{smithCureWorseDisease2015,
  title = {Is the {{Cure Worse Than}} the {{Disease}}? {{Overfitting}} in {{Automated Program Repair}}},
  booktitle = {Proceedings of the 2015 10th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Smith, Edward K. and Barr, Earl T. and Le Goues, Claire and Brun, Yuriy},
  date = {2015},
  series = {{{ESEC}}/{{FSE}} 2015},
  pages = {532--543},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2786805.2786825},
  url = {http://doi.acm.org/10.1145/2786805.2786825},
  isbn = {978-1-4503-3675-8},
  venue = {Bergamo, Italy},
  keywords = {Automated Program Repair,GenProg,notion,TrpAutoRepair}
}

@inproceedings{smithFoundationsQuantitativeInformation2009,
  title = {On the {{Foundations}} of {{Quantitative Information Flow}}},
  booktitle = {Foundations of {{Software Science}} and {{Computational Structures}}},
  author = {Smith, Geoffrey},
  editor = {family=Alfaro, given=Luca, prefix=de, useprefix=true},
  date = {2009},
  pages = {288--302},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  abstract = {There is growing interest in quantitative theories of information flow in a variety of contexts, such as secure information flow, anonymity protocols, and side-channel analysis. Such theories offer an attractive way to relax the standard noninterference properties, letting us tolerate “small” leaks that are necessary in practice. The emerging consensus is that quantitative information flow should be founded on the concepts of Shannon entropy and mutual information. But a useful theory of quantitative information flow must provide appropriate security guarantees: if the theory says that an attack leaks x bits of secret information, then x should be useful in calculating bounds on the resulting threat. In this paper, we focus on the threat that an attack will allow the secret to be guessed correctly in one try. With respect to this threat model, we argue that the consensus definitions actually fail to give good security guarantees—the problem is that a random variable can have arbitrarily large Shannon entropy even if it is highly vulnerable to being guessed. We then explore an alternative foundation based on a concept of vulnerability (closely related to Bayes risk) and which measures uncertainty using Rényi's min-entropy, rather than Shannon entropy.},
  isbn = {978-3-642-00596-1},
  keywords = {notion}
}

@article{smithNonparametricEstimationSpecies1984,
  title = {Nonparametric Estimation of Species Richness},
  author = {Smith, Eric P and family=Belle, given=Gerald, prefix=van, useprefix=true},
  date = {1984},
  journaltitle = {Biometrics},
  pages = {119--129},
  publisher = {JSTOR},
  keywords = {notion}
}

@misc{sobaniaRecentDevelopmentsProgram2021,
  title = {Recent {{Developments}} in {{Program Synthesis}} with {{Evolutionary Algorithms}}},
  author = {Sobania, Dominik and Schweim, Dirk and Rothlauf, Franz},
  date = {2021},
  keywords = {notion}
}

@article{sobreiraDissectionBugDataset2018,
  title = {Dissection of a {{Bug Dataset}}: {{Anatomy}} of 395 {{Patches}} from {{Defects4J}}},
  author = {Sobreira, Victor and Durieux, Thomas and Delfim, Fernanda Madeiral and Monperrus, Martin and Maia, Marcelo de Almeida},
  date = {2018},
  journaltitle = {CoRR},
  volume = {abs/1801.06393},
  url = {http://arxiv.org/abs/1801.06393},
  keywords = {notion}
}

@inproceedings{sohnAmortisedDeepParameter2016a,
  title = {Amortised {{Deep Parameter Optimisation}} of {{GPGPU Work Group Size}} for {{OpenCV}}},
  booktitle = {Search {{Based Software Engineering}}},
  author = {Sohn, Jeongju and Lee, Seongmin and Yoo, Shin},
  editor = {Sarro, Federica and Deb, Kalyanmoy},
  date = {2016},
  year = {2016},
  pages = {211--217},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-47106-8_14},
  abstract = {GPGPU (General Purpose computing on Graphics Processing Units) enables massive parallelism by taking advantage of the Single Instruction Multiple Data (SIMD) architecture of the large number of cores found on modern graphics cards. A parameter called local work group size controls how many work items are concurrently executed on a single compute unit. Though critical to the performance, there is no deterministic way to tune it, leaving developers to manual trial and error. This paper applies amortised optimisation to determine the best local work group size for GPGPU implementations of OpenCV template matching feature. The empirical evaluation shows that optimised local work group size can outperform the default value with large effect sizes.},
  isbn = {978-3-319-47106-8},
  langid = {english},
  keywords = {Execution Time,Global Memory,Graphic Processing Unit,Single Instruction Multiple Data,Template Match},
  file = {/Users/bohrok/Zotero/storage/L2NQR8NH/Sohn et al. - 2016 - Amortised Deep Parameter Optimisation of GPGPU Work Group Size for OpenCV.pdf}
}

@inproceedings{sohnAssistingBugReport2021,
  title = {Assisting {{Bug Report Assignment Using Automated Fault Localisation}}: {{An Industrial Case Study}}},
  booktitle = {2021 14th {{IEEE Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}} ({{ICST}})},
  author = {Sohn, Jeongju and An, Gabin and Hong, Jingun and Hwang, Dongwon and Yoo, Shin},
  date = {2021},
  pages = {284--294},
  doi = {10.1109/ICST49551.2021.00041},
  keywords = {notion}
}

@inproceedings{sohnCEMENTUseEvolutionary2022,
  title = {{{CEMENT}}: {{On}} the {{Use}} of {{Evolutionary Coupling Between Tests}} and {{Code Units}}. {{A Case Study}} on {{Fault Localization}}},
  booktitle = {2022 {{IEEE}} 33rd {{International Symposium}} on {{Software Reliability Engineering}} ({{ISSRE}})},
  author = {Sohn, Jeongju and Papadakis, Mike},
  date = {2022},
  pages = {133--144},
  doi = {10.1109/ISSRE55969.2022.00023},
  keywords = {notion}
}

@inproceedings{sohnFLUCCSUsingCode2017,
  title = {{{FLUCCS}}: Using Code and Change Metrics to Improve Fault Localization},
  author = {Sohn, Jeongju and Yoo, Shin},
  date = {2017-07},
  pages = {273--283},
  doi = {10.1145/3092703.3092717},
  keywords = {notion}
}

@misc{sohnUsingEvolutionaryCoupling2022,
  title = {Using {{Evolutionary Coupling}} to {{Establish Relevance Links Between Tests}} and {{Code Units}}. {{A}} Case Study on Fault Localization},
  author = {Sohn, Jeongju and Papadakis, Mike},
  date = {2022},
  doi = {10.48550/ARXIV.2203.11343},
  url = {https://arxiv.org/abs/2203.11343},
  organization = {arXiv},
  keywords = {FOS: Computer and information sciences,notion,Software Engineering (cs.SE)}
}

@article{solowEffectDependenceEstimating2000,
  title = {The Effect of Dependence on Estimating Sample Coverage},
  author = {Solow, Andrew R.},
  date = {2000},
  journaltitle = {Environmetrics},
  volume = {11},
  number = {2},
  pages = {245--249},
  doi = {10.1002/(SICI)1099-095X(200003/04)11:2<245::AID-ENV408>3.0.CO;2-S},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291099-095X%28200003/04%2911%3A2%3C245%3A%3AAID-ENV408%3E3.0.CO%3B2-S},
  abstract = {Abstract The coverage of a sample of individuals from a biological community is defined as the total relative abundance of the species represented in the sample. A simple estimator of coverage is based on the number of species represented in the sample by a single individual. This estimator works well when the individuals are sampled independently with respect to their species. This paper explores the performance of this estimator under a Markovian sampling model. It is shown that this performance is badly degraded when there is even modest dependence. Copyright \textbackslash copyright 2000 John Wiley \& Sons, Ltd.},
  keywords = {biological community,coverage estimation,Markov,notion}
}

@inproceedings{soltaniGuidedGeneticAlgorithm2017,
  title = {A {{Guided Genetic Algorithm}} for {{Automated Crash Reproduction}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Software Engineering}}},
  author = {Soltani, Mozhan and Panichella, Annibale and family=Deursen, given=Arie, prefix=van, useprefix=true},
  date = {2017},
  series = {{{ICSE}} '17},
  pages = {209--220},
  publisher = {IEEE Press},
  location = {Piscataway, NJ, USA},
  doi = {10.1109/ICSE.2017.27},
  url = {https://doi.org/10.1109/ICSE.2017.27},
  isbn = {978-1-5386-3868-2},
  venue = {Buenos Aires, Argentina},
  keywords = {Automated Crash Reproduction,EvoCrash,EvoSuite,Genetic Algorithm (GA),notion,SBSE,SBST}
}

@inproceedings{songC2D2ExtractingCritical2024,
  title = {{{C2D2}}: {{Extracting Critical Changes}} for {{Real-World Bugs}} with {{Dependency-Sensitive Delta Debugging}}},
  shorttitle = {{{C2D2}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Song, Xuezhi and Wu, Yijian and Liu, Shuning and Chen, Bihuan and Lin, Yun and Peng, Xin},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {300--312},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3652129},
  url = {https://dl.acm.org/doi/10.1145/3650212.3652129},
  urldate = {2024-09-21},
  abstract = {Data-driven techniques are promising for automatically locating and fixing bugs, which can reduce enormous time and effort for developers. However, the effectiveness of these techniques heavily   relies on the quality and scale of bug datasets. Despite that emerging approaches to automatic bug dataset construction partially provide a solution for scalability, data quality remains a concern. Specifically, it remains a barrier for humans to isolate the minimal set of bug-inducing or bug-fixing changes, known as critical changes. Although delta debugging (DD) techniques are capable of extracting critical changes on benchmark datasets in academia, the efficiency and accuracy are still limited when dealing with real-world bugs, where code change dependencies could be overly complicated.   In this paper, we propose C2D2, a novel delta debugging approach for critical change extraction, which estimates the probabilities of dependencies between code change elements. C2D2 considers the probabilities of dependencies and introduces a matrix-based search mechanism to resolve compilation errors (CE) caused by missing dependencies. It also provides hybrid mechanisms for flexibly selecting code change elements during the DD process. Experiments on Defect4J and a real-world regression bug dataset reveal that C2D2 is significantly more efficient than the traditional DD algorithm ddmin with competitive effectiveness, and significantly more effective and more efficient than the state-of-the-art DD algorithm ProbDD. Furthermore, compared to human-isolated critical changes, C2D2 produces the same or better critical change results in 56\% cases in Defects4J and 86\% cases in the regression dataset, demonstrating its usefulness in automatically extracting   critical changes and saving human efforts in constructing large-scale bug datasets with real-world bugs.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/N43NWK7L/Song et al. - 2024 - C2D2 Extracting Critical Changes for Real-World Bugs with Dependency-Sensitive Delta Debugging.pdf}
}

@inproceedings{songEHBDroidGUITesting2017,
  title = {{{EHBDroid}}: {{Beyond GUI Testing}} for {{Android Applications}}},
  booktitle = {Proceedings of the {{32Nd IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Song, Wei and Qian, Xiangxing and Huang, Jeff},
  date = {2017},
  series = {{{ASE}} 2017},
  pages = {27--37},
  publisher = {IEEE Press},
  location = {Piscataway, NJ, USA},
  url = {http://dl.acm.org/citation.cfm?id=3155562.3155570},
  isbn = {978-1-5386-2684-9},
  venue = {Urbana-Champaign, IL, USA},
  keywords = {Android,automated testing,event generation,event handlers,notion}
}

@article{soremekunEvaluatingImpactExperimental,
  title = {Evaluating the {{Impact}} of {{Experimental Assumptions}} in {{Automated Fault Localization}}},
  author = {Soremekun, Ezekiel and Kirschner, Lukas and Böhme, Marcel and Papadakis, Mike},
  keywords = {notion}
}

@misc{soremekunLocatingFaultsProgram2021,
  title = {Locating {{Faults}} with {{Program Slicing}}: {{An Empirical Analysis}}},
  author = {Soremekun, Ezekiel and Kirschner, Lukas and Böhme, Marcel and Zeller, Andreas},
  date = {2021},
  keywords = {notion}
}

@article{soremekunProgrammersShouldStill2016,
  title = {Programmers Should Still Use Slices When Debugging},
  author = {Soremekun, Ezekiel O and Böhme, Marcel and Zeller, Andreas},
  date = {2016},
  keywords = {notion}
}

@inproceedings{soSynthesizingPatternPrograms2018,
  title = {Synthesizing {{Pattern Programs}} from {{Examples}}.},
  booktitle = {{{IJCAI}}},
  author = {So, Sunbeom and Oh, Hakjoo},
  date = {2018},
  pages = {1618--1624},
  keywords = {notion}
}

@inproceedings{sousaDataflowProgrammingConcept2012,
  title = {Dataflow Programming Concept, Languages and Applications},
  booktitle = {Doctoral {{Symposium}} on {{Informatics Engineering}}},
  author = {Sousa, Tiago Boldt},
  date = {2012},
  volume = {130},
  keywords = {notion}
}

@inproceedings{spirtesDirectedCyclicGraphical1995,
  title = {Directed {{Cyclic Graphical Representations}} of {{Feedback Models}}},
  booktitle = {Proceedings of the {{Eleventh Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Spirtes, Peter},
  date = {1995},
  series = {{{UAI}}'95},
  pages = {491--498},
  publisher = {Morgan Kaufmann Publishers Inc.},
  location = {San Francisco, CA, USA},
  isbn = {1-55860-385-9},
  venue = {Montréal, Qué, Canada},
  keywords = {notion}
}

@inproceedings{sridharaAutomaticallyDetectingDescribing2011,
  title = {Automatically Detecting and Describing High Level Actions within Methods},
  booktitle = {2011 33rd {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Sridhara, Giriprasad and Pollock, Lori and Vijay-Shanker, K.},
  date = {2011},
  pages = {101--110},
  doi = {10.1145/1985793.1985808},
  keywords = {notion}
}

@inproceedings{stoelingaInterpretingSuccessfulTesting2009,
  title = {Interpreting a {{Successful Testing Process}}: {{Risk}} and {{Actual Coverage}}},
  shorttitle = {Interpreting a {{Successful Testing Process}}},
  booktitle = {2009 {{Third IEEE International Symposium}} on {{Theoretical Aspects}} of {{Software Engineering}}},
  author = {Stoelinga, Mariëlle and Timmer, Mark},
  date = {2009-07},
  pages = {251--258},
  doi = {10.1109/TASE.2009.26},
  url = {https://ieeexplore.ieee.org/abstract/document/5198509},
  urldate = {2024-10-29},
  abstract = {Testing is inherently incomplete; no test suite will ever be able to test all possible usage scenarios of a system. It is therefore vital to assess the implication of a system passing a test suite. This paper quantifies that implication by means of two distinct, but related, measures: the risk quantifies the confidence in a system after it passes a test suite, i.e., the number of faults still expected to be present (weighted by their severity); the actual coverage quantifies the extent to which faults have been shown absent, i.e., the fraction of possible faults that has been covered. We provide evaluation algorithms that calculate these metrics for a given test suite, as well as optimisation algorithms that yield the best test suite for a given optimisation criterion.},
  eventtitle = {2009 {{Third IEEE International Symposium}} on {{Theoretical Aspects}} of {{Software Engineering}}},
  keywords = {Computer science,Costs,coverage,Error probability,formal testing,Mathematical model,notion,probabilistic,Random variables,risk,Risk management,Software engineering,Software testing,Solid modeling,System testing},
  file = {/Users/bohrok/Zotero/storage/KSTSSRAM/Stoelinga and Timmer - 2009 - Interpreting a Successful Testing Process Risk and Actual Coverage.pdf;/Users/bohrok/Zotero/storage/35BEYHR9/5198509.html}
}

@book{stroockProbabilityTheoryAnalytic2010a,
  title = {Probability Theory: An Analytic View},
  author = {Stroock, Daniel W},
  date = {2010},
  publisher = {Cambridge university press},
  keywords = {notion}
}

@inproceedings{suBenchmarkingAutomatedGUI2021,
  title = {Benchmarking {{Automated GUI Testing}} for {{Android}} against {{Real-World Bugs}}},
  booktitle = {Proceedings of the 29th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Su, Ting and Wang, Jue and Su, Zhendong},
  date = {2021},
  series = {{{ESEC}}/{{FSE}} 2021},
  pages = {119--130},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3468264.3468620},
  url = {https://doi.org/10.1145/3468264.3468620},
  abstract = {For ensuring the reliability of Android apps, there has been tremendous, continuous progress on improving automated GUI testing in the past decade. Specifically, dozens of testing techniques and tools have been developed and demonstrated to be effective in detecting crash bugs and outperform their respective prior work in the number of detected crashes. However, an overarching question "How effectively and thoroughly can these tools find crash bugs in practice?" has not been well-explored, which requires a ground-truth benchmark with real-world bugs. Since prior studies focus on tool comparisons w.r.t. some selected apps, they cannot provide direct, in-depth answers to this question. To complement existing work and tackle the above question, this paper offers the first ground-truth empirical evaluation of automated GUI testing for Android. To this end, we devote substantial manual effort to set up the Themis benchmark set, including (1) a carefully constructed dataset with 52 real, reproducible crash bugs (taking two person-months for its collection and validation), and (2) a unified, extensible infrastructure with six recent state-of-the-art testing tools. The whole evaluation has taken over 10,920 CPU hours. We find a considerable gap in these tools finding the collected real bugs — 18 bugs cannot be detected by any tool. Our systematic analysis further identifies five major common challenges that these tools face, and reveals additional findings such as factors affecting these tools in bug finding and opportunities for tool improvements. Overall, this work offers new concrete insights, most of which are previously unknown/unstated and difficult to obtain. Our study presents a new, complementary perspective from prior studies to understand and analyze the effectiveness of existing testing tools, as well as a benchmark for future research on this topic. The Themis benchmark is publicly available at https://github.com/the-themis-benchmarks/home.},
  isbn = {978-1-4503-8562-6},
  venue = {Athens, Greece},
  keywords = {Android apps,Benchmarking,Crash bugs,GUI testing,notion}
}

@inproceedings{suDynamicInferenceLikely2015,
  title = {Dynamic {{Inference}} of {{Likely Metamorphic Properties}} to {{Support Differential Testing}}},
  booktitle = {2015 {{IEEE}}/{{ACM}} 10th {{International Workshop}} on {{Automation}} of {{Software Test}}},
  author = {Su, F. H. and Bell, J. and Murphy, C. and Kaiser, G.},
  date = {2015-05},
  pages = {55--59},
  doi = {10.1109/AST.2015.19},
  abstract = {Metamorphic testing is an advanced technique to test programs without a true test oracle such as machine learning applications. Because these programs have no general oracle to identify their correctness, traditional testing techniques such as unit testing may not be helpful for developers to detect potential bugs. This paper presents a novel system, KABU, which can dynamically infer properties of methods' states in programs that describe the characteristics of a method before and after transforming its input. These Metamorphic Properties (MPs) are pivotal to detecting potential bugs in programs without test oracles, but most previous work relies solely on human effort to identify them and only considers MPs between input parameters and output result (return value) of a program or method. This paper also proposes a testing concept, Metamorphic Differential Testing (MDT). By detecting different sets of MPs between different versions for the same method, KABU reports potential bugs for human review. We have performed a preliminary evaluation of KABU by comparing the MPs detected by humans with the MPs detected by KABU. Our preliminary results are promising: KABU can find more MPs than human developers, and MDT is effective at detecting function changes in methods.},
  keywords = {Kabu,Metamorphic Testing,notion,Software Testing}
}

@inproceedings{sugiyamaDirectImportanceEstimation2007,
  title = {Direct {{Importance Estimation}} with {{Model Selection}} and {{Its Application}} to {{Covariate Shift Adaptation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sugiyama, Masashi and Nakajima, Shinichi and Kashima, Hisashi and Buenau, Paul and Kawanabe, Motoaki},
  editor = {Platt, J. and Koller, D. and Singer, Y. and Roweis, S.},
  date = {2007},
  volume = {20},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/be83ab3ecd0db773eb2dc1b0a17836a1-Paper.pdf},
  keywords = {notion}
}

@inproceedings{suGuidedStochasticModelbased2017,
  title = {Guided, Stochastic Model-Based {{GUI}} Testing of {{Android}} Apps},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Su, Ting and Meng, Guozhu and Chen, Yuting and Wu, Ke and Yang, Weiming and Yao, Yao and Pu, Geguang and Liu, Yang and Su, Zhendong},
  date = {2017},
  pages = {245--256},
  keywords = {notion}
}

@article{sultanaUnderstandingFixingMultiple2016,
  title = {Understanding and {{Fixing Multiple Language Interoperability Issues}}: {{The C}}/{{Fortran Case}}},
  author = {Sultana, Nawrin and Middleton, Justin and Overbey, Jeffrey and Hafiz, Munawar},
  date = {2016},
  journaltitle = {2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE)},
  pages = {772--783},
  keywords = {notion}
}

@inproceedings{sumnerComparativeCausalityExplaining2013,
  title = {Comparative Causality: {{Explaining}} the Differences between Executions},
  booktitle = {2013 35th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Sumner, William N. and Zhang, Xiangyu},
  date = {2013},
  pages = {272--281},
  doi = {10.1109/ICSE.2013.6606573},
  keywords = {notion}
}

@article{sunPersesSyntaxGuidedProgram2018,
  title = {Perses: {{Syntax-Guided Program Reduction}}},
  author = {Sun, Chengnian and Li, Yuanbo and Zhang, Qirun and Gu, Tianxiao and Su, Zhendong},
  date = {2018},
  keywords = {notion}
}

@inproceedings{sunPropertiesEffectiveMetrics2016,
  title = {Properties of {{Effective Metrics}} for {{Coverage-Based Statistical Fault Localization}}},
  booktitle = {2016 {{IEEE International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}} ({{ICST}})},
  author = {Sun, S. and Podgurski, A.},
  date = {2016},
  pages = {124--134},
  keywords = {notion}
}

@inproceedings{szegediDynamicSlicingJava2005,
  title = {Dynamic Slicing of {{Java}} Bytecode Programs},
  booktitle = {Fifth {{IEEE International Workshop}} on {{Source Code Analysis}} and {{Manipulation}} ({{SCAM}}'05)},
  author = {Szegedi, A. and Gyimothy, T.},
  date = {2005-09},
  pages = {35--44},
  issn = {null},
  doi = {10.1109/SCAM.2005.8},
  keywords = {Concrete,Conferences,Debugging,dynamic program slicing,Instruments,Java,Java bytecode programs,Manuals,notion,program compilers,Program processors,program slicing,Software engineering,source code locations,virtual machine,Virtual machining}
}

@article{szegedyIntriguingPropertiesNeural2013,
  title = {Intriguing Properties of Neural Networks},
  author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian J. and Fergus, Rob},
  date = {2013},
  journaltitle = {CoRR},
  volume = {abs/1312.6199},
  url = {http://arxiv.org/abs/1312.6199},
  keywords = {notion}
}

@misc{taeSliceTunerSelective2020,
  title = {Slice {{Tuner}}: {{A Selective Data Collection Framework}} for {{Accurate}} and {{Fair Machine Learning Models}}},
  author = {Tae, Ki Hyun and Whang, Steven Euijong},
  date = {2020},
  keywords = {notion}
}

@inproceedings{taoPartitioningCompositeCode2015,
  title = {Partitioning {{Composite Code Changes}} to {{Facilitate Code Review}}},
  booktitle = {2015 {{IEEE}}/{{ACM}} 12th {{Working Conference}} on {{Mining Software Repositories}}},
  author = {Tao, Y. and Kim, S.},
  date = {2015-05},
  pages = {180--190},
  issn = {2160-1852},
  doi = {10.1109/MSR.2015.24},
  keywords = {Cloning,code review,Force,Inspection,Libraries,Manuals,notion,open source code,partitioning composite code changes,Pattern matching,public domain software,Software,software maintenance,source code (software),source code changes}
}

@inproceedings{tavaresParametricInverseSimulation2016,
  title = {Parametric {{Inverse Simulation}}},
  author = {Tavares, Zenna},
  date = {2016},
  keywords = {notion}
}

@report{teamLearningPrivacyScale2017,
  title = {Learning with {{Privacy}} at {{Scale}}},
  author = {Team, Differential Privacy},
  date = {2017-12},
  institution = {Apple Machine Learning Research},
  url = {https://machinelearning.apple.com/research/learning-with-privacy-at-scale},
  urldate = {2024-01-01},
  keywords = {notion,Privacy}
}

@misc{terragniAPIzationGeneratingReusable2021,
  title = {{{APIzation}}: {{Generating Reusable APIs}} from {{StackOverflow Code Snippets}}},
  author = {Terragni, Valerio and Salza, Pasquale},
  date = {2021},
  keywords = {notion}
}

@article{teymourianFastClusteringAlgorithm2020,
  title = {A Fast Clustering Algorithm for Modularization of Large-Scale Software Systems},
  author = {Teymourian, Navid and Izadkhah, Habib and Isazadeh, Ayaz},
  date = {2020},
  journaltitle = {IEEE Transactions on Software Engineering},
  pages = {1--1},
  doi = {10.1109/TSE.2020.3022212},
  keywords = {notion}
}

@inproceedings{tianAutomaticallyDiagnosingRepairing2017,
  title = {Automatically {{Diagnosing}} and {{Repairing Error Handling Bugs}} in {{C}}},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Tian, Yuchi and Ray, Baishakhi},
  date = {2017},
  series = {{{ESEC}}/{{FSE}} 2017},
  pages = {752--762},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/3106237.3106300},
  url = {http://doi.acm.org/10.1145/3106237.3106300},
  isbn = {978-1-4503-5105-8},
  venue = {Paderborn, Germany},
  keywords = {Automated Program Repair,Bug Detection,Error Handling Bug,notion}
}

@inproceedings{tianDeepTestAutomatedTesting2018,
  title = {{{DeepTest}}: {{Automated Testing}} of {{Deep-Neural-Network-Driven Autonomous Cars}}},
  booktitle = {2018 {{IEEE}}/{{ACM}} 40th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Tian, Yuchi and Pei, Kexin and Jana, Suman and Ray, Baishakhi},
  date = {2018},
  pages = {303--314},
  keywords = {notion}
}

@inproceedings{tianIdentifyingConditionalCausal2004,
  title = {Identifying {{Conditional Causal Effects}}},
  booktitle = {Proceedings of the {{Twentieth Conference}} on {{Uncertainty}} in {{Artificial Intelligence}} ({{UAI}})},
  author = {Tian, Jin},
  date = {2004},
  keywords = {notion}
}

@inproceedings{tianLargeLanguageModels2024,
  title = {Large {{Language Models}} for {{Equivalent Mutant Detection}}: {{How Far Are We}}?},
  shorttitle = {Large {{Language Models}} for {{Equivalent Mutant Detection}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Tian, Zhao and Shu, Honglin and Wang, Dong and Cao, Xuejie and Kamei, Yasutaka and Chen, Junjie},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {1733--1745},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3680395},
  url = {https://dl.acm.org/doi/10.1145/3650212.3680395},
  urldate = {2024-09-21},
  abstract = {Mutation testing is vital for ensuring software quality. However, the presence of equivalent mutants is known to introduce redundant cost and bias issues, hindering the effectiveness of mutation testing in practical use. Although numerous equivalent mutant detection (EMD) techniques have been proposed, they exhibit limitations due to the scarcity of training data and challenges in generalizing to unseen mutants. Recently, large language models (LLMs) have been extensively adopted in various code-related tasks and have shown superior performance by more accurately capturing program semantics. Yet the performance of LLMs in equivalent mutant detection remains largely unclear. In this paper, we conduct an empirical study on 3,302 method-level Java mutant pairs to comprehensively investigate the effectiveness and efficiency of LLMs for equivalent mutant detection. Specifically, we assess the performance of LLMs compared to existing EMD techniques, examine the various strategies of LLMs, evaluate the orthogonality between EMD techniques, and measure the time overhead of training and inference. Our findings demonstrate that LLM-based techniques significantly outperform existing techniques (i.e., the average improvement of 35.69\% in terms of F1-score), with the fine-tuned code embedding strategy being the most effective. Moreover, LLM-based techniques offer an excellent balance between cost (relatively low training and inference time) and effectiveness. Based on our findings, we further discuss the impact of model size and embedding quality, and provide several promising directions for future research. This work is the first to examine LLMs in equivalent mutant detection, affirming their effectiveness and efficiency.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/TTXGTX9S/Tian et al. - 2024 - Large Language Models for Equivalent Mutant Detection How Far Are We.pdf}
}

@article{tiarksWhatProgrammersReally2011,
  title = {What {{Programmers Really Do}} - {{An Observational Study}}},
  author = {Tiarks, Rebecca},
  date = {2011},
  journaltitle = {Softwaretechnik-Trends},
  volume = {31},
  number = {2},
  url = {http://pi.informatik.uni-siegen.de/stt/31\_2/01\_Fachgruppenberichte/sre/17-tiarks.pdf},
  keywords = {notion}
}

@inproceedings{tirmaziBorgNextGeneration2020,
  title = {Borg: The {{Next Generation}}},
  booktitle = {Proceedings of the {{Fifteenth European Conference}} on {{Computer Systems}} ({{EuroSys}}'20)},
  author = {Tirmazi, Muhammad and Barker, Adam and Deng, Nan and Haque, Md E. and Qin, Zhijing Gene and Hand, Steven and Harchol-Balter, Mor and Wilkes, John},
  date = {2020},
  publisher = {ACM},
  location = {Heraklion, Greece},
  doi = {10.1145/3342195.3387517},
  url = {https://doi.org/10.1145/3342195.3387517},
  abstract = {This paper analyzes a newly-published trace that covers 8 different Borg clusters for the month of May 2019. The trace enables researchers to explore how scheduling works in large-scale production compute clusters. We highlight how Borg has evolved and perform a longitudinal comparison of the newly-published 2019 trace against the 2011 trace, which has been highly cited within the research community. Our findings show that Borg features such as alloc sets are used for resource-heavy workloads; automatic vertical scaling is effective; job-dependencies account for much of the high failure rates reported by prior studies; the workload arrival rate has increased, as has the use of resource over-commitment; the workload mix has changed, jobs have migrated from the free tier into the best-effort batch tier; the workload exhibits an extremely heavy-tailed distribution where the top 1\% of jobs consume over 99\% of resources; and there is a great deal of variation between different clusters.},
  isbn = {978-1-4503-6882-7},
  keywords = {cloud computing,data centers,notion}
}

@inproceedings{trabishChoppedSymbolicExecution2018,
  title = {Chopped {{Symbolic Execution}}},
  booktitle = {2018 {{IEEE}}/{{ACM}} 40th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Trabish, David and Mattavelli, Andrea and Rinetzky, Noam and Cadar, Cristian},
  date = {2018},
  pages = {350--360},
  doi = {10.1145/3180155.3180251},
  keywords = {Computer bugs,Engines,Explosions,notion,Program slicing,Runtime,Software,Static analysis,Symbolic execution}
}

@unpublished{triantafillouCausalMarkovBoundaries2021,
  title = {Causal Markov Boundaries},
  author = {Triantafillou, Sofia and Jabbari, Fattaneh and Cooper, Greg},
  date = {2021},
  eprint = {2103.07560},
  eprinttype = {arXiv},
  keywords = {notion}
}

@software{ucdavisLatteIntegrale,
  title = {Latte Integrale},
  author = {UC Davis, Mathematics},
  url = {http://www.math.ucdavis.edu/~latte},
  version = {1.7.6},
  keywords = {notion}
}

@inproceedings{udeshiCallistoEntropybasedTest2020,
  title = {Callisto: {{Entropy-based Test Generation}} and {{Data Quality Assessment}} for {{Machine Learning Systems}}},
  booktitle = {2020 {{IEEE}} 13th {{International Conference}} on {{Software Testing}}, {{Validation}} and {{Verification}} ({{ICST}})},
  author = {Udeshi, S. and Jiang, X. and Chattopadhyay, S.},
  date = {2020},
  pages = {448--453},
  doi = {10.1109/ICST46399.2020.00060},
  keywords = {notion}
}

@article{untchMutationAnalysisUsing1993,
  title = {Mutation {{Analysis Using Mutant Schemata}}},
  author = {Untch, Roland H. and Offutt, A. Jefferson and Harrold, Mary Jean},
  date = {1993-07},
  journaltitle = {SIGSOFT Softw. Eng. Notes},
  volume = {18},
  number = {3},
  pages = {139--148},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {0163-5948},
  doi = {10.1145/174146.154265},
  url = {https://doi.org/10.1145/174146.154265},
  abstract = {Mutation analysis is a powerful technique for assessing and improving the quality of test data used to unit test software. Unfortunately, current automated mutation analysis systems suffer from severe performance problems. This paper presents a new method for performing mutation analysis that uses program schemata to encode all mutants for a program into one metaprogram, which is subsequently compiled and run at speeds substantially higher than achieved by previous interpretive systems. Preliminary performance improvements of over 300\% are reported. This method has the additional advantages of being easier to implement than interpretive systems, being simpler to port across a wide range of hardware and software platforms, and using the same compiler and run-time support system that is used during development and/or deployment.},
  keywords = {fault-based testing,mutation analysis,notion,program schemata,software testing}
}

@inproceedings{untchMutationAnalysisUsing1993a,
  title = {Mutation {{Analysis Using Mutant Schemata}}},
  booktitle = {Proceedings of the 1993 {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Untch, Roland H. and Offutt, A. Jefferson and Harrold, Mary Jean},
  date = {1993},
  series = {{{ISSTA}} '93},
  pages = {139--148},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/154183.154265},
  url = {https://doi.org/10.1145/154183.154265},
  abstract = {Mutation analysis is a powerful technique for assessing and improving the quality of test data used to unit test software. Unfortunately, current automated mutation analysis systems suffer from severe performance problems. This paper presents a new method for performing mutation analysis that uses program schemata to encode all mutants for a program into one metaprogram, which is subsequently compiled and run at speeds substantially higher than achieved by previous interpretive systems. Preliminary performance improvements of over 300\% are reported. This method has the additional advantages of being easier to implement than interpretive systems, being simpler to port across a wide range of hardware and software platforms, and using the same compiler and run-time support system that is used during development and/or deployment.},
  isbn = {0-89791-608-5},
  venue = {Cambridge, Massachusetts, USA},
  keywords = {fault-based testing,mutation analysis,notion,program schemata,software testing}
}

@inproceedings{valiantEstimatingUnseenImproved2013,
  title = {Estimating the {{Unseen}}: {{Improved Estimators}} for {{Entropy}} and Other {{Properties}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Valiant, Paul and Valiant, Gregory},
  editor = {Burges, C. J. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
  date = {2013},
  volume = {26},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/53c04118df112c13a8c34b38343b9c10-Paper.pdf},
  keywords = {notion}
}

@article{valiantEstimatingUnseenImproved2017,
  title = {Estimating the {{Unseen}}: {{Improved Estimators}} for {{Entropy}} and {{Other Properties}}},
  author = {Valiant, Gregory and Valiant, Paul},
  date = {2017-10},
  journaltitle = {J. ACM},
  volume = {64},
  number = {6},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {0004-5411},
  doi = {10.1145/3125643},
  url = {https://doi.org/10.1145/3125643},
  abstract = {We show that a class of statistical properties of distributions, which includes such practically relevant properties as entropy, the number of distinct elements, and distance metrics between pairs of distributions, can be estimated given a sublinear sized sample. Specifically, given a sample consisting of independent draws from any distribution over at most k distinct elements, these properties can be estimated accurately using a sample of size O(k log k). For these estimation tasks, this performance is optimal, to constant factors. Complementing these theoretical results, we also demonstrate that our estimators perform exceptionally well, in practice, for a variety of estimation tasks, on a variety of natural distributions, for a wide range of parameters. The key step in our approach is to first use the sample to characterize the “unseen” portion of the distribution—effectively reconstructing this portion of the distribution as accurately as if one had a logarithmic factor larger sample. This goes beyond such tools as the Good-Turing frequency estimation scheme, which estimates the total probability mass of the unobserved portion of the distribution: We seek to estimate the shape of the unobserved portion of the distribution. This work can be seen as introducing a robust, general, and theoretically principled framework that, for many practical applications, essentially amplifies the sample size by a logarithmic factor; we expect that it may be fruitfully used as a component within larger machine learning and statistical analysis systems.},
  keywords = {distinct elements,entropy estimation,notion,Statistical property estimation,unseen species}
}

@inproceedings{valiantInstanceOptimalLearning2016,
  title = {Instance {{Optimal Learning}} of {{Discrete Distributions}}},
  booktitle = {Proceedings of the {{Forty-Eighth Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  author = {Valiant, Gregory and Valiant, Paul},
  date = {2016},
  series = {{{STOC}} '16},
  pages = {142--155},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2897518.2897641},
  url = {https://doi.org/10.1145/2897518.2897641},
  abstract = {We consider the following basic learning task: given independent draws from an unknown distribution over a discrete support, output an approximation of the distribution that is as accurate as possible in L1 distance (equivalently, total variation distance, or "statistical distance"). Perhaps surprisingly, it is often possible to "de-noise" the empirical distribution of the samples to return an approximation of the true distribution that is significantly more accurate than the empirical distribution, without relying on any prior assumptions on the distribution. We present an instance optimal learning algorithm which optimally performs this de-noising for every distribution for which such a de-noising is possible. More formally, given n independent draws from a distribution p, our algorithm returns a labelled vector whose expected distance from p is equal to the minimum possible expected error that could be obtained by any algorithm, even one that is given the true unlabeled vector of probabilities of distribution p and simply needs to assign labels—up to an additive subconstant term that is independent of p and goes to zero as n gets large. This somewhat surprising result has several conceptual implications, including the fact that, for any large sample from a distribution over discrete support, prior knowledge of the rates of decay of the tails of the distribution (e.g. power-law type assumptions) is not significantly helpful for the task of learning the distribution. As a consequence of our techniques, we also show that given a set of n samples from an arbitrary distribution, one can accurately estimate the expected number of distinct elements that will be observed in a sample of any size up to n log n. This sort of extrapolation is practically relevant, particularly to domains such as genomics where it is important to understand how much more might be discovered given larger sample sizes, and we are optimistic that our approach is practically viable.},
  isbn = {978-1-4503-4132-5},
  venue = {Cambridge, MA, USA},
  keywords = {Distribution learning,Good-Turing frequency estimation,instance optimality,notion,property estimation,the unseen species problem}
}

@unpublished{valleApplyingExtendingDelta2023,
  title = {Applying and {{Extending}} the {{Delta Debugging Algorithm}} for {{Elevator Dispatching Algorithms}} ({{Experience Paper}})},
  author = {Valle, Pablo and Arrieta, Aitor and Arratibel, Maite},
  date = {2023},
  eprint = {2305.17803},
  eprinttype = {arXiv},
  keywords = {notion}
}

@article{valPreciselyMeasuringQuantitative2016,
  title = {Precisely {{Measuring Quantitative Information Flow}}: {{10K Lines}} of {{Code}} and {{Beyond}}},
  author = {Val, Celina Gomes Do and Enescu, Michael A. and Bayless, Sam and Aiello, William and Hu, Alan J.},
  date = {2016},
  journaltitle = {2016 IEEE European Symposium on Security and Privacy (EuroS\&P)},
  pages = {31--46},
  url = {https://api.semanticscholar.org/CorpusID:16644725},
  keywords = {notion}
}

@inproceedings{vantonderLightweightMultiLanguageSyntax2019,
  title = {Lightweight {{Multi-Language Syntax Transformation}} with {{Parser Parser Combinators}}},
  booktitle = {Proceedings of the 40th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {family=Tonder, given=Rijnard, prefix=van, useprefix=true and Le Goues, Claire},
  date = {2019},
  series = {{{PLDI}} 2019},
  pages = {363--378},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3314221.3314589},
  url = {https://doi.org/10.1145/3314221.3314589},
  abstract = {Automatically transforming programs is hard, yet critical for automated program refactoring, rewriting, and repair. Multi-language syntax transformation is especially hard due to heterogeneous representations in syntax, parse trees, and abstract syntax trees (ASTs). Our insight is that the problem can be decomposed such that (1) a common grammar expresses the central context-free language (CFL) properties shared by many contemporary languages and (2) open extension points in the grammar allow customizing syntax (e.g., for balanced delimiters) and hooks in smaller parsers to handle language-specific syntax (e.g., for comments). Our key contribution operationalizes this decomposition using a Parser Parser combinator (PPC), a mechanism that generates parsers for matching syntactic fragments in source code by parsing declarative user-supplied templates. This allows our approach to detach from translating input programs to any particular abstract syntax tree representation, and lifts syntax rewriting to a modularly-defined parsing problem. A notable effect is that we skirt the complexity and burden of defining additional translation layers between concrete user input templates and an underlying abstract syntax representation. We demonstrate that these ideas admit efficient and declarative rewrite templates across 12 languages, and validate effectiveness of our approach by producing correct and desirable lightweight transformations on popular real-world projects (over 50 syntactic changes produced by our approach have been merged into 40+). Our declarative rewrite patterns require an order of magnitude less code compared to analog implementations in existing, language-specific tools.},
  isbn = {978-1-4503-6712-7},
  venue = {Phoenix, AZ, USA},
  keywords = {notion,parsers,rewriting,syntax,transformation}
}

@article{vantonderStaticAutomatedProgram2018,
  title = {Static {{Automated Program Repair}} for {{Heap Properties}}},
  author = {family=Tonder, given=Rijnard, prefix=van, useprefix=true and Le Goues, Claire},
  date = {2018},
  keywords = {notion}
}

@article{vardiProgramVerificationVision2021,
  title = {Program {{Verification}}: {{Vision}} and {{Reality}}},
  author = {Vardi, Moshe Y.},
  date = {2021-06},
  journaltitle = {Commun. ACM},
  volume = {64},
  number = {7},
  pages = {5},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {0001-0782},
  doi = {10.1145/3469113},
  url = {https://doi.org/10.1145/3469113},
  keywords = {notion}
}

@article{varghaCritiqueImprovementCL2000,
  title = {A Critique and Improvement of the {{CL}} Common Language Effect Size Statistics of {{McGraw}} and {{Wong}}},
  author = {Vargha, András and Delaney, Harold D},
  date = {2000},
  journaltitle = {Journal of Educational and Behavioral Statistics},
  volume = {25},
  number = {2},
  pages = {101--132},
  publisher = {Sage Publications Sage CA: Los Angeles, CA},
  keywords = {notion}
}

@article{verdecchiaKnowYouNeighbor2021,
  title = {Know {{You Neighbor}}: {{Fast Static Prediction}} of {{Test Flakiness}}},
  author = {Verdecchia, Roberto and Cruciani, Emilio and Miranda, Breno and Bertolino, Antonia},
  date = {2021},
  journaltitle = {IEEE Access},
  volume = {9},
  pages = {76119--76134},
  keywords = {notion}
}

@inproceedings{vermaLargescaleClusterManagement2015,
  title = {Large-Scale Cluster Management at {{Google}} with {{Borg}}},
  booktitle = {Proceedings of the {{European Conference}} on {{Computer Systems}} ({{EuroSys}}'15)},
  author = {Verma, Abhishek and Pedrosa, Luis and Korupolu, Madhukar R. and Oppenheimer, David and Tune, Eric and Wilkes, John},
  date = {2015},
  location = {Bordeaux, France},
  doi = {10.1145/2741948.2741964},
  url = {https://dl.acm.org/doi/10.1145/2741948.2741964},
  abstract = {Google's Borg system is a cluster manager that runs hundreds of thousands of jobs, from many thousands of different applications, across a number of clusters each with up to tens of thousands of machines. It achieves high utilization by combining admission control, efficient task-packing, over-commitment, and machine sharing with process-level performance isolation. It supports high-availability applications with runtime features that minimize fault-recovery time, and scheduling policies that reduce the probability of correlated failures. Borg simplifies life for its users by offering a declarative job specification language, name service integration, real-time job monitoring, and tools to analyze and simulate system behavior. We present a summary of the Borg system architecture and features, important design decisions, a quantitative analysis of some of its policy decisions, and a qualitative examination of lessons learned from a decade of operational experience with it.},
  keywords = {notion}
}

@article{viggiatoIdentifyingSimilarTest2022,
  title = {Identifying {{Similar Test Cases That Are Specified}} in {{Natural Language}}},
  author = {Viggiato, Markos and Paas, Dale and Buzon, Chris and Bezemer, Cor-Paul},
  date = {2022},
  journaltitle = {IEEE Transactions on Software Engineering},
  pages = {1--1},
  doi = {10.1109/TSE.2022.3170272},
  keywords = {notion}
}

@inproceedings{vikramGrowingTestCorpus2021,
  title = {Growing {{A Test Corpus}} with {{Bonsai Fuzzing}}},
  booktitle = {2021 {{IEEE}}/{{ACM}} 43rd {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Vikram, Vasudev and Padhye, Rohan and Sen, Koushik},
  date = {2021},
  pages = {723--735},
  doi = {10.1109/ICSE43902.2021.00072},
  keywords = {notion}
}

@inproceedings{vikramGuidingGreyboxFuzzing2023a,
  title = {Guiding {{Greybox Fuzzing}} with {{Mutation Testing}}},
  booktitle = {Proceedings of the 32nd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Vikram, Vasudev and Laybourn, Isabella and Li, Ao and Nair, Nicole and OBrien, Kelton and Sanna, Rafaello and Padhye, Rohan},
  date = {2023-07-13},
  series = {{{ISSTA}} 2023},
  pages = {929--941},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3597926.3598107},
  url = {https://dl.acm.org/doi/10.1145/3597926.3598107},
  urldate = {2024-09-19},
  abstract = {Greybox fuzzing and mutation testing are two popular but mostly independent fields of software testing research that have so far had limited overlap. Greybox fuzzing, generally geared towards   searching for new bugs, predominantly uses code coverage for selecting inputs to save. Mutation testing is primarily used as a stronger alternative to code coverage in assessing the quality of regression tests; the idea is to evaluate tests for their ability to identify artificially injected faults in the target program. But what if we wanted to use greybox fuzzing to synthesize high-quality   regression tests?   In this paper, we develop and evaluate Mu2, a Java-based framework for incorporating mutation analysis in the greybox fuzzing loop, with the goal of producing a test-input corpus with a high   mutation score. Mu2 makes use of a differential oracle for identifying inputs that exercise interesting program behavior without causing crashes. This paper describes several dynamic optimizations implemented in Mu2 to overcome the high cost of performing mutation analysis with every fuzzer-generated input. These optimizations introduce trade-offs in fuzzing throughput and mutation killing ability, which we evaluate empirically on five real-world Java benchmarks. Overall, variants of Mu2 are able to synthesize test-input corpora with a higher mutation score than state-of-the-art Java fuzzer Zest.},
  isbn = {9798400702211},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/XUQWDVUD/Vikram et al. - 2023 - Guiding Greybox Fuzzing with Mutation Testing.pdf}
}

@misc{vowelsDyaDAGsSurvey2021,
  title = {D'ya like {{DAGs}}? {{A Survey}} on {{Structure Learning}} and {{Causal Discovery}}},
  author = {Vowels, Matthew J. and Camgoz, Necati Cihan and Bowden, Richard},
  date = {2021},
  keywords = {notion}
}

@article{walkinshawBoundingRandomTest2024,
  title = {Bounding {{Random Test Set Size}} with {{Computational Learning Theory}}},
  author = {Walkinshaw, Neil and Foster, Michael and Rojas, José Miguel and Hierons, Robert M.},
  date = {2024-07-12},
  journaltitle = {Proc. ACM Softw. Eng.},
  volume = {1},
  pages = {112:2538--112:2560},
  doi = {10.1145/3660819},
  url = {https://dl.acm.org/doi/10.1145/3660819},
  urldate = {2024-10-09},
  abstract = {Random testing approaches work by generating inputs at random, or by selecting inputs randomly from some pre-defined operational profile. One long-standing question that arises in this and other testing contexts is as follows: When can we stop testing? At what point can we be certain that executing further tests in this manner will not explore previously untested (and potentially buggy) software behaviors? This is analogous to the question in Machine Learning, of how many training examples are required in order to infer an accurate model. In this paper we show how probabilistic approaches to answer this question in Machine Learning (arising from Computational Learning Theory) can be applied in our testing context. This enables us to produce an upper bound on the number of tests that are required to achieve a given level of adequacy. We are the first to enable this from only knowing the number of coverage targets (e.g. lines of code) in the source code, without needing to observe a sample test executions. We validate this bound on a large set of Java units, and an autonomous driving system.},
  issue = {FSE},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/N42232ML/Walkinshaw et al. - 2024 - Bounding Random Test Set Size with Computational Learning Theory.pdf}
}

@inproceedings{walkinshawUncertaintyDrivenBlackBoxTest2017,
  title = {Uncertainty-{{Driven Black-Box Test Data Generation}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}} ({{ICST}})},
  author = {Walkinshaw, N. and Fraser, G.},
  date = {2017-03},
  pages = {253--263},
  doi = {10.1109/ICST.2017.30},
  abstract = {We can never be certain that a software system is correct simply by testing it, but with every additional successful test we become less uncertain about its correctness. In absence of source code or elaborate specifications and models, tests are usually generated or chosen randomly. However, rather than randomly choosing tests, it would be preferable to choose those tests that decrease our uncertainty about correctness the most. In order to guide test generation, we apply what is referred to in Machine Learning as "Query Strategy Framework": We infer a behavioural model of the system under test and select those tests which the inferred model is "least certain" about. Running these tests on the system under test thus directly targets those parts about which tests so far have failed to inform the model. We provide an implementation that uses a genetic programming engine for model inference in order to enable an uncertainty sampling technique known as "query by committee", and evaluate it on eight subject systems from the Apache Commons Math framework and JodaTime. The results indicate that test generation using uncertainty sampling outperforms conventional and Adaptive Random Testing.},
  keywords = {adaptive random testing,Apache commons math framework,black-box testing,Data models,genetic algorithms,genetic programming,Genetic programming,genetic programming engine,JodaTime,learning (artificial intelligence),machine learning,Machine learning algorithms,notion,program testing,query strategy framework,sampling methods,Software systems,Subspace constraints,test generation,Testing,Uncertainty,uncertainty sampling,uncertainty-driven black-box test data generation}
}

@article{wallisBinomialConfidenceIntervals2013,
  title = {Binomial {{Confidence Intervals}} and {{Contingency Tests}}: {{Mathematical Fundamentals}} and the {{Evaluation}} of {{Alternative Methods}}},
  author = {Wallis, Sean},
  date = {2013},
  journaltitle = {Journal of Quantitative Linguistics},
  volume = {20},
  pages = {178--208},
  keywords = {notion}
}

@article{wangAttentionPleaseConsider2018,
  title = {Attention {{Please}}: {{Consider Mockito}} When {{Evaluating Newly Released Automated Program Repair Techniques}}},
  author = {Wang, Shangwen and Wen, Ming and Yang, Deheng and Mao, Xiaoguang},
  date = {2018},
  keywords = {notion}
}

@article{wangAutomaticSegmentationMethod2014,
  title = {Automatic {{Segmentation}} of {{Method Code}} into {{Meaningful Blocks}}: {{Design}} and {{Evaluation}}},
  author = {Wang, Xiaoran and Pollock, Lori and Vijay-Shanker, K.},
  date = {2014},
  journaltitle = {Journal of Software: Evolution and Process},
  volume = {26},
  number = {1},
  pages = {27--49},
  doi = {10.1002/smr.1581},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.1581},
  abstract = {SUMMARY Good programming practice and guidelines suggest that programmers use both vertical and horizontal spacing to visibly delineate between code segments that represent different algorithmic steps or high-level actions. Unfortunately, programmers do not always follow these guidelines. Editors and integrated development environments (IDEs) can easily indent codes based on syntax, but they do not currently support automatic blank line insertion, which presents more significant challenges involving the semantics. This paper presents and evaluates a heuristic solution to the automatic blank line insertion problem by leveraging both program structure and naming information to identify `meaningful blocks', consecutive statements that logically implement a high-level action. Our tool, SEGMENT, takes as input a Java method and outputs a segmented version that separates meaningful blocks by vertical spacing. We report on several studies involving human judgments to evaluate the effectiveness of the automatic blank line insertion algorithm, for different size methods and for different levels of programmer expertise. The results indicate strong positive overall opinion of SEGMENT's effectiveness in comparison with both developer-written blank lines and blank lines inserted by newcomers to the code. The results vary only slightly among short and long methods, and among novice and advanced programmers. SEGMENT assists in making users obtain an overall picture of a method's actions and comprehend it quicker as well as provides hints for internal documentation placement. Copyright \textbackslash copyright 2013 John Wiley \& Sons, Ltd.},
  keywords = {automatic formatting,notion,program understanding,readability,software tool}
}

@inproceedings{wangCoRADecomposingDescribing2019,
  title = {{{CoRA}}: {{Decomposing}} and {{Describing Tangled Code Changes}} for {{Reviewer}}},
  booktitle = {2019 34th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Wang, M. and Lin, Z. and Zou, Y. and Xie, B.},
  date = {2019-11},
  pages = {1050--1061},
  issn = {1938-4300},
  doi = {10.1109/ASE.2019.00101},
  keywords = {Code changes decomposition,Code changes description,Code review,notion,Program comprehension}
}

@inproceedings{wangDanceADSOrchestrating2024,
  title = {Dance of the {{ADS}}: {{Orchestrating Failures}} through {{Historically-Informed Scenario Fuzzing}}},
  shorttitle = {Dance of the {{ADS}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Wang, Tong and Gu, Taotao and Deng, Huan and Li, Hu and Kuang, Xiaohui and Zhao, Gang},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {1086--1098},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3680344},
  url = {https://dl.acm.org/doi/10.1145/3650212.3680344},
  urldate = {2024-09-21},
  abstract = {As autonomous driving systems (ADS) advance towards higher levels of autonomy, orchestrating their safety verification becomes increasingly intricate. This paper unveils ScenarioFuzz, a pioneering scenario-based fuzz testing methodology. Designed like a choreographer who understands the past performances, it uncovers vulnerabilities in ADS without the crutch of predefined scenarios. Leveraging map road networks, such as OPENDRIVE, we extract essential data to form a foundational scenario seed corpus. This corpus, enriched with pertinent information, provides the necessary boundaries for fuzz testing in the absence of starting scenarios. Our approach integrates specialized mutators and mutation techniques, combined with a graph neural network model, to predict and filter out high-risk scenario seeds, optimizing the fuzzing process using historical test data. Compared to other methods, our approach reduces the time cost by an average of 60.3\%, while the number of error scenarios discovered per unit of time increases by 103\%. Furthermore, we propose a self-supervised collision trajectory clustering method, which aids in identifying and summarizing 54 high-risk scenario categories prone to inducing ADS faults. Our experiments have successfully uncovered 58 bugs across six tested systems, emphasizing the critical safety concerns of ADS.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/KEEQ8RGU/Wang et al. - 2024 - Dance of the ADS Orchestrating Failures through Historically-Informed Scenario Fuzzing.pdf}
}

@inproceedings{wangDataDrivenSynthesisProvably2021,
  title = {Data-{{Driven Synthesis}} of {{Provably Sound Side Channel Analyses}}},
  booktitle = {2021 {{IEEE}}/{{ACM}} 43rd {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Wang, Jingbo and Sung, Chungha and Raghothaman, Mukund and Wang, Chao},
  date = {2021},
  pages = {810--822},
  doi = {10.1109/ICSE43902.2021.00079},
  keywords = {notion}
}

@article{wangDuelingNetworkArchitectures2015,
  title = {Dueling {{Network Architectures}} for {{Deep Reinforcement Learning}}},
  author = {Wang, Ziyu and family=Freitas, given=Nando, prefix=de, useprefix=false and Lanctot, Marc},
  date = {2015},
  journaltitle = {CoRR},
  volume = {abs/1511.06581},
  url = {http://arxiv.org/abs/1511.06581},
  keywords = {notion}
}

@inproceedings{wangEvaluatingUsefulnessIRbased2015,
  title = {Evaluating the {{Usefulness}} of {{IR-based Fault Localization Techniques}}},
  booktitle = {Proceedings of the 2015 {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Wang, Qianqian and Parnin, Chris and Orso, Alessandro},
  date = {2015},
  series = {{{ISSTA}} 2015},
  pages = {1--11},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2771783.2771797},
  url = {http://doi.acm.org/10.1145/2771783.2771797},
  isbn = {978-1-4503-3620-8},
  venue = {Baltimore, MD, USA},
  keywords = {Fault localization,information retrieval,notion,user studies}
}

@inproceedings{wangHunterNextGenerationCode2016,
  title = {Hunter: {{Next-Generation Code Reuse}} for {{Java}}},
  booktitle = {Proceedings of the 2016 24th {{ACM SIGSOFT International Symposium}} on {{Foundations}} of {{Software Engineering}}},
  author = {Wang, Yuepeng and Feng, Yu and Martins, Ruben and Kaushik, Arati and Dillig, Isil and Reiss, Steven P.},
  date = {2016},
  series = {{{FSE}} 2016},
  pages = {1028--1032},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2950290.2983934},
  url = {https://doi.org/10.1145/2950290.2983934},
  abstract = {In many common scenarios, programmers need to implement functionality that is already provided by some third party library. This paper presents a tool called Hunter that facilitates code reuse by finding relevant methods in large code bases and automatically synthesizing any necessary wrapper code. Since Hunter internally uses advanced program synthesis technology, it can automatically reuse existing methods even when code adaptation is necessary. We have implemented Hunter as an Eclipse plug-in and evaluate it by (a) comparing it against S6, a state-of-the-art code reuse tool, and (b) performing a user study. Our evaluation shows that Hunter compares favorably with S6 and increases programmer productivity.},
  isbn = {978-1-4503-4218-6},
  venue = {Seattle, WA, USA},
  keywords = {code adaptation,code reuse,notion,program synthesis}
}

@inproceedings{wangOptimalConcolicTesting2018,
  title = {Towards Optimal Concolic Testing},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Software Engineering}}},
  author = {Wang, Xinyu and Sun, Jun and Chen, Zhenbang and Zhang, Peixin and Wang, Jingyi and Lin, Yun},
  date = {2018},
  pages = {291--302},
  doi = {10.1145/3180155.3180177},
  keywords = {notion}
}

@inproceedings{wangProbabilisticDeltaDebugging2021,
  title = {Probabilistic {{Delta Debugging}}},
  booktitle = {Proceedings of the 2021 15th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Wang, Guancheng and Shen, Ruobing and Chen, Junjie and Xiong, Yingfei and Zhang, Lu},
  date = {2021},
  series = {{{ESEC}}/{{FSE}} 2021},
  keywords = {notion}
}

@inproceedings{wangQTEPQualityawareTest2017,
  title = {{{QTEP}}: {{Quality-aware Test Case Prioritization}}},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Wang, Song and Nam, Jaechang and Tan, Lin},
  date = {2017},
  series = {{{ESEC}}/{{FSE}} 2017},
  pages = {523--534},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/3106237.3106258},
  url = {http://doi.acm.org/10.1145/3106237.3106258},
  isbn = {978-1-4503-5105-8},
  venue = {Paderborn, Germany},
  keywords = {notion,QTEP,Test Case Prioritization}
}

@article{wangSoKProgressChallenges2020,
  title = {{{SoK}}: {{The Progress}}, {{Challenges}}, and {{Perspectives}} of {{Directed Greybox Fuzzing}}},
  author = {Wang, Pengfei and Zhou, Xu},
  date = {2020},
  journaltitle = {CoRR},
  volume = {abs/2005.11907},
  eprint = {2005.11907},
  eprinttype = {arXiv},
  url = {https://arxiv.org/abs/2005.11907},
  keywords = {notion}
}

@inproceedings{wangStatisticalModelChecking2021,
  title = {Statistical Model Checking for Hyperproperties},
  booktitle = {2021 {{IEEE}} 34th {{Computer Security Foundations Symposium}} ({{CSF}})},
  author = {Wang, Yu and Nalluri, Siddhartha and Bonakdarpour, Borzoo and Pajic, Miroslav},
  date = {2021},
  pages = {1--16},
  publisher = {IEEE},
  keywords = {notion}
}

@inproceedings{wangTamingCoincidentalCorrectness2009,
  title = {Taming Coincidental Correctness: {{Coverage}} Refinement with Context Patterns to Improve Fault Localization},
  booktitle = {2009 {{IEEE}} 31st {{International Conference}} on {{Software Engineering}}},
  author = {Wang, Xinming and Cheung, S.C. and Chan, W.K. and Zhang, Zhenyu},
  date = {2009},
  pages = {45--55},
  doi = {10.1109/ICSE.2009.5070507},
  keywords = {notion}
}

@inproceedings{wangUsingCompressedBytecode2004,
  title = {Using Compressed Bytecode Traces for Slicing {{Java}} Programs},
  booktitle = {Proceedings. 26th {{International Conference}} on {{Software Engineering}}},
  author = {Wang, Tao and Roychoudhury, A.},
  date = {2004-05},
  pages = {512--521},
  issn = {0270-5257},
  doi = {10.1109/ICSE.2004.1317473},
  abstract = {Dynamic slicing is a well-known program debugging technique. Given a program P and input I, it finds all program statements which directly/indirectly affect the values of some variables' occurrences when P is executed with I. Dynamic slicing algorithms often proceed by traversing the execution trace of P produced by input I (or a dependence graph which captures control/data flow in the execution trace). Consequently, it is important to develop space efficient representations of the execution trace. In this paper, we use results from data compression to compactly represent bytecode traces of sequential Java programs. The major space savings come from the optimized representation of data (instruction) addresses used by memory reference (branch) bytecodes as operands. We give detailed experimental results on the space efficiency and time overheads for our compact trace representation. We then show how dynamic slicing algorithms can directly traverse our compact traces without resorting to costly decompression. We also develop an extension of dynamic slicing which allows us to explain omission errors (i.e. why some events did not happen during program execution).},
  keywords = {Bytecode Trace,Java,notion,Program Slicing}
}

@article{weiConvolutionalPoseMachines2016,
  title = {Convolutional {{Pose Machines}}},
  author = {Wei, Shih-En and Ramakrishna, Varun and Kanade, Takeo and Sheikh, Yaser},
  date = {2016},
  journaltitle = {CoRR},
  volume = {abs/1602.00134},
  url = {http://arxiv.org/abs/1602.00134},
  keywords = {notion}
}

@inproceedings{weiLambdaNetProbabilisticType2020,
  title = {{{LambdaNet}}: {{Probabilistic Type Inference}} Using {{Graph Neural Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Wei, Jiayi and Goyal, Maruth and Durrett, Greg and Dillig, Isil},
  date = {2020},
  url = {https://openreview.net/forum?id=Hkx6hANtwH},
  keywords = {notion}
}

@inproceedings{weiLDAbasedDocumentModels2006,
  title = {{{LDA-based Document Models}} for {{Ad-hoc Retrieval}}},
  booktitle = {Proceedings of the 29th {{Annual International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Wei, Xing and Croft, W. Bruce},
  date = {2006},
  series = {{{SIGIR}} '06},
  pages = {178--185},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/1148170.1148204},
  url = {http://doi.acm.org/10.1145/1148170.1148204},
  isbn = {1-59593-369-7},
  venue = {Seattle, Washington, USA},
  keywords = {Information Retrieval,Language Model,Latent Dirichlet Allocation (LDA),notion}
}

@inproceedings{weimerAutomaticallyFindingPatches2009,
  title = {Automatically {{Finding Patches Using Genetic Programming}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Software Engineering}}},
  author = {Weimer, Westley and Nguyen, ThanhVu and Le Goues, Claire and Forrest, Stephanie},
  date = {2009},
  series = {{{ICSE}} '09},
  pages = {364--374},
  publisher = {IEEE Computer Society},
  location = {Washington, DC, USA},
  doi = {10.1109/ICSE.2009.5070536},
  url = {http://dx.doi.org/10.1109/ICSE.2009.5070536},
  isbn = {978-1-4244-3453-4},
  keywords = {Automatic Program Repair,Genetic Programming (GP),notion}
}

@inproceedings{weimerLeveragingProgramEquivalence2013,
  title = {Leveraging {{Program Equivalence}} for {{Adaptive Program Repair}}: {{Models}} and {{First Results}}},
  booktitle = {Proceedings of the 28th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Weimer, Westley and Fry, Zachary P. and Forrest, Stephanie},
  date = {2013},
  series = {{{ASE}}'13},
  pages = {356--366},
  publisher = {IEEE Press},
  location = {Piscataway, NJ, USA},
  doi = {10.1109/ASE.2013.6693094},
  url = {https://doi.org/10.1109/ASE.2013.6693094},
  isbn = {978-1-4799-0215-6},
  venue = {Silicon Valley, CA, USA},
  keywords = {Automated Program Repair,Mutation Testing,notion,Program Equivalence,SBSE}
}

@inproceedings{weimerMiningTemporalSpecifications2005,
  title = {Mining {{Temporal Specifications}} for {{Error Detection}}},
  booktitle = {Tools and {{Algorithms}} for the {{Construction}} and {{Analysis}} of {{Systems}}},
  author = {Weimer, Westley and Necula, George C.},
  editor = {Halbwachs, Nicolas and Zuck, Lenore D.},
  date = {2005},
  pages = {461--476},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  abstract = {Specifications are necessary in order to find software bugs using program verification tools. This paper presents a novel automatic specification mining algorithm that uses information about error handling to learn temporal safety rules. Our algorithm is based on the observation that programs often make mistakes along exceptional control-flow paths, even when they behave correctly on normal execution paths. We show that this focus improves the effectiveness of the miner for discovering specifications beneficial for bug finding.},
  isbn = {978-3-540-31980-1},
  keywords = {notion}
}

@inproceedings{weiserDATADifferentialAddress2018,
  title = {{{DATA}} – {{Differential Address Trace Analysis}}: {{Finding Address-based Side-Channels}} in {{Binaries}}},
  booktitle = {27th {{USENIX Security Symposium}} ({{USENIX Security}} 18)},
  author = {Weiser, Samuel and Zankl, Andreas and Spreitzer, Raphael and Miller, Katja and Mangard, Stefan and Sigl, Georg},
  date = {2018-08},
  pages = {603--620},
  publisher = {USENIX Association},
  location = {Baltimore, MD},
  url = {https://www.usenix.org/conference/usenixsecurity18/presentation/weiser},
  isbn = {978-1-939133-04-5},
  keywords = {notion}
}

@inproceedings{weiserProgramSlicing1981,
  title = {Program Slicing},
  booktitle = {Proceedings of the 5th International Conference on {{Software}} Engineering},
  author = {Weiser, Mark},
  date = {1981},
  pages = {439--449},
  publisher = {IEEE Press},
  keywords = {notion,Program Slicing}
}

@inproceedings{weishiAlleviatingImpactCoincidental2014,
  title = {Alleviating the Impact of Coincidental Correctness on the Effectiveness of Sfl by Clustering Test Cases},
  booktitle = {2014 {{Theoretical Aspects}} of {{Software Engineering Conference}}},
  author = {Weishi, Li and Mao, Xiaoguang},
  date = {2014},
  pages = {66--69},
  publisher = {IEEE},
  keywords = {notion}
}

@inproceedings{wenContextAwarePatchGeneration2018,
  title = {Context-{{Aware Patch Generation}} for {{Better Automated Program Repair}}},
  author = {Wen, Ming and Chen, Junjie and Wu, Rongxin and Hao, Dan and Cheung, Shing-Chi},
  date = {2018},
  keywords = {notion}
}

@inproceedings{wenExploringExploitingCorrelations2019,
  title = {Exploring and Exploiting the Correlations between Bug-Inducing and Bug-Fixing Commits},
  booktitle = {Proceedings of the 2019 27th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Wen, Ming and Wu, Rongxin and Liu, Yepang and Tian, Yongqiang and Xie, Xuan and Cheung, Shing-Chi and Su, Zhendong},
  date = {2019},
  series = {{{ESEC}}/{{FSE}} 2019},
  pages = {326--337},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3338906.3338962},
  url = {https://doi.org/10.1145/3338906.3338962},
  abstract = {Bug-inducing commits provide important information to understand when and how bugs were introduced. Therefore, they have been extensively investigated by existing studies and frequently leveraged to facilitate bug fixings in industrial practices. Due to the importance of bug-inducing commits in software debugging, we are motivated to conduct the first systematic empirical study to explore the correlations between bug-inducing and bug-fixing commits in terms of code elements and modifications. To facilitate the study, we collected the inducing and fixing commits for 333 bugs from seven large open-source projects. The empirical findings reveal important and significant correlations between a bug's inducing and fixing commits. We further exploit the usefulness of such correlation findings from two aspects. First, they explain why the SZZ algorithm, the most widely-adopted approach to collecting bug-inducing commits, is imprecise. In view of SZZ's imprecision, we revisited the findings of previous studies based on SZZ, and found that 8 out of 10 previous findings are significantly affected by SZZ's imprecision. Second, they shed lights on the design of automated debugging techniques. For demonstration, we designed approaches that exploit the correlations with respect to statements and change actions. Our experiments on Defects4J show that our approaches can boost the performance of fault localization significantly and also advance existing APR techniques.},
  isbn = {978-1-4503-5572-8},
  venue = {Tallinn, Estonia},
  keywords = {bug-inducing commits,Empirical study,fault localization and repair,notion}
}

@inproceedings{wengLecture11GoodTuring2010,
  title = {Lecture 11 : {{The Good-Turing Estimate Scribes}} :},
  author = {Weng, Ellis and Owens, Andrew},
  date = {2010},
  keywords = {notion}
}

@article{wenHistoricalSpectrumBased2019,
  title = {Historical {{Spectrum}} Based {{Fault Localization}}},
  author = {Wen, M. and Chen, J. and Tian, Y. and Wu, R. and Hao, D. and Han, S. and Cheung, S. C.},
  date = {2019},
  journaltitle = {IEEE Transactions on Software Engineering},
  pages = {1--1},
  doi = {10.1109/TSE.2019.2948158},
  keywords = {notion}
}

@inproceedings{wenLocusLocatingBugs2016,
  title = {Locus: {{Locating}} Bugs from Software Changes},
  booktitle = {2016 31st {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Wen, M. and Wu, R. and Cheung, S. C.},
  date = {2016-09},
  pages = {262--273},
  abstract = {Various information retrieval (IR) based techniques have been proposed recently to locate bugs automatically at the file level. However, their usefulness is often compromised by the coarse granularity of files and the lack of contextual information. To address this, we propose to locate bugs using software changes, which offer finer granularity than files and provide important contextual clues for bug-fixing. We observe that bug inducing changes can facilitate the bug fixing process. For example, it helps triage the bug fixing task to the developers who committed the bug inducing changes or enables developers to fix bugs by reverting these changes. Our study further identifies that change logs and the naturally small granularity of changes can help boost the performance of IR-based bug localization. Motivated by these observations, we propose an IR-based approach Locus to locate bugs from software changes, and evaluate it on six large open source projects. The results show that Locus outperforms existing techniques at the source file level localization significantly. MAP and MRR in particular have been improved, on average, by 20.1\% and 20.5\%, respectively. Locus is also capable of locating the inducing changes within top 5 for 41.0\% of the bugs. The results show that Locus can significantly reduce the number of lines needing to be scanned to locate the bug compared with existing techniques.},
  keywords = {Fault Localization,Information Retrieval,Locus,notion}
}

@inproceedings{wenSoftwareFaultLocalization2012,
  title = {Software {{Fault Localization Based}} on {{Program Slicing Spectrum}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Software Engineering}}},
  author = {Wen, Wanzhi},
  date = {2012},
  series = {{{ICSE}} '12},
  pages = {1511--1514},
  publisher = {IEEE Press},
  location = {Piscataway, NJ, USA},
  url = {http://dl.acm.org/citation.cfm?id=2337223.2337469},
  isbn = {978-1-4673-1067-3},
  venue = {Zurich, Switzerland},
  keywords = {Fault Localization,notion,Program Slicing}
}

@article{whiteGINoTime2017,
  title = {{{GI}} in {{No Time}}},
  author = {White, David R.},
  date = {2017},
  journaltitle = {GECCO},
  keywords = {Genetic Improvement (GI),notion}
}

@article{whiteSortingTransformingProgram2017,
  title = {Sorting and {{Transforming Program Repair Ingredients}} via {{Deep Learning Code Similarities}}},
  author = {White, Martin and Tufano, Michele and Martinez, Matias and Monperrus, Martin and Poshyvanyk, Denys},
  date = {2017},
  journaltitle = {CoRR},
  volume = {abs/1707.04742},
  url = {http://arxiv.org/abs/1707.04742},
  keywords = {notion}
}

@inproceedings{wichelmannMicrowalkCIPracticalSideChannel2022,
  title = {Microwalk-{{CI}}: {{Practical Side-Channel Analysis}} for {{JavaScript Applications}}},
  booktitle = {Proceedings of the 2022 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Wichelmann, Jan and Sieck, Florian and Pätschke, Anna and Eisenbarth, Thomas},
  date = {2022},
  series = {{{CCS}} '22},
  pages = {2915--2929},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3548606.3560654},
  url = {https://doi.org/10.1145/3548606.3560654},
  abstract = {Secret-dependent timing behavior in cryptographic implementations has resulted in exploitable vulnerabilities, undermining their security. Over the years, numerous tools to automatically detect timing leakage or even to prove their absence have been proposed. However, a recent study at IEEE S\&P 2022 showed that, while many developers are aware of one or more analysis tools, they have major difficulties integrating these into their workflow, as existing tools are tedious to use and mapping discovered leakages to their originating code segments requires expert knowledge. In addition, existing tools focus on compiled languages like C, or analyze binaries, while the industry and open-source community moved to interpreted languages, most notably JavaScript. In this work, we introduce Microwalk-CI, a novel side-channel analysis framework for easy integration into a JavaScript development workflow. First, we extend existing dynamic approaches with a new analysis algorithm, that allows efficient localization and quantification of leakages, making it suitable for use in practical development. We then present a technique for generating execution traces from JavaScript applications, which can be further analyzed with our and other algorithms originally designed for binary analysis. Finally, we discuss how Microwalk-CI can be integrated into a continuous integration (CI) pipeline for efficient and ongoing monitoring. We evaluate our analysis framework by conducting a thorough evaluation of several popular JavaScript cryptographic libraries, and uncover a number of critical leakages.},
  isbn = {978-1-4503-9450-5},
  venue = {Los Angeles, CA, USA},
  keywords = {leakage analysis,notion,side-channel attacks,software development}
}

@inproceedings{wichelmannMicroWalkFrameworkFinding2018,
  title = {{{MicroWalk}}: {{A Framework}} for {{Finding Side Channels}} in {{Binaries}}},
  booktitle = {Proceedings of the 34th {{Annual Computer Security Applications Conference}}},
  author = {Wichelmann, Jan and Moghimi, Ahmad and Eisenbarth, Thomas and Sunar, Berk},
  date = {2018},
  series = {{{ACSAC}} '18},
  pages = {161--173},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3274694.3274741},
  url = {https://doi.org/10.1145/3274694.3274741},
  abstract = {Microarchitectural side channels expose unprotected software to information leakage attacks where a software adversary is able to track runtime behavior of a benign process and steal secrets such as cryptographic keys. As suggested by incremental software patches for the RSA algorithm against variants of side-channel attacks within different versions of cryptographic libraries, protecting security-critical algorithms against side channels is an intricate task. Software protections avoid leakages by operating in constant time with a uniform resource usage pattern independent of the processed secret. In this respect, automated testing and verification of software binaries for leakage-free behavior is of importance, particularly when the source code is not available. In this work, we propose a novel technique based on Dynamic Binary Instrumentation and Mutual Information Analysis to efficiently locate and quantify memory based and control-flow based microarchitectural leakages. We develop a software framework named MicroWalk for side-channel analysis of binaries which can be extended to support new classes of leakage. For the first time, by utilizing MicroWalk, we perform rigorous leakage analysis of two widely-used closed-source cryptographic libraries: Intel IPP and Microsoft CNG. We analyze 15 different cryptographic implementations consisting of 112 million instructions in about 105 minutes of CPU time. By locating previously unknown leakages in hardened implementations, our results suggest that MicroWalk can efficiently find microarchitectural leakages in software binaries.},
  isbn = {978-1-4503-6569-7},
  venue = {San Juan, PR, USA},
  keywords = {binary instrumentation,cache attacks,constant time,cryptographic implementations,dynamic program analysis,microarchitectural leakage,mutual information,notion,side channel}
}

@inproceedings{wildeLocatingUserFunctionality1992,
  title = {Locating User Functionality in Old Code},
  booktitle = {Proceedings {{Conference}} on {{Software Maintenance}} 1992},
  author = {Wilde, N. and Gomez, J.A. and Gust, T. and Strasburg, D.},
  date = {1992},
  pages = {200--205},
  doi = {10.1109/ICSM.1992.242542},
  keywords = {notion}
}

@misc{wildNeurallyGuidedGenetic2021,
  title = {Neurally {{Guided Genetic Programming}} for {{Turing Complete Programming}} by {{Example}}},
  author = {Wild, Alexander Newton and Porter, Barry},
  date = {2021},
  url = {https://openreview.net/forum?id=O358nrve1W},
  keywords = {notion}
}

@article{wilhelmWorstcaseExecutiontimeProblem2008,
  title = {The Worst-Case Execution-Time Problem—Overview of Methods and Survey of Tools},
  author = {Wilhelm, Reinhard and Engblom, Jakob and Ermedahl, Andreas and Holsti, Niklas and Thesing, Stephan and Whalley, David and Bernat, Guillem and Ferdinand, Christian and Heckmann, Reinhold and Mitra, Tulika and Mueller, Frank and Puaut, Isabelle and Puschner, Peter and Staschulat, Jan and Stenström, Per},
  date = {2008-05-08},
  journaltitle = {ACM Trans. Embed. Comput. Syst.},
  volume = {7},
  number = {3},
  pages = {36:1--36:53},
  issn = {1539-9087},
  doi = {10.1145/1347375.1347389},
  url = {https://dl.acm.org/doi/10.1145/1347375.1347389},
  urldate = {2024-11-12},
  abstract = {The determination of upper bounds on execution times, commonly called worst-case execution times (WCETs), is a necessary step in the development and validation process for hard real-time systems. This problem is hard if the underlying processor architecture has components, such as caches, pipelines, branch prediction, and other speculative components. This article describes different approaches to this problem and surveys several commercially available tools1 and research prototypes.},
  file = {/Users/bohrok/Zotero/storage/9UHWBW5D/Wilhelm et al. - 2008 - The worst-case execution-time problem—overview of methods and survey of tools.pdf}
}

@report{wilkesGoogleClusterusageTraces2020,
  type = {Technical Report},
  title = {Google Cluster-Usage Traces V3},
  author = {Wilkes, John},
  date = {2020-04},
  institution = {Google Inc.},
  location = {Mountain View, CA, USA},
  abstract = {This document describes the semantics, data format, and schema of usage traces of a few Google compute cells. This document describes version 3 of the trace format.},
  keywords = {notion}
}

@misc{wilkesMoreGoogleCluster2011,
  title = {More {{Google}} Cluster Data},
  author = {Wilkes, John},
  date = {2011-11},
  location = {Mountain View, CA, USA},
  keywords = {notion},
  annotation = {Published: Google research blog}
}

@article{wilsonProbableInferenceLaw1927,
  title = {Probable Inference, the Law of Succession, and Statistical Inference},
  author = {Wilson, Edwin B},
  date = {1927},
  journaltitle = {Journal of the American Statistical Association},
  volume = {22},
  number = {158},
  pages = {209--212},
  publisher = {Taylor \& Francis},
  doi = {10.1080/01621459.1927.10502953},
  keywords = {notion}
}

@unpublished{wolffExplainableFuzzerEvaluation2022,
  title = {Explainable {{Fuzzer Evaluation}}},
  author = {Wolff, Dylan and Böhme, Marcel and Roychoudhury, Abhik},
  date = {2022},
  eprint = {2212.09519},
  eprinttype = {arXiv},
  keywords = {notion}
}

@article{wolpertEstimatingFunctionsProbability1995,
  title = {Estimating Functions of Probability Distributions from a Finite Set of Samples},
  author = {Wolpert, David H and Wolf, David R},
  date = {1995},
  journaltitle = {Physical Review E},
  volume = {52},
  number = {6},
  pages = {6841},
  publisher = {APS},
  keywords = {notion}
}

@inproceedings{wongBoostingBugReportOrientedFault2014,
  title = {Boosting {{Bug-Report-Oriented Fault Localization}} with {{Segmentation}} and {{Stack-Trace Analysis}}},
  booktitle = {2014 {{IEEE International Conference}} on {{Software Maintenance}} and {{Evolution}}},
  author = {Wong, C. P. and Xiong, Y. and Zhang, H. and Hao, D. and Zhang, L. and Mei, H.},
  date = {2014-09},
  pages = {181--190},
  issn = {1063-6773},
  doi = {10.1109/ICSME.2014.40},
  abstract = {To deal with post-release bugs, many software projects set up public bug repositories for users all over the world to report bugs that they have encountered. Recently, researchers have proposed various information retrieval based approaches to localizing faults based on bug reports. In these approaches, source files are processed as single units, where noise in large files may affect the accuracy of fault localization. Furthermore, bug reports often contain stack-trace information, but existing approaches often treat this information as plain text. In this paper, we propose to use segmentation and stack-trace analysis to improve the performance of bug localization. Specifically, given a bug report, we divide each source code file into a series of segments and use the segment most similar to the bug report to represent the file. We also analyze the bug report to identify possible faulty files in a stack trace and favor these files in our retrieval. According to our empirical results, our approach is able to significantly improve Bug Locator, a representative fault localization approach, on all the three software projects (i.e., Eclipse, AspectJ, and SWT) used in our empirical evaluation. Furthermore, segmentation and stack-trace analysis are complementary to each other for boosting the performance of bug-report-oriented fault localization.},
  keywords = {AspectJ,boosting bug-report-oriented fault localization,bug localization,bug locator,bug report,bug reportq,Computer bugs,Eclipse,fault diagnosis,fault localization,faulty files,feature location,information retrieval,Information retrieval,Java,Logistics,Measurement,Noise,notion,post-release bugs,program debugging,program diagnostics,public bug repository,representative fault localization approach,Software,software projects,source code (software),source code file,source files,stack-trace analysis,stack-trace information,SWT}
}

@misc{wongEfficientlyFindingHigherOrder2020,
  title = {Efficiently {{Finding Higher-Order Mutants}}},
  author = {Wong, Chu-Pan and Meinicke, Jens and Chen, Leo and Diniz, João P. and Kästner, Christian and Figueiredo, Eduardo},
  date = {2020},
  keywords = {notion}
}

@article{wongFamilyCodeCoveragebased2010,
  title = {A Family of Code Coverage-Based Heuristics for Effective Fault Localization},
  author = {Wong, W. Eric and Debroy, Vidroha and Choi, Byoungju},
  date = {2010},
  journaltitle = {J. Syst. Softw.},
  volume = {83},
  pages = {188--208},
  keywords = {notion}
}

@article{wongFasterVariationalExecution2018,
  title = {Faster {{Variational Execution}} with {{Transparent Bytecode Transformation}}},
  author = {Wong, Chu-Pan and Meinicke, Jens and Lazarek, Lukas and Kästner, Christian},
  date = {2018-10},
  journaltitle = {Proc. ACM Program. Lang.},
  volume = {2},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3276487},
  url = {https://doi.org/10.1145/3276487},
  abstract = {Variational execution is a novel dynamic analysis technique for exploring highly configurable systems and accurately tracking information flow. It is able to efficiently analyze many configurations by aggressively sharing redundancies of program executions. The idea of variational execution has been demonstrated to be effective in exploring variations in the program, especially when the configuration space grows out of control. Existing implementations of variational execution often require heavy lifting of the runtime interpreter, which is painstaking and error-prone. Furthermore, the performance of this approach is suboptimal. For example, the state-of-the-art variational execution interpreter for Java, VarexJ, slows down executions by 100 to 800 times over a single execution for small to medium size Java programs. Instead of modifying existing JVMs, we propose to transform existing bytecode to make it variational, so it can be executed on an unmodified commodity JVM. Our evaluation shows a dramatic improvement on performance over the state-of-the-art, with a speedup of 2 to 46 times, and high efficiency in sharing computations.},
  issue = {OOPSLA},
  keywords = {Bytecode Transformation,Configurable System,Java Virtual Machine,notion,Variational Execution}
}

@article{wongSurveySoftwareFault2016,
  title = {A {{Survey}} on {{Software Fault Localization}}},
  author = {Wong, W. E. and Gao, R. and Li, Y. and Abreu, R. and Wotawa, F.},
  date = {2016-08},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {42},
  number = {8},
  pages = {707--740},
  issn = {0098-5589},
  doi = {10.1109/TSE.2016.2521368},
  abstract = {Software fault localization, the act of identifying the locations of faults in a program, is widely recognized to be one of the most tedious, time consuming, and expensive - yet equally critical - activities in program debugging. Due to the increasing scale and complexity of software today, manually locating faults when failures occur is rapidly becoming infeasible, and consequently, there is a strong demand for techniques that can guide software developers to the locations of faults in a program with minimal human intervention. This demand in turn has fueled the proposal and development of a broad spectrum of fault localization techniques, each of which aims to streamline the fault localization process and make it more effective by attacking the problem in a unique way. In this article, we catalog and provide a comprehensive overview of such techniques and discuss key issues and concerns that are pertinent to software fault localization as a whole.},
  keywords = {Complexity theory,Computer bugs,Debugging,execution trace,Fault diagnosis,human intervention,notion,program debugging,program fault locations,Software debugging,software developers,Software engineering,software fault localization,Software fault localization,software reliability,software testing,survey,suspicious code}
}

@inproceedings{wongVarFixBalancingEdit2021,
  title = {{{VarFix}}: {{Balancing Edit Expressiveness}} and {{Search Effectiveness}} in {{Automated Program Repair}}},
  booktitle = {Proceedings of the 29th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Wong, Chu-Pan and Santiesteban, Priscila and Kästner, Christian and Le Goues, Claire},
  date = {2021},
  series = {{{ESEC}}/{{FSE}} 2021},
  pages = {354--366},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3468264.3468600},
  url = {https://doi.org/10.1145/3468264.3468600},
  abstract = {Automatically repairing a buggy program is essentially a search problem, searching for code transformations that pass a set of tests. Various search strategies have been explored, but they either navigate the search space in an ad hoc way using heuristics, or systemically but at the cost of limited edit expressiveness in the kinds of supported program edits. In this work, we explore the possibility of systematically navigating the search space without sacrificing edit expressiveness. The key enabler of this exploration is variational execution, a dynamic analysis technique that has been shown to be effective at exploring many similar executions in large search spaces. We evaluate our approach on IntroClassJava and Defects4J, showing that a systematic search is effective at leveraging and combining fixing ingredients to find patches, including many high-quality patches and multi-edit patches.},
  isbn = {978-1-4503-8562-6},
  venue = {Athens, Greece},
  keywords = {automatic program repair,notion,variational execution}
}

@article{wuCHEBYSHEVPOLYNOMIALSMOMENT2019,
  title = {{{CHEBYSHEV POLYNOMIALS}}, {{MOMENT MATCHING}}, {{AND OPTIMAL ESTIMATION OF THE UNSEEN}}},
  author = {Wu, Yihong and Yang, Pengkun},
  date = {2019},
  journaltitle = {The Annals of Statistics},
  volume = {47},
  number = {2},
  eprint = {26581884},
  eprinttype = {jstor},
  pages = {pp. 857--883},
  publisher = {Institute of Mathematical Statistics},
  issn = {00905364, 21688966},
  url = {https://www.jstor.org/stable/26581884},
  urldate = {2023-04-08},
  abstract = {We consider the problem of estimating the support size of a discrete distribution whose minimum nonzero mass is at least 1 k . Under the independent sampling model, we show that the sample complexity, that is, the minimal sample size to achieve an additive error of εk with probability at least 0.1 is within universal constant factors of k logk log 2 1 ε , which improves the state-of-the-art result of k ε 2 logk in [In Advances in Neural Information Processing Systems (2013) 2157–2165]. Similar characterization of the minimax risk is also obtained. Our procedure is a linear estimator based on the Chebyshev polynomial and its approximation-theoretic properties, which can be evaluated in O(n+log² k) time and attains the sample complexity within constant factors. The superiority of the proposed estimator in terms of accuracy, computational efficiency and scalability is demonstrated in a variety of synthetic and real datasets.},
  keywords = {notion}
}

@inproceedings{wuEliminatingTimingSideChannel2018,
  title = {Eliminating {{Timing Side-Channel Leaks Using Program Repair}}},
  booktitle = {Proceedings of the 27th {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Wu, Meng and Guo, Shengjian and Schaumont, Patrick and Wang, Chao},
  date = {2018},
  series = {{{ISSTA}} 2018},
  pages = {15--26},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3213846.3213851},
  url = {https://doi.org/10.1145/3213846.3213851},
  abstract = {We propose a method, based on program analysis and transformation, for eliminating timing side channels in software code that implements security-critical applications. Our method takes as input the original program together with a list of secret variables (e.g., cryptographic keys, security tokens, or passwords) and returns the transformed program as output. The transformed program is guaranteed to be functionally equivalent to the original program and free of both instruction- and cache-timing side channels. Specifically, we ensure that the number of CPU cycles taken to execute any path is independent of the secret data, and the cache behavior of memory accesses, in terms of hits and misses, is independent of the secret data. We have implemented our method in LLVM and validated its effectiveness on a large set of applications, which are cryptographic libraries with 19,708 lines of C/C++ code in total. Our experiments show the method is both scalable for real applications and effective in eliminating timing side channels.},
  isbn = {978-1-4503-5699-2},
  venue = {Amsterdam, Netherlands},
  keywords = {abstract interpretation,cache,countermeasure,notion,program repair,program synthesis,Side-channel attack,static analysis,timing}
}

@inproceedings{wuenscheDetectingHigherOrderMerge2020,
  title = {Detecting {{Higher-Order Merge Conflicts}} in {{Large Software Projects}}},
  booktitle = {2020 {{IEEE}} 13th {{International Conference}} on {{Software Testing}}, {{Validation}} and {{Verification}} ({{ICST}})},
  author = {Wuensche, T. and Andrzejak, A. and Schwedes, S.},
  date = {2020},
  pages = {353--363},
  doi = {10.1109/ICST46399.2020.00043},
  keywords = {notion}
}

@article{wuEvaluatingImprovingNeural2022,
  title = {Evaluating and {{Improving Neural Program-Smoothing-based Fuzzing}}},
  author = {Wu, Mingyuan and Jiang, Lingixao and Xiang, Jiahong and Zhang, Yuqun and Yang, Guowei and Ma, Huixin and Nie, Sen and Wu, Shi and Cui, Heming and Zhang, Lingming},
  date = {2022},
  journaltitle = {2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)},
  pages = {847--858},
  url = {https://api.semanticscholar.org/CorpusID:248913280},
  keywords = {notion}
}

@inproceedings{wuLogosLogGuided2024,
  title = {Logos: {{Log Guided Fuzzing}} for {{Protocol Implementations}}},
  shorttitle = {Logos},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Wu, Feifan and Luo, Zhengxiong and Zhao, Yanyang and Du, Qingpeng and Yu, Junze and Peng, Ruikang and Shi, Heyuan and Jiang, Yu},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {1720--1732},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3680394},
  url = {https://dl.acm.org/doi/10.1145/3650212.3680394},
  urldate = {2024-09-21},
  abstract = {Network protocols are extensively used in a variety of network devices, making the security of their implementations crucial. Protocol fuzzing has shown promise in uncovering vulnerabilities in these implementations. However traditional methods often require instrumentation of the target implementation to provide guidance, which is intrusive, adds overhead, and can hinder black-box testing. This paper presents Logos, a protocol fuzzer that utilizes non-intrusive runtime log information for fuzzing guidance. Logos first standardizes the unstructured logs and embeds them into a high-dimensional vector space for semantic representation.Then, Logos filters the semantic representation and dynamically maintains a semantic coverage to chart the explored space for customized guidance.We evaluate Logos on eight widely used implementations of well-known protocols. Results show that, compared to existing intrusive or expert knowledge-driven protocol fuzzers, Logos achieves 26.75\%-106.19\% higher branch coverage within 24 hours. Furthermore, Logos exposed 12 security-critical vulnerabilities in these prominent protocol implementations, with 9 CVEs assigned.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/G9YS4WRT/Wu et al. - 2024 - Logos Log Guided Fuzzing for Protocol Implementations.pdf}
}

@online{xiaKeepConversationGoing2023,
  title = {Keep the {{Conversation Going}}: {{Fixing}} 162 out of 337 Bugs for \$0.42 Each Using {{ChatGPT}}},
  shorttitle = {Keep the {{Conversation Going}}},
  author = {Xia, Chunqiu Steven and Zhang, Lingming},
  date = {2023-04-01},
  eprint = {2304.00385},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.00385},
  url = {http://arxiv.org/abs/2304.00385},
  urldate = {2024-09-19},
  abstract = {Automated Program Repair (APR) aims to automatically generate patches for buggy programs. Recent APR work has been focused on leveraging modern Large Language Models (LLMs) to directly generate patches for APR. Such LLM-based APR tools work by first constructing an input prompt built using the original buggy code and then queries the LLM to generate patches. While the LLM-based APR tools are able to achieve state-of-the-art results, it still follows the classic Generate and Validate repair paradigm of first generating lots of patches and then validating each one afterwards. This not only leads to many repeated patches that are incorrect but also miss the crucial information in test failures as well as in plausible patches. To address these limitations, we propose ChatRepair, the first fully automated conversation-driven APR approach that interleaves patch generation with instant feedback to perform APR in a conversational style. ChatRepair first feeds the LLM with relevant test failure information to start with, and then learns from both failures and successes of earlier patching attempts of the same bug for more powerful APR. For earlier patches that failed to pass all tests, we combine the incorrect patches with their corresponding relevant test failure information to construct a new prompt for the LLM to generate the next patch. In this way, we can avoid making the same mistakes. For earlier patches that passed all the tests, we further ask the LLM to generate alternative variations of the original plausible patches. In this way, we can further build on and learn from earlier successes to generate more plausible patches to increase the chance of having correct patches. While our approach is general, we implement ChatRepair using state-of-the-art dialogue-based LLM -- ChatGPT. By calculating the cost of accessing ChatGPT, we can fix 162 out of 337 bugs for \textbackslash\$0.42 each!},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Software Engineering,notion},
  file = {/Users/bohrok/Zotero/storage/QYN4XBVG/Xia and Zhang - 2023 - Keep the Conversation Going Fixing 162 out of 337 bugs for $0.42 each using ChatGPT.pdf;/Users/bohrok/Zotero/storage/9LCJAACQ/2304.html}
}

@article{xiaMeasuringProgramComprehension2018,
  title = {Measuring {{Program Comprehension}}: {{A Large-Scale Field Study}} with {{Professionals}}},
  author = {Xia, Xin and Bao, Lingfeng and Lo, David and Xing, Zhenchang and Hassan, Ahmed E. and Li, Shanping},
  date = {2018},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {44},
  number = {10},
  pages = {951--976},
  doi = {10.1109/TSE.2017.2734091},
  keywords = {notion}
}

@inproceedings{xiangCriticalCodeGuided2024,
  title = {Critical {{Code Guided Directed Greybox Fuzzing}} for {{Commits}}},
  booktitle = {33rd {{USENIX Security Symposium}} ({{USENIX Security}} 24)},
  author = {Xiang, Yi and Zhang, Xuhong and Liu, Peiyu and Ji, Shouling and Liang, Hong and Xu, Jiacheng and Wang, Wenhai},
  date = {2024-08},
  pages = {2459--2474},
  publisher = {USENIX Association},
  location = {Philadelphia, PA},
  url = {https://www.usenix.org/conference/usenixsecurity24/presentation/xiang-yi},
  isbn = {978-1-939133-44-1},
  keywords = {notion}
}

@article{xiaWhatDevelopersSearch2017,
  title = {What Do Developers Search for on the Web?},
  author = {Xia, Xin and Bao, Lingfeng and Lo, David and Kochhar, Pavneet Singh and Hassan, Ahmed E. and Xing, Zhenchang},
  date = {2017-12-01},
  journaltitle = {Empirical Software Engineering},
  volume = {22},
  number = {6},
  pages = {3149--3185},
  issn = {1573-7616},
  doi = {10.1007/s10664-017-9514-4},
  url = {https://doi.org/10.1007/s10664-017-9514-4},
  abstract = {Developers commonly make use of a web search engine such as Google to locate online resources to improve their productivity. A better understanding of what developers search for could help us understand their behaviors and the problems that they meet during the software development process. Unfortunately, we have a limited understanding of what developers frequently search for and of the search tasks that they often find challenging. To address this gap, we collected search queries from 60 developers, surveyed 235 software engineers from more than 21 countries across five continents. In particular, we asked our survey participants to rate the frequency and difficulty of 34 search tasks which are grouped along the following seven dimensions: general search, debugging and bug fixing, programming, third party code reuse, tools, database, and testing. We find that searching for explanations for unknown terminologies, explanations for exceptions/error messages (e.g., HTTP 404), reusable code snippets, solutions to common programming bugs, and suitable third-party libraries/services are the most frequent search tasks that developers perform, while searching for solutions to performance bugs, solutions to multi-threading bugs, public datasets to test newly developed algorithms or systems, reusable code snippets, best industrial practices, database optimization solutions, solutions to security bugs, and solutions to software configuration bugs are the most difficult search tasks that developers consider. Our study sheds light as to why practitioners often perform some of these tasks and why they find some of them to be challenging. We also discuss the implications of our findings to future research in several research areas, e.g., code search engines, domain-specific search engines, and automated generation and refinement of search queries.},
  keywords = {notion}
}

@inproceedings{xinLeveragingSyntaxrelatedCode2017,
  title = {Leveraging {{Syntax-related Code}} for {{Automated Program Repair}}},
  booktitle = {Proceedings of the {{32Nd IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Xin, Qi and Reiss, Steven P.},
  date = {2017},
  series = {{{ASE}} 2017},
  pages = {660--670},
  publisher = {IEEE Press},
  location = {Piscataway, NJ, USA},
  url = {http://dl.acm.org/citation.cfm?id=3155562.3155644},
  isbn = {978-1-5386-2684-9},
  venue = {Urbana-Champaign, IL, USA},
  keywords = {Automated program repair,code search,code transfer,notion}
}

@inproceedings{xinProgramDebloatingStochastic2020a,
  title = {Program {{Debloating}} via {{Stochastic Optimization}}},
  booktitle = {Proceedings of the {{ACM}}/{{IEEE}} 42nd {{International Conference}} on {{Software Engineering}}: {{New Ideas}} and {{Emerging Results}}},
  author = {Xin, Qi and Kim, Myeongsoo and Zhang, Qirun and Orso, Alessandro},
  date = {2020},
  series = {{{ICSE-NIER}} '20},
  pages = {65--68},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3377816.3381739},
  url = {https://doi.org/10.1145/3377816.3381739},
  abstract = {Programs typically provide a broad range of features. Because different typologies of users tend to use only a subset of these features, and unnecessary features can harm performance and security, program debloating techniques, which can reduce the size of a program by eliminating (possibly) unneeded features, are becoming increasingly popular. Most existing debloating techniques tend to focus on program-size reduction alone and, although effective, ignore other important aspects of debloating. We believe that program debloating is a multifaceted problem that must be addressed in a more general way. In this spirit, we propose a general approach that allows for formulating program debloating as a multi-objective optimization problem. Given a program to be debloated, our approach lets users specify (1) a usage profile for the program (i.e., a set of inputs with associated usage probabilities), (2) the factors of interest for debloating, and (3) the relative importance of these factors. Based on this information, the approach defines a suitable objective function for associating a score to every possible reduced program and aims to generate an optimal solution that maximizes the objective function. We also present and evaluate Debop, a specific instance of our approach that considers three objectives: size reduction, attack-surface reduction, and generality (i.e., the extent to which the reduced program handles inputs in the usage profile provided). Our results, albeit still preliminary, are promising and show that our approach can be effective at generating debloated programs that achieve a good trade-off between the different de-bloating objectives considered. Our results also provide insights on the performance of our general approach when compared to a specialized single-goal technique.},
  isbn = {978-1-4503-7126-1},
  venue = {Seoul, South Korea},
  keywords = {notion}
}

@article{xiongIdentifyingPatchCorrectness2018,
  title = {Identifying {{Patch Correctness}} in {{Test-Based Program Repair}}},
  author = {Xiong, Yingfei and Liu, Xinyuan and Zeng, Muhan and Zhang, Lu and Huang, Gang},
  date = {2018},
  keywords = {notion}
}

@inproceedings{xuAdaptiveFitnessFunction2017,
  title = {An {{Adaptive Fitness Function Based}} on {{Branch Hardness}} for {{Search Based Testing}}},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {Xu, Xiong and Zhu, Ziming and Jiao, Li},
  date = {2017},
  series = {{{GECCO}} '17},
  pages = {1335--1342},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/3071178.3071184},
  url = {http://doi.acm.org/10.1145/3071178.3071184},
  isbn = {978-1-4503-4920-8},
  venue = {Berlin, Germany},
  keywords = {adaptivity,branch hardness,fitness functions,notion,search based software testing,test data generation}
}

@inproceedings{xuanLearningCombineMultiple2014,
  title = {Learning to {{Combine Multiple Ranking Metrics}} for {{Fault Localization}}},
  booktitle = {2014 {{IEEE International Conference}} on {{Software Maintenance}} and {{Evolution}}},
  author = {Xuan, J. and Monperrus, M.},
  date = {2014-09},
  pages = {191--200},
  issn = {1063-6773},
  doi = {10.1109/ICSME.2014.41},
  abstract = {Fault localization is an inevitable step in software debugging. Spectrum-based fault localization consists in computing a ranking metric on execution traces to identify faulty source code. Existing empirical studies on fault localization show that there is no optimal ranking metric for all faults in practice. In this paper, we propose Multric, a learning-based approach to combining multiple ranking metrics for effective fault localization. In Multric, a suspiciousness score of a program entity is a combination of existing ranking metrics. Multric consists two major phases: learning and ranking. Based on training faults, Multric builds a ranking model by learning from pairs of faulty and non-faulty source code elements. When a new fault appears, Multric computes the final ranking with the learned model. Experiments are conducted on 5386 seeded faults in ten open-source Java programs. We empirically compare Multric against four widely-studied metrics and three recently-proposed one. Our experimental results show that Multric localizes faults more effectively than state-of-art metrics, such as Tarantula, Ochiai, and Ample.},
  keywords = {Fault Localization,Multric,notion}
}

@article{xuanNopolAutomaticRepair2017,
  title = {Nopol: {{Automatic Repair}} of {{Conditional Statement Bugs}} in {{Java Programs}}},
  author = {Xuan, J. and Martinez, M. and DeMarco, F. and Clément, M. and Marcote, S. L. and Durieux, T. and Berre, D. Le and Monperrus, M.},
  date = {2017-01},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {43},
  number = {1},
  pages = {34--55},
  issn = {0098-5589},
  doi = {10.1109/TSE.2016.2560811},
  keywords = {angelic fix localization,Apache Commons Lang,Apache Commons Math,automatic conditional statement bug repairing,Automatic repair,buggy IF conditions,buggy program,code patch,computability,Computer bugs,conditional expression,Encoding,fault localization,Indexes,Java,Java programs,Maintenance engineering,Nopol,notion,object-oriented programming,objected-oriented features,Open source software,open-source projects,patch generation,program debugging,public domain software,Runtime,runtime trace collection,satisfiability modulo theory problem,SMT,SMT problem,software fault tolerance,software maintenance,test execution}
}

@article{xuBriefSurveyProgram2005,
  title = {A {{Brief Survey}} of {{Program Slicing}}},
  author = {Xu, Baowen and Qian, Ju and Zhang, Xiaofang and Wu, Zhongqiang and Chen, Lin},
  date = {2005-03},
  journaltitle = {SIGSOFT Softw. Eng. Notes},
  volume = {30},
  number = {2},
  pages = {1--36},
  publisher = {ACM},
  location = {New York, NY, USA},
  issn = {0163-5948},
  doi = {10.1145/1050849.1050865},
  url = {http://doi.acm.org/10.1145/1050849.1050865},
  keywords = {notion,Program Slicing,Survey}
}

@inproceedings{xuDebuggingIntelligenceProbabilistic2018a,
  title = {Debugging with {{Intelligence}} via {{Probabilistic Inference}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Software Engineering}}},
  author = {Xu, Zhaogui and Ma, Shiqing and Zhang, Xiangyu and Zhu, Shuofei and Xu, Baowen},
  date = {2018},
  series = {{{ICSE}} '18},
  pages = {1171--1181},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/3180155.3180237},
  url = {http://doi.acm.org/10.1145/3180155.3180237},
  isbn = {978-1-4503-5638-1},
  venue = {Gothenburg, Sweden},
  keywords = {debugging,notion,probabilistic inference,Python}
}

@inproceedings{xueTrimmingTestSuites2014,
  title = {Trimming {{Test Suites}} with {{Coincidentally Correct Test Cases}} for {{Enhancing Fault Localizations}}},
  booktitle = {2014 {{IEEE}} 38th {{Annual Computer Software}} and {{Applications Conference}}},
  author = {Xue, Xiaozhen and Pang, Yulei and Namin, Akbar Siami},
  date = {2014},
  pages = {239--244},
  doi = {10.1109/COMPSAC.2014.32},
  keywords = {notion}
}

@article{xuInfectionGraphModel2019,
  title = {An {{Infection Graph Model}} for {{Reasoning}} of {{Multiple Faults}} in {{Software}}},
  author = {Xu, J. and Chen, R. and Deng, W. and Zhao, H.},
  date = {2019},
  journaltitle = {IEEE Access},
  volume = {7},
  pages = {77116--77133},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2922351},
  keywords = {coverage information,dynamic control flow dependences,fault diagnosis,fault masking,fault propagation,graph theory,IG model,infection graph model,inference mechanisms,logical reasoning algorithm,model-based diagnosis,multi-fault program,multiple-fault context,notion,probabilistic reasoning,program debugging,program diagnostics,program verification,ranking diagnosis candidates,software fault diagnosis,Software fault diagnosis,software fault tolerance}
}

@inproceedings{yadavallyPartialProgramDependence2023,
  title = {({{Partial}}) {{Program Dependence Learning}}},
  booktitle = {Proceedings of the 45th {{International Conference}} on {{Software Engineering}}},
  author = {Yadavally, Aashish and Nguyen, Tien N. and Wang, Wenbo and Wang, Shaohua},
  date = {2023},
  series = {{{ICSE}} '23},
  pages = {2501--2513},
  publisher = {IEEE Press},
  location = {Melbourne, Victoria, Australia},
  doi = {10.1109/ICSE48619.2023.00209},
  url = {https://doi.org/10.1109/ICSE48619.2023.00209},
  abstract = {Code fragments from developer forums often migrate to applications due to the code reuse practice. Owing to the incomplete nature of such programs, analyzing them to early determine the presence of potential vulnerabilities is challenging. In this work, we introduce NEURALPDA, a neural network-based program dependence analysis tool for both complete and partial programs. Our tool efficiently incorporates intrastatement and inter-statement contextual features into statement representations, thereby modeling program dependence analysis as a statement-pair dependence decoding task. In the empirical evaluation, we report that NEURALPDA predicts the CFG and PDG edges in complete Java and C/C++ code with combined F-scores of 94.29\% and 92.46\%, respectively. The F-score values for partial Java and C/C++ code range from 94.29\%–97.17\% and 92.46\%–96.01\%, respectively. We also test the usefulness of the PDGs predicted by NEURALPDA (i.e., PDG*) on the downstream task of method-level vulnerability detection. We discover that the performance of the vulnerability detection tool utilizing PDG* is only 1.1\% less than that utilizing the PDGs generated by a program analysis tool. We also report the detection of 14 real-world vulnerable code snippets from StackOverflow by a machine learning-based vulnerability detection tool that employs the PDGs predicted by NEURALPDA for these code snippets.},
  isbn = {978-1-66545-701-9},
  keywords = {deep learning,neural networks,neural partial program analysis,neural program dependence analysis,notion}
}

@inproceedings{yadegariGenericApproachAutomatic2015,
  title = {A {{Generic Approach}} to {{Automatic Deobfuscation}} of {{Executable Code}}},
  booktitle = {2015 {{IEEE Symposium}} on {{Security}} and {{Privacy}}},
  author = {Yadegari, B. and Johannesmeyer, B. and Whitely, B. and Debray, S.},
  date = {2015-05},
  pages = {674--691},
  issn = {1081-6011},
  doi = {10.1109/SP.2015.47},
  abstract = {Malicious software are usually obfuscated to avoid detection and resist analysis. When new malware is encountered, such obfuscations have to be penetrated or removed ("deobfuscated") in order to understand the internal logic of the code and devise countermeasures. This paper discusses a generic approach for deobfuscation of obfuscated executable code. Our approach does not make any assumptions about the nature of the obfuscations used, but instead uses semantics-preserving program transformations to simplify away obfuscation code. We have applied a prototype implementation of our ideas to a variety of different kinds of obfuscation, including emulation-based obfuscation, emulation-based obfuscation with runtime code unpacking, and return-oriented programming. Our experimental results are encouraging and suggest that this approach can be effective in extracting the internal logic from code obfuscated using a variety of obfuscation techniques, including tools such as Themida that previous approaches could not handle.},
  keywords = {Deobfuscation,notion,Return Oriented Programming,Virtualization-Obfuscation}
}

@inproceedings{yandrapallyAutomatedModularizationGUI2015,
  title = {Automated {{Modularization}} of {{GUI Test Cases}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Software Engineering}} - {{Volume}} 1},
  author = {Yandrapally, Rahulkrishna and Sridhara, Giriprasad and Sinha, Saurabh},
  date = {2015},
  series = {{{ICSE}} '15},
  pages = {44--54},
  publisher = {IEEE Press},
  location = {Piscataway, NJ, USA},
  url = {http://dl.acm.org/citation.cfm?id=2818754.2818763},
  isbn = {978-1-4799-1934-5},
  venue = {Florence, Italy},
  keywords = {GUI,notion,Test Case Modularization}
}

@inproceedings{yangCausalVAEDisentangledRepresentation2021,
  title = {{{CausalVAE}}: {{Disentangled Representation Learning}} via {{Neural Structural Causal Models}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Yang, Mengyue and Liu, Furui and Chen, Zhitang and Shen, Xinwei and Hao, Jianye and Wang, Jun},
  date = {2021-06},
  pages = {9593--9602},
  keywords = {notion}
}

@inproceedings{yangLanguageAgnosticDynamicAnalysis2022,
  title = {Language-{{Agnostic Dynamic Analysis}} of {{Multilingual Code}}: {{Promises}}, {{Pitfalls}}, and {{Prospects}}},
  booktitle = {{{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}} ({{ESEC}}/{{FSE}})},
  author = {Yang, Haoran and Li, Wen and Cai, Haipeng},
  date = {2022},
  keywords = {language agnostic,multilingual,notion,orbs}
}

@incollection{yedidiaUnderstandingBeliefPropagation2003,
  title = {Understanding Belief Propagation and Its Generalizations},
  booktitle = {Exploring {{Artificial Intelligence}} in the {{New Millenium}}},
  author = {Yedidia, Jonathan and Freeman, William and Weiss, Yair},
  date = {2003-01},
  volume = {8},
  pages = {239--269},
  isbn = {1-55860-811-7},
  keywords = {notion}
}

@article{yiCorrelationStudyAutomated2017,
  title = {A Correlation Study between Automated Program Repair and Test-Suite Metrics},
  author = {Yi, Jooyong and Tan, Shin Hwei and Mechtaev, Sergey and Böhme, Marcel and Roychoudhury, Abhik},
  date = {2017-09-30},
  journaltitle = {Empirical Software Engineering},
  issn = {1573-7616},
  doi = {10.1007/s10664-017-9552-y},
  url = {https://doi.org/10.1007/s10664-017-9552-y},
  abstract = {Automated program repair is increasingly gaining traction, due to its potential to reduce debugging cost greatly. The feasibility of automated program repair has been shown in a number of works, and the research focus is gradually shifting toward the quality of generated patches. One promising direction is to control the quality of generated patches by controlling the quality of test-suites used for automated program repair. In this paper, we ask the following research question: “Can traditional test-suite metrics proposed for the purpose of software testing also be used for the purpose of automated program repair?” We empirically investigate whether traditional test-suite metrics such as statement/branch coverage and mutation score are effective in controlling the reliability of generated repairs (the likelihood that repairs cause regression errors). We conduct the largest-scale experiments of this kind to date with real-world software, and for the first time perform a correlation study between various test-suite metrics and the reliability of generated repairs. Our results show that in general, with the increase of traditional test suite metrics, the reliability of repairs tend to increase. In particular, such a trend is most strongly observed in statement coverage. Our results imply that the traditional test suite metrics proposed for software testing can also be used for automated program repair to improve the reliability of repairs.},
  keywords = {notion}
}

@inproceedings{yiFeasibilityStudyUsing2017,
  title = {A {{Feasibility Study}} of {{Using Automated Program Repair}} for {{Introductory Programming Assignments}}},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Yi, Jooyong and Ahmed, Umair Z. and Karkare, Amey and Tan, Shin Hwei and Roychoudhury, Abhik},
  date = {2017},
  series = {{{ESEC}}/{{FSE}} 2017},
  pages = {740--751},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/3106237.3106262},
  url = {http://doi.acm.org/10.1145/3106237.3106262},
  isbn = {978-1-4503-5105-8},
  venue = {Paderborn, Germany},
  keywords = {Automated Program Repair,Intelligent Tutoring System,notion}
}

@inproceedings{yinImplementationinducedInconsistencyNondeterminism2020,
  title = {Implementation-Induced {{Inconsistency}} and {{Nondeterminism}} in {{Deterministic Clustering Algorithms}}},
  booktitle = {2020 {{IEEE}} 13th {{International Conference}} on {{Software Testing}}, {{Validation}} and {{Verification}} ({{ICST}})},
  author = {Yin, X. and Neamtiu, I. and Patil, S. and Andrews, S. T.},
  date = {2020},
  pages = {231--242},
  doi = {10.1109/ICST46399.2020.00032},
  keywords = {notion}
}

@inproceedings{yooEvolvingHumanCompetitive2012,
  title = {Evolving {{Human Competitive Spectra-Based Fault Localisation Techniques}}},
  booktitle = {Search {{Based Software Engineering}}},
  author = {Yoo, Shin},
  editor = {Fraser, Gordon and Teixeira de Souza, Jerffeson},
  date = {2012},
  pages = {244--258},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  abstract = {Spectra-Based Fault Localisation (SBFL) aims to assist debugging by applying risk evaluation formulæ (sometimes called suspiciousness metrics) to program spectra and ranking statements according to the predicted risk. Designing a risk evaluation formula is often an intuitive process done by human software engineer. This paper presents a Genetic Programming (GP) approach for evolving risk assessment formulæ. The empirical evaluation using 92 faults from four Unix utilities produces promising results. Equations evolved by Genetic Programming can consistently outperform many of the human-designed formulæ, such as Tarantula, Ochiai, Jaccard, Ample, and Wong1/2, up to 6 times. More importantly, they can perform equally as well as Op2, which was recently proved to be optimal against If-Then-Else-2 (ITE2) structure, or even outperform it against other program structures.},
  isbn = {978-3-642-33119-0},
  keywords = {notion}
}

@article{yooFaultLocalizationPrioritization2013a,
  title = {Fault Localization Prioritization: {{Comparing}} Information-Theoretic and Coverage-Based Approaches},
  author = {Yoo, Shin and Harman, Mark and Clark, David},
  date = {2013-07},
  journaltitle = {ACM Transactions on Software Engineering Methodology},
  volume = {22},
  number = {3},
  pages = {19:1--19:29},
  publisher = {ACM},
  location = {New York, NY, USA},
  keywords = {Fault Localization,Information Theory,notion,Test Case Prioritization}
}

@article{yooGPGPUTestSuite2013,
  title = {{{GPGPU}} Test Suite Minimisation: {{Search Based Software Engineering Performance Improvement}} Using {{Graphics Cards}}},
  author = {Yoo, Shin and Harman, Mark and Ur, Shmuel},
  date = {2013},
  journaltitle = {Empirical Software Engineering},
  volume = {18},
  number = {3},
  pages = {550--593},
  publisher = {Springer US},
  issn = {1382-3256},
  langid = {english},
  keywords = {GPGPU,notion,Regression Testing,SBSE,Test Suite Minimisation}
}

@article{yooHumanCompetitivenessGenetic2017,
  title = {Human {{Competitiveness}} of {{Genetic Programming}} in {{SBFL}}: {{Theoretical}} and {{Empirical Analysis}}},
  author = {Yoo, Shin and Xie, Xiaoyuan and Kuo, Fei-Ching and Chen, Tsong Yueh and Harman, Mark},
  date = {2017},
  journaltitle = {ACM Transactions on Software Engineering and Methodology},
  keywords = {Genetic Programming (GP),notion,Spectrum-based Fault Localization (SBFL)}
}

@inproceedings{yooImprovingConfigurabilityUnitlevel2021,
  title = {Improving {{Configurability}} of {{Unit-level Continuous Fuzzing}}: {{An Industrial Case Study}} with {{SAP HANA}}},
  booktitle = {The 36th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}.},
  author = {Yoo, Hanyoung and Hong, Jingun and Bader, Lucas and Hwang, Dong Won and Hong, Shin},
  date = {2021},
  keywords = {notion}
}

@article{yooObservationalSlicingBased2016,
  title = {Observational Slicing Based on Visual Semantics},
  author = {Yoo, Shin and Binkley, David and Eastman, Roger},
  date = {2016},
  journaltitle = {Journal of Systems and Software},
  volume = {129},
  pages = {60--78},
  keywords = {notion,ORBS,Program Slicing}
}

@article{yooRegressionTestingMinimisation2012,
  title = {Regression {{Testing Minimisation}}, {{Selection}} and {{Prioritisation}}: {{A Survey}}},
  author = {Yoo, Shin and Harman, Mark},
  date = {2012-03},
  journaltitle = {Software Testing, Verification, and Reliability},
  volume = {22},
  number = {2},
  pages = {67--120},
  keywords = {notion,Regression Testing,Survey,Test Case Prioritization,Test Suite Minimisation}
}

@inproceedings{yooSeeingSlicingObservation2014,
  title = {Seeing {{Is Slicing}}: {{Observation Based Slicing}} of {{Picture Description Languages}}},
  booktitle = {2014 {{IEEE}} 14th {{International Working Conference}} on {{Source Code Analysis}} and {{Manipulation}}},
  author = {Yoo, S. and Binkley, D. and Eastman, R.},
  date = {2014-09},
  pages = {175--184},
  doi = {10.1109/SCAM.2014.26},
  abstract = {Program slicing has seen a plethora of applications and variations since its introduction over thirty years ago. The dominant method for computing slices involves significant complex source-code analysis to model the dependences in the code. A recently introduced alternative, Observation-Based Slicing (ORBS), sidesteps this complexity by observing the behavior of candidate slices. ORBS has several other strengths, including the ability to slice multi-language systems. However, ORBS remains rooted in tradition as it captures semantics by comparing sequences of values. This raises the question of whether it is possible to extend slicing beyond its traditional semantic roots. A few existing projects have attempted this, but the extension requires considerable effort. If it is possible to build on the ORBS platform to more easily generalize slicing to languages with non-traditional semantics, then there is the potential to vastly increase the range of programming languages to which slicing can be applied. ORBS supports this by reducing the problem to that of generalizing how semantics are captured. Taking Picture Description Languages as a case study, the challenges and effectiveness of such a generalization are considered. The results show that not only is it possible to generalize the ORBS algorithm, but the resulting slicer is quite effective removing from 27\% to 98\% of the original source code with an average of 85\%. Finally a qualitative look at the slices finds the technique very effective, at times producing minimal slices.},
  keywords = {notion,ORBS,Picture Description Language,Program Slicing}
}

@article{yooTestDataRegeneration2012,
  title = {Test {{Data Regeneration}}: {{Generating New Test Data}} from {{Existing Test Data}}},
  author = {Yoo, Shin and Harman, Mark},
  date = {2012},
  journaltitle = {Journal of Software Testing, Verification and Reliability},
  volume = {22},
  number = {3},
  pages = {171--201},
  keywords = {notion,Test Generation}
}

@inproceedings{younesProbabilisticVerificationDiscrete2002,
  title = {Probabilistic {{Verification}} of {{Discrete Event Systems Using Acceptance Sampling}}},
  author = {Younes, Hakan and Simmons, Reid},
  date = {2002-05},
  doi = {10.1007/3-540-45657-0_17},
  isbn = {978-3-540-43997-4},
  keywords = {notion}
}

@inproceedings{yuanCacheQLQuantifyingLocalizing2023,
  title = {{{CacheQL}}: {{Quantifying}} and {{Localizing Cache Side-Channel Vulnerabilities}} in {{Production Software}}},
  booktitle = {32nd {{USENIX Security Symposium}} ({{USENIX Security}} 23)},
  author = {Yuan, Yuanyuan and Liu, Zhibo and Wang, Shuai},
  date = {2023},
  keywords = {notion}
}

@article{yuanImprovingSoftwareDiagnosability2012,
  title = {Improving {{Software Diagnosability}} via {{Log Enhancement}}},
  author = {Yuan, Ding and Zheng, Jing and Park, Soyeon and Zhou, Yuanyuan and Savage, Stefan},
  date = {2012-02},
  journaltitle = {ACM Trans. Comput. Syst.},
  volume = {30},
  number = {1},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  issn = {0734-2071},
  doi = {10.1145/2110356.2110360},
  url = {https://doi.org/10.1145/2110356.2110360},
  abstract = {Diagnosing software failures in the field is notoriously difficult, in part due to the fundamental complexity of troubleshooting any complex software system, but further exacerbated by the paucity of information that is typically available in the production setting. Indeed, for reasons of both overhead and privacy, it is common that only the run-time log generated by a system (e.g., syslog) can be shared with the developers. Unfortunately, the ad-hoc nature of such reports are frequently insufficient for detailed failure diagnosis. This paper seeks to improve this situation within the rubric of existing practice. We describe a tool, LogEnhancer that automatically “enhances” existing logging code to aid in future post-failure debugging. We evaluate LogEnhancer on eight large, real-world applications and demonstrate that it can dramatically reduce the set of potential root failure causes that must be considered while imposing negligible overheads.},
  keywords = {debugging,failure diagnostics,Log,notion,program analysis,software diagnosability}
}

@inproceedings{yuBayesianNetworkBased2016,
  title = {Bayesian {{Network Based Program Dependence Graph}} for {{Fault Localization}}},
  booktitle = {2016 {{IEEE International Symposium}} on {{Software Reliability Engineering Workshops}} ({{ISSREW}})},
  author = {Yu, X. and Liu, J. and Yang, Z. J. and Liu, X. and Yin, X. and Yi, S.},
  date = {2016-10},
  pages = {181--188},
  doi = {10.1109/ISSREW.2016.35},
  keywords = {Bayes methods,Bayesian Network,Bayesian network based program dependence graph,belief networks,BNPDG-based fault localization approach,Cognition,conditional probability,Fault localization,Graphical models,inference capability,inference mechanisms,local anomaly,Microwave integrated circuits,Mutual information,notion,PPDG,probabilistic graphical models,Probabilistic logic,probabilistic reasoning about program behaviors,probability,Probability distribution,program analysis,Program Analysis,program diagnostics,reasoning about programs,reasoning limitation}
}

@article{yuBayesianNetworkBased2017,
  title = {The {{Bayesian Network}} Based Program Dependence Graph and Its Application to Fault Localization},
  author = {Yu, Xiao and Liu, Jin and Yang, Zijiang and Liu, Xiao},
  date = {2017},
  journaltitle = {Journal of Systems and Software},
  volume = {134},
  pages = {44--53},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2017.08.025},
  url = {http://www.sciencedirect.com/science/article/pii/S0164121217301796},
  abstract = {Fault localization is an important and expensive task in software debugging. Some probabilistic graphical models such as probabilistic program dependence graph (PPDG) have been used in fault localization. However, PPDG is insufficient to reason across nonadjacent nodes and only support making inference about local anomaly. In this paper, we propose a novel probabilistic graphical model called Bayesian Network based Program Dependence Graph (BNPDG) that has the excellent inference capability for reasoning across nonadjacent nodes. We focus on applying the BNPDG to fault localization. Compared with the PPDG, our BNPDG-based fault localization approach overcomes the reasoning limitation across nonadjacent nodes and provides more precise fault localization by taking its output nodes as the common conditions to calculate the conditional probability of each non-output node. The experimental results show that our BNPDG-based fault localization approach can significantly improve the effectiveness of fault localization.},
  keywords = {Bayesian network,Fault localization,notion,Program analysis}
}

@inproceedings{yuDESCRYReproducingSystemlevel2017,
  title = {{{DESCRY}}: {{Reproducing System-level Concurrency Failures}}},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Yu, Tingting and Zaman, Tarannum S. and Wang, Chao},
  date = {2017},
  series = {{{ESEC}}/{{FSE}} 2017},
  pages = {694--704},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/3106237.3106266},
  url = {http://doi.acm.org/10.1145/3106237.3106266},
  isbn = {978-1-4503-5105-8},
  venue = {Paderborn, Germany},
  keywords = {Concurrency Failures,Debugging,Failure Reproduction,Multi-Process Applications,notion}
}

@inproceedings{yuPractitionersExpectationsAutomated2024,
  title = {Practitioners’ {{Expectations}} on {{Automated Test Generation}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Yu, Xiao and Liu, Lei and Hu, Xing and Keung, Jacky and Xia, Xin and Lo, David},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {1618--1630},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3680386},
  url = {https://dl.acm.org/doi/10.1145/3650212.3680386},
  urldate = {2024-09-21},
  abstract = {Automated test generation can help developers craft high-quality software tests while mitigating the manual effort needed for writing test code. Despite significant research efforts in automated test generation for nearly 50 years, there is a lack of clarity about what practitioners expect from automated test generation tools and whether the existing research meets their needs. To address this issue, we follow a mixed-methods approach to gain insights into practitioners' expectations of automated test generation. We first conduct the qualitative analysis from semi-structured interviews with 13 professionals, followed by a quantitative survey of 339 practitioners from 46 countries across five continents. We then conduct a literature review of premier venue papers from 2022 to 2024 (in the last three years) and compare current research findings with practitioners' expectations. From this comparison, we outline future research directions for researchers to bridge the gap between automated test generation research and practitioners' expectations.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/CJ4ZPJCT/Yu et al. - 2024 - Practitioners’ Expectations on Automated Test Generation.pdf}
}

@article{yurIncrementalFlowContextsensitive1999,
  title = {An Incremental Flow- and Context-Sensitive Pointer Aliasing Analysis},
  author = {Yur, Jyh-Shiarn and Ryder, Barbara G. and Landi, William},
  date = {1999},
  journaltitle = {Proceedings of the 1999 International Conference on Software Engineering (IEEE Cat. No.99CB37002)},
  pages = {442--451},
  url = {https://api.semanticscholar.org/CorpusID:8350461},
  keywords = {notion}
}

@article{zahlJackknifingIndexDiversity1977,
  title = {Jackknifing {{An Index}} of {{Diversity}}},
  author = {Zahl, Samuel},
  date = {1977},
  journaltitle = {Ecology},
  volume = {58},
  number = {4},
  eprint = {1936227},
  eprinttype = {jstor},
  pages = {907--913},
  publisher = {Ecological Society of America},
  issn = {00129658, 19399170},
  url = {http://www.jstor.org/stable/1936227},
  urldate = {2023-10-02},
  abstract = {The method of jackknifing is introduced for estimating an index of diversity and is illustrated with tree data. The method yields approximately normally distributed jackknife estimates and also gives estimated standard deviations, making possible tests of hypotheses and confidence interval estimates. These results apparently can be obtained under the usual conditions of field sampling, where associations within and between species or between quadrats or segments of a traverse may be found. Because of these associations there is no guarantee that the method works; hence an eyball and a statistical test for the approximate normality of the estimates are given and illustrated with the tree data. The tree data come from 24 quadrats arranged in two blocks of contiguous quadrats. The estimated tree-species diversity, using both indices considered, showed a smooth monotonic decrease over a 19-yr interval providing striking confirmation of the reality of this ecological parameter. The normality tests on this data showed that the normal approximations to the distributions of the jackknife estimates were justified, except in three instances, only one of which seemed to be seriously in error. A conclusion is that it seems reasonable to suppose that in many, if not most, cases, only moderate sample sizes will be needed in practice for the jackknife estimate, at least for forest data.},
  keywords = {notion}
}

@article{zakerinasrabadiFormatawareLearnfuzzDeep2021,
  title = {Format-Aware Learn\&fuzz: Deep Test Data Generation for Efficient Fuzzing},
  author = {Zakeri Nasrabadi, Morteza and Parsa, Saeed and Kalaee, Akram},
  date = {2021-03-01},
  journaltitle = {Neural Computing and Applications},
  volume = {33},
  number = {5},
  pages = {1497--1513},
  doi = {10.1007/s00521-020-05039-7},
  url = {https://doi.org/10.1007/s00521-020-05039-7},
  abstract = {Appropriate test data are a crucial factor to succeed in fuzz testing. Most of the real-world applications, however, accept complex structure inputs containing data surrounded by meta-data which is processed in several stages comprising of the parsing and rendering (execution). The complex structure of some input files makes it difficult to generate efficient test data automatically. The success of deep learning to cope with complex tasks, specifically generative tasks, has motivated us to exploit it in the context of test data generation for complicated structures such as PDF files. In this respect, a neural language model (NLM) based on deep recurrent neural networks (RNNs) is used to learn the structure of complex inputs. To target both the parsing and rendering steps of the software under test (SUT), our approach generates new test data while distinguishing between data and meta-data that significantly improve the input fuzzing. To assess the proposed approach, we have developed a modular file format fuzzer, IUST-DeepFuzz. Our experimental results demonstrate the relatively high coverage of MuPDF code by our proposed fuzzer, IUST-DeepFuzz, in comparison with the state-of-the-art tools such as learn\&fuzz, AFL, Augmented-AFL, and random fuzzing. In summary, our experiments with many deep learning models revealed the fact that the simpler the deep learning models applied to generate test data, the higher the code coverage of the software under test will be.},
  isbn = {1433-3058},
  keywords = {notion}
}

@article{zeilerVisualizingUnderstandingConvolutional2013,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  date = {2013},
  journaltitle = {CoRR},
  volume = {abs/1311.2901},
  url = {http://arxiv.org/abs/1311.2901},
  keywords = {notion}
}

@book{zelkowitzPrinciplesSoftwareEngineering1979,
  title = {Principles of Software Engineering and Design},
  author = {Zelkowitz, Marvin V and Shaw, Alan C and Gannon, John D},
  date = {1979},
  publisher = {Prentice Hall Professional Technical Reference},
  keywords = {notion}
}

@inproceedings{zellerIsolatingCauseEffectChains2002,
  title = {Isolating {{Cause-Effect Chains}} from {{Computer Programs}}},
  booktitle = {Proceedings of the 10th {{ACM SIGSOFT Symposium}} on {{Foundations}} of {{Software Engineering}}},
  author = {Zeller, Andreas},
  date = {2002},
  series = {{{SIGSOFT}} '02/{{FSE-10}}},
  pages = {1--10},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/587051.587053},
  url = {https://doi.org/10.1145/587051.587053},
  abstract = {Consider the execution of a failing program as a sequence of program states. Each state induces the following state, up to the failure. Which variables and values of a program state are relevant for the failure? We show how the Delta Debugging algorithm isolates the relevant variables and values by systematically narrowing the state difference between a passing run and a failing run—by assessing the outcome of altered executions to determine wether a change in the program state makes a difference in the test outcome. Applying Delta Debugging to multiple states of the program automatically reveals the cause-effect chain of the failure—that is, the variables and values that caused the failure.In a case study, our prototype implementation successfully isolated the cause-effect chain for a failure of the GNU C compiler: "Initially, the C program to be compiled contained an addition of 1.0; this caused an addition operator in the intermediate RTL representation; this caused a cycle in the RTL tree—and this caused the compiler to crash."},
  isbn = {1-58113-514-9},
  venue = {Charleston, South Carolina, USA},
  keywords = {automated debugging,notion,program comprehension,testing,tracing}
}

@article{zellerSimplifyingIsolatingFailureInducing2002,
  title = {Simplifying and {{Isolating Failure-Inducing Input}}},
  author = {Zeller, Andreas and Hildebrandt, Ralf},
  date = {2002-02},
  journaltitle = {IEEE Trans. Softw. Eng.},
  volume = {28},
  number = {2},
  pages = {183--200},
  publisher = {IEEE Press},
  location = {Piscataway, NJ, USA},
  issn = {0098-5589},
  doi = {10.1109/32.988498},
  url = {http://dx.doi.org/10.1109/32.988498},
  keywords = {Delta Debugging,notion}
}

@inproceedings{zengFaultLocalizationEfficient2022,
  title = {Fault {{Localization}} via {{Efficient Probabilistic Modeling}} of {{Program Semantics}}},
  booktitle = {International {{Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Zeng, Muhan and Wu, Yiqian and Ye, Zhentao and Xiong, Yingfei and Zhang, Xin and Zhang, Lu},
  date = {2022},
  keywords = {notion}
}

@article{zengJackknifeApproachEstimation2018,
  title = {Jackknife Approach to the Estimation of Mutual Information},
  author = {Zeng, Xianli and Xia, Yingcun and Tong, Howell},
  date = {2018},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {40},
  pages = {9956--9961},
  doi = {10.1073/pnas.1715593115},
  url = {https://www.pnas.org/doi/abs/10.1073/pnas.1715593115},
  abstract = {Quantifying the dependence between two random variables is a fundamental issue in data analysis, and thus many measures have been proposed. Recent studies have focused on the renowned mutual information (MI) [Reshef DN, et al. (2011) Science 334:1518–1524]. However, “Unfortunately, reliably estimating mutual information from finite continuous data remains a significant and unresolved problem” [Kinney JB, Atwal GS (2014) Proc Natl Acad Sci USA 111:3354–3359]. In this paper, we examine the kernel estimation of MI and show that the bandwidths involved should be equalized. We consider a jackknife version of the kernel estimate with equalized bandwidth and allow the bandwidth to vary over an interval. We estimate the MI by the largest value among these kernel estimates and establish the associated theoretical underpinnings.},
  keywords = {notion}
}

@inproceedings{zhangAPICraftFuzzDriver2021,
  title = {{{APICraft}}: {{Fuzz Driver Generation}} for {{Closed-source SDK Libraries}}},
  booktitle = {30th {{USENIX Security Symposium}} ({{USENIX Security}} 21)},
  author = {Zhang, Cen and Lin, Xingwei and Li, Yuekang and Xue, Yinxing and Xie, Jundong and Chen, Hongxu and Ying, Xinlei and Wang, Jiashui and Liu, Yang},
  date = {2021-08},
  pages = {2811--2828},
  publisher = {USENIX Association},
  url = {https://www.usenix.org/conference/usenixsecurity21/presentation/zhang-cen},
  isbn = {978-1-939133-24-3},
  keywords = {notion}
}

@inproceedings{zhangAutoCodeRoverAutonomousProgram2024,
  title = {{{AutoCodeRover}}: {{Autonomous Program Improvement}}},
  shorttitle = {{{AutoCodeRover}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Zhang, Yuntong and Ruan, Haifeng and Fan, Zhiyu and Roychoudhury, Abhik},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {1592--1604},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3680384},
  url = {https://dl.acm.org/doi/10.1145/3650212.3680384},
  urldate = {2024-09-21},
  abstract = {Researchers have made significant progress in automating the software development process in the past decades. Automated techniques for issue summarization, bug reproduction, fault localization, and program repair have been built to ease the workload of developers. Recent progress in Large Language Models (LLMs) has significantly impacted the development process, where developers can use LLM-based programming assistants to achieve automated coding. Nevertheless, software engineering involves the process of program improvement apart from coding, specifically to enable software maintenance (e.g. program repair to fix bugs) and software evolution (e.g. feature additions). In this paper, we propose an automated approach for solving Github issues to autonomously achieve program improvement. In our approach called AutoCodeRover, LLMs are combined with sophisticated code search capabilities, ultimately leading to a program modification or patch. In contrast to recent LLM agent approaches from AI researchers and practitioners, our outlook is more software engineering oriented. We work on a program representation (abstract syntax tree) as opposed to viewing a software project as a mere collection of files. Our code search exploits the program structure in the form of classes/methods to enhance LLM’s understanding of the issue’s root cause, and effectively retrieve a context via iterative search. The use of spectrum-based fault localization using tests, further sharpens the context, as long as a test-suite is available. Experiments on the recently proposed SWE-bench-lite (300 real-life Github issues) show increased efficacy in solving Github issues (19\% on SWE-bench-lite), which is higher than the efficacy of the recently reported Swe-agent. Interestingly, our approach resolved 57 GitHub issues in about 4 minutes each (pass@1), whereas developers spent more than 2.68 days on average. In addition, AutoCodeRover achieved this efficacy with significantly lower cost (on average, \$0.43 USD), compared to other baselines. We posit that our workflow enables autonomous software engineering, where, in future, auto-generated code from LLMs can be autonomously improved.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/FMLHIHXL/Zhang et al. - 2024 - AutoCodeRover Autonomous Program Improvement.pdf}
}

@inproceedings{zhangAutomatedTransplantationDifferential2017,
  title = {Automated {{Transplantation}} and {{Differential Testing}} for {{Clones}}},
  booktitle = {2017 {{IEEE}}/{{ACM}} 39th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Zhang, T. and Kim, M.},
  date = {2017},
  pages = {665--676},
  doi = {10.1109/ICSE.2017.67},
  keywords = {notion,transplantation}
}

@inproceedings{zhangAutomaticallyGeneratingDescriptive2016,
  title = {Towards Automatically Generating Descriptive Names for Unit Tests},
  booktitle = {2016 31st {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Zhang, B. and Hill, E. and Clause, J.},
  date = {2016-09},
  pages = {625--636},
  abstract = {During maintenance, developers often need to understand the purpose of a test. One of the most potentially useful sources of information for understanding a test is its name. Ideally, test names are descriptive in that they accurately summarize both the scenario and the expected outcome of the test. Despite the benefits of being descriptive, test names often fall short of this goal. In this paper we present a new approach for automatically generating descriptive names for existing test bodies. Using a combination of natural-language program analysis and text generation, the technique creates names that summarize the test's scenario and the expected outcome. The results of our evaluation show that, (1) compared to alternative approaches, the names generated by our technique are significantly more similar to human-generated names and are nearly always preferred by developers, (2) the names generated by our technique are preferred over or are equivalent to the original test names in 83\% of cases, and (3) our technique is several orders of magnitude faster than manually writing test names.},
  keywords = {Descriptive Names,Maintenance,notion,Unit Testing}
}

@article{zhangBDAPracticalDependence2019,
  title = {{{BDA}}: {{Practical Dependence Analysis}} for {{Binary Executables}} by {{Unbiased Whole-Program Path Sampling}} and per-{{Path Abstract Interpretation}}},
  author = {Zhang, Zhuo and You, Wei and Tao, Guanhong and Wei, Guannan and Kwon, Yonghwi and Zhang, Xiangyu},
  date = {2019-10},
  journaltitle = {Proc. ACM Program. Lang.},
  volume = {3},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3360563},
  url = {https://doi.org/10.1145/3360563},
  abstract = {Binary program dependence analysis determines dependence between instructions and hence is important for many applications that have to deal with executables without any symbol information. A key challenge is to identify if multiple memory read/write instructions access the same memory location. The state-of-the-art solution is the value set analysis (VSA) that uses abstract interpretation to determine the set of addresses that are possibly accessed by memory instructions. However, VSA is conservative and hence leads to a large number of bogus dependences and then substantial false positives in downstream analyses such as malware behavior analysis. Furthermore, existing public VSA implementations have difficulty scaling to complex binaries. In this paper, we propose a new binary dependence analysis called BDA enabled by a randomized abstract interpretation technique. It features a novel whole program path sampling algorithm that is not biased by path length, and a per-path abstract interpretation avoiding precision loss caused by merging paths in traditional analyses. It also provides probabilistic guarantees. Our evaluation on SPECINT2000 programs shows that it can handle complex binaries such as gcc whereas VSA implementations from the-state-of-art platforms have difficulty producing results for many SPEC binaries. In addition, the dependences reported by BDA are 75 and 6 times smaller than Alto, a scalable binary dependence analysis tool, and VSA, respectively, with only 0.19\% of true dependences observed during dynamic execution missed (by BDA). Applying BDA to call graph generation and malware analysis shows that BDA substantially supersedes the commercial tool IDA in recovering indirect call targets and outperforms a state-of-the-art malware analysis tool Cuckoo by disclosing 3 times more hidden payloads.},
  issue = {OOPSLA},
  keywords = {Abstract Interpretation,Binary Analysis,Data Dependence,notion,Path Sampling}
}

@inproceedings{zhangBoostingSpectrumbasedFault2017,
  title = {Boosting {{Spectrum-based Fault Localization Using PageRank}}},
  booktitle = {Proceedings of the 26th {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Zhang, Mengshi and Li, Xia and Zhang, Lingming and Khurshid, Sarfraz},
  date = {2017},
  series = {{{ISSTA}} 2017},
  pages = {261--272},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/3092703.3092731},
  url = {http://doi.acm.org/10.1145/3092703.3092731},
  isbn = {978-1-4503-5076-1},
  venue = {Santa Barbara, CA, USA},
  keywords = {notion,PageRank,Software Testing,Spectrum-based Fault Localization (SBFL)}
}

@article{zhangCNNFLEffectiveApproach2019,
  title = {{{CNN-FL}}: {{An Effective Approach}} for {{Localizing Faults}} Using {{Convolutional Neural Networks}}},
  author = {Zhang, Zhuo and Lei, Yan and Mao, Xiaoguang and Li, Panpan},
  date = {2019},
  journaltitle = {2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER)},
  pages = {445--455},
  keywords = {notion}
}

@inproceedings{zhangCostEffectiveDynamic2004,
  title = {Cost {{Effective Dynamic Program Slicing}}},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 2004 {{Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Zhang, Xiangyu and Gupta, Rajiv},
  date = {2004},
  series = {{{PLDI}} '04},
  pages = {94--106},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/996841.996855},
  url = {http://doi.acm.org/10.1145/996841.996855},
  isbn = {1-58113-807-5},
  venue = {Washington DC, USA},
  keywords = {debugging,dynamic dependence graph,notion,testing}
}

@inproceedings{zhangDaisyEffectiveFuzz2023,
  title = {Daisy: {{Effective Fuzz Driver Synthesis}} with {{Object Usage Sequence Analysis}}},
  booktitle = {2023 {{IEEE}}/{{ACM}} 45th {{International Conference}} on {{Software Engineering}}: {{Software Engineering}} in {{Practice}} ({{ICSE-SEIP}})},
  author = {Zhang, Mingrui and Zhou, Chijin and Liu, Jianzhong and Wang, Mingzhe and Liang, Jie and Zhu, Juan and Jiang, Yu},
  date = {2023},
  pages = {87--98},
  doi = {10.1109/ICSE-SEIP58684.2023.00013},
  keywords = {notion}
}

@article{zhangDynamicUncertainCausality2015,
  title = {Dynamic {{Uncertain Causality Graph}} for {{Knowledge Representation}} and {{Probabilistic Reasoning}}: {{Directed Cyclic Graph}} and {{Joint Probability Distribution}}},
  author = {Zhang, Q.},
  date = {2015},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {26},
  number = {7},
  pages = {1503--1517},
  keywords = {notion}
}

@article{zhangEfficientMonteCarlo2019,
  title = {Efficient {{Monte Carlo}} Resampling for Probability Measure Changes from {{Bayesian}} Updating},
  author = {Zhang, Jiaxin and Shields, Michael D.},
  date = {2019},
  journaltitle = {Probabilistic Engineering Mechanics},
  volume = {55},
  pages = {54--66},
  issn = {0266-8920},
  doi = {10.1016/j.probengmech.2018.10.002},
  url = {https://www.sciencedirect.com/science/article/pii/S0266892018300535},
  abstract = {The objective of Bayesian inference is often to infer, from data, a probability measure for a random variable that can be used as input for Monte Carlo simulation. When datasets for Bayesian inference are small, a principle challenge is that, as additional data are collected, the probability measure inferred from Bayesian inference may change significantly. In such cases, expensive Monte Carlo simulations may have already been performed using the original distribution and it is infeasible to start again and perform a new Monte Carlo analysis using the updated density due to the large added computational cost. This work explores four strategies for updating Monte Carlo simulations for such a change in probability measure. The efficiency of each strategy is compared and the ultimate aim is to achieve the change in distribution with a minimal number of added computational simulations. The results show that, when the change in measure is small, importance sampling reweighting can be very effective. Otherwise, a proposed mixed augmenting-filtering algorithm can robustly and efficiently accommodate a measure change in Monte Carlo simulation. The strategy is then applied for uncertainty quantification in the buckling strength of a simple plate given ongoing data collection to estimate uncertainty in the yield stress.},
  keywords = {Bayesian inference,Importance sampling,Monte Carlo simulation,notion,Uncertainty quantification}
}

@inproceedings{zhangEmpiricallyRevisitingTest2014,
  title = {Empirically {{Revisiting}} the {{Test Independence Assumption}}},
  booktitle = {Proceedings of the 2014 {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Zhang, Sai and Jalali, Darioush and Wuttke, Jochen and Muşlu, Kıvanç and Lam, Wing and Ernst, Michael D. and Notkin, David},
  date = {2014},
  series = {{{ISSTA}} 2014},
  pages = {385--396},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/2610384.2610404},
  url = {http://doi.acm.org/10.1145/2610384.2610404},
  isbn = {978-1-4503-2645-2},
  venue = {San Jose, CA, USA},
  keywords = {detection algorithms,empirical studies,notion,Test dependence}
}

@article{zhangEmpiricalStudyBoosting2021,
  title = {An {{Empirical Study}} of {{Boosting Spectrum-Based Fault Localization}} via {{PageRank}}},
  author = {Zhang, Mengshi and Li, Yaoxian and Li, Xia and Chen, Lingchao and Zhang, Yuqun and Zhang, Lingming and Khurshid, Sarfraz},
  date = {2021},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {47},
  number = {6},
  pages = {1089--1113},
  doi = {10.1109/TSE.2019.2911283},
  keywords = {notion}
}

@inproceedings{zhangHowEffectiveAre2024,
  title = {How {{Effective Are They}}? {{Exploring Large Language Model Based Fuzz Driver Generation}}},
  shorttitle = {How {{Effective Are They}}?},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Zhang, Cen and Zheng, Yaowen and Bai, Mingqiang and Li, Yeting and Ma, Wei and Xie, Xiaofei and Li, Yuekang and Sun, Limin and Liu, Yang},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {1223--1235},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3680355},
  url = {https://dl.acm.org/doi/10.1145/3650212.3680355},
  urldate = {2024-09-21},
  abstract = {Fuzz drivers are essential for library API fuzzing. However, automatically generating fuzz drivers is a complex task, as it demands the creation of high-quality, correct, and robust API usage code. An LLM-based (Large Language Model) approach for generating fuzz drivers is a promising area of research. Unlike traditional program analysis-based generators, this text-based approach is more generalized and capable of harnessing a variety of API usage information, resulting in code that is friendly for human readers. However, there is still a lack of understanding regarding the fundamental issues on this direction, such as its effectiveness and potential challenges.     To bridge this gap, we conducted the first in-depth study targeting the important issues of using LLMs to generate effective fuzz drivers. Our study features a curated dataset with 86 fuzz driver generation questions from 30 widely-used C projects. Six prompting strategies are designed and tested across five state-of-the-art LLMs with five different temperature settings. In total, our study evaluated 736,430 generated fuzz drivers, with 0.85 billion token costs (\$8,000+ charged tokens). Additionally, we compared the LLM-generated drivers against those utilized in industry, conducting extensive fuzzing experiments (3.75 CPU-year). Our study uncovered that:     1) While LLM-based fuzz driver generation is a promising direction, it still encounters several obstacles towards practical applications;   2) LLMs face difficulties in generating effective fuzz drivers for APIs with intricate specifics. Three featured design choices of prompt strategies can be beneficial: issuing repeat queries, querying with examples, and employing an iterative querying process;   3) While LLM-generated drivers can yield fuzzing outcomes that are on par with those used in the industry, there are substantial opportunities for enhancement, such as extending contained API usage, or integrating semantic oracles to facilitate logical bug detection.     Our insights have been implemented to improve the OSS-Fuzz-Gen project, facilitating practical fuzz driver generation in industry.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/5FPJJ2G6/Zhang et al. - 2024 - How Effective Are They Exploring Large Language Model Based Fuzz Driver Generation.pdf}
}

@inproceedings{zhangIntelliGenAutomaticDriver2021,
  title = {{{IntelliGen}}: {{Automatic Driver Synthesis}} for {{Fuzz Testing}}},
  booktitle = {2021 {{IEEE}}/{{ACM}} 43rd {{International Conference}} on {{Software Engineering}}: {{Software Engineering}} in {{Practice}} ({{ICSE-SEIP}})},
  author = {Zhang, Mingrui and Liu, Jianzhong and Ma, Fuchen and Zhang, Huafeng and Jiang, Yu},
  date = {2021},
  pages = {318--327},
  doi = {10.1109/ICSE-SEIP52600.2021.00041},
  keywords = {notion}
}

@incollection{zhangLocatingFaultsAutomated2006,
  title = {Locating {{Faults}} through {{Automated Predicate Switching}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Software Engineering}}},
  author = {Zhang, Xiangyu and Gupta, Neelam and Gupta, Rajiv},
  date = {2006},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  url = {http://lps3.doi.org.libra.kaist.ac.kr/10.1145/1134285.1134324},
  abstract = {Typically debugging begins when during a program execution a point is reached at which an obviously incorrect value is observed. A general and powerful approach to automated debugging can be based upon identifying modifications to the program state that will bring the execution to a successful conclusion. However, searching for arbitrary changes to the program state is difficult due to the extremely large search space. In this paper we demonstrate that by forcibly switching a predicate's outcome at runtime and altering the control flow, the program state can not only be inexpensively modified, but in addition it is often possible to bring the program execution to a successful completion (i.e., program produces the desired output). By examining the switched predicate, also called the critical predicate, the cause of the bug can then be identified. Since the outcome of a branch can only be either true or false, the number of modified states resulting by predicate switching is far less than those possible through arbitrary state changes. Thus, it is possible to automatically search through modified states to find one that leads to the correct output. We have developed an implementation based upon dynamic instrumentation to perform this search through program re-execution – the program is executed from the beginning and a predicate's outcome is switched to produce the desired change in control flow. To evaluate our approach, we tried our technique on several reported bugs for a number of UNIX utility programs. Our technique was found to be practical (i.e., acceptable in time taken) and effective (i.e., we were able to automatically identify critical predicates). Moreover we show that bidirectional dynamic slices of critical predicates capture the faulty code.},
  isbn = {1-59593-375-1},
  keywords = {notion}
}

@inproceedings{zhangLPRLargeLanguage2024,
  title = {{{LPR}}: {{Large Language Models-Aided Program Reduction}}},
  shorttitle = {{{LPR}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Zhang, Mengxiao and Tian, Yongqiang and Xu, Zhenyang and Dong, Yiwen and Tan, Shin Hwei and Sun, Chengnian},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {261--273},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3652126},
  url = {https://dl.acm.org/doi/10.1145/3650212.3652126},
  urldate = {2024-09-21},
  abstract = {Program reduction is a widely used technique to facilitate debugging                                compilers by automatically minimizing programs that trigger                                compiler bugs. Existing program reduction techniques are either                                generic to a wide range of languages (such as Perses and Vulcan)                                or specifically optimized for one certain language by exploiting                                language-specific knowledge (e.g., C-Reduce). However, synergistically                                combining both generality across languages and optimality                                to a specific language in program reduction is yet to be explored.                                This paper proposes LPR, the first LLMs-aided technique leveraging                                LLMs to perform language-specific program reduction for                                multiple languages. The key insight is to utilize both the language                                generality of program reducers such as Perses and the languagespecific                                semantics learned by LLMs. Concretely, language-generic                                program reducers can efficiently reduce programs into a small size                                that is suitable for LLMs to process; LLMs can effectively transform                                programs via the learned semantics to create new reduction opportunities                                for the language-generic program reducers to further                                reduce the programs.                                Our thorough evaluation on 50 benchmarks across three programming                                languages (i.e., C, Rust and JavaScript) has demonstrated                                LPR’s practicality and superiority over Vulcan, the state-of-the-art                                language-generic program reducer. For effectiveness, LPR surpasses                                Vulcan by producing 24.93\%, 4.47\%, and 11.71\% smaller programs                                on benchmarks in C, Rust and JavaScript, separately. Moreover, LPR                                and Vulcan have the potential to complement each other. For the C                                language for which C-Reduce is optimized, by applying Vulcan to                                the output produced by LPR, we can attain program sizes that are                                on par with those achieved by C-Reduce. For efficiency perceived                                by users, LPR is more efficient when reducing large and complex                                programs, taking 10.77\%, 34.88\%, 36.96\% less time than Vulcan to                                finish all the benchmarks in C, Rust and JavaScript, separately.},
  isbn = {9798400706127},
  keywords = {notion},
  file = {/Users/bohrok/Zotero/storage/KY64STMR/Zhang et al. - 2024 - LPR Large Language Models-Aided Program Reduction.pdf}
}

@article{zhangMixupEmpiricalRisk2017,
  title = {Mixup: {{Beyond Empirical Risk Minimization}}},
  author = {Zhang, Hongyi and Cissé, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
  date = {2017},
  journaltitle = {CoRR},
  volume = {abs/1710.09412},
  url = {http://arxiv.org/abs/1710.09412},
  keywords = {notion}
}

@article{zhangPredictiveMutationTesting2019,
  title = {Predictive {{Mutation Testing}}},
  author = {Zhang, Jie and Zhang, Lingming and Harman, Mark and Hao, Dan and Jia, Yue and Zhang, Lu},
  date = {2019},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {45},
  number = {9},
  pages = {898--918},
  doi = {10.1109/TSE.2018.2809496},
  keywords = {notion}
}

@article{zhangStackGANTextPhotorealistic2016,
  title = {{{StackGAN}}: {{Text}} to {{Photo-realistic Image Synthesis}} with {{Stacked Generative Adversarial Networks}}},
  author = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Huang, Xiaolei and Wang, Xiaogang and Metaxas, Dimitris N.},
  date = {2016},
  journaltitle = {CoRR},
  volume = {abs/1612.03242},
  url = {http://arxiv.org/abs/1612.03242},
  keywords = {notion}
}

@article{zhangStudyEffectivenessDynamic2007,
  title = {A Study of Effectiveness of Dynamic Slicing in Locating Real Faults},
  author = {Zhang, Xiangyu and Gupta, Neelam and Gupta, Rajiv},
  date = {2007-03},
  journaltitle = {Empirical Software Engineering},
  volume = {12},
  pages = {143--160},
  doi = {10.1007/s10664-006-9007-3},
  keywords = {notion}
}

@inproceedings{zhaoConservativeClaimsProbability2015,
  title = {Conservative Claims about the Probability of Perfection of Software-Based Systems},
  booktitle = {2015 {{IEEE}} 26th {{International Symposium}} on {{Software Reliability Engineering}} ({{ISSRE}})},
  author = {Zhao, Xingyu and Littlewood, Bev and Povyakalo, Andrey and Wright, David},
  date = {2015-11},
  pages = {130--140},
  doi = {10.1109/ISSRE.2015.7381807},
  url = {https://ieeexplore.ieee.org/abstract/document/7381807},
  urldate = {2024-10-29},
  abstract = {In recent years we have become interested in the problem of assessing the probability of perfection of software-based systems which are sufficiently simple that they are "possibly perfect". By "perfection" we mean that the software of interest will never fail in a specific operating environment. We can never be certain that it is perfect, so our interest lies in claims for its probability of perfection. Our approach is Bayesian: our aim is to model the changes to this probability of perfection as we see evidence of failure-free working. Much of the paper considers the difficult problem of expressing prior beliefs about the probability of failure on demand (pfd), and representing these mathematically. This requires the assessor to state his prior belief in perfection as a probability, and also to state what he believes are likely values of the pfd in the event that the system is not perfect. We take the view that it will be impractical for an assessor to express these beliefs as a complete distribution for pfd. Our approach to the problem has three threads. Firstly we assume that, although he cannot provide a full probabilistic description of his uncertainty in a single distribution, the assessor can express some precise but partial beliefs about the unknowns. Secondly, we assume that in the inevitable presence of such incompleteness, the Bayesian analysis needs to provide results that are guaranteed to be conservative (because the analyses we have in mind relate to critical systems). Finally, we seek to prune the set of prior distributions that the assessor finds acceptable in order that the conservatism of the results is no greater than it has to be, i.e. we propose, and eliminate, sets of priors that would appear generally unreasonable. We give some illustrative numerical examples of this approach, and note that the numerical values obtained for the posterior probability of perfection in this way seem potentially useful (although we make no claims for the practical realism of the numbers we use). We also note that the general approach here to the problem of expressing and using limited prior belief in a Bayesian analysis may have wider applicability than to the problem we have addressed.},
  eventtitle = {2015 {{IEEE}} 26th {{International Symposium}} on {{Software Reliability Engineering}} ({{ISSRE}})},
  keywords = {1oo2 systems,Bayes methods,conservative claims,notion,Phase frequency detector,Probability of perfection,reliability assessment,Safety,Software reliability,Testing,Uncertainty},
  file = {/Users/bohrok/Zotero/storage/PXR6N8YJ/Zhao et al. - 2015 - Conservative claims about the probability of perfection of software-based systems.pdf;/Users/bohrok/Zotero/storage/72Z4CYPD/7381807.html}
}

@inproceedings{zhaoLog20FullyAutomated2017,
  title = {Log20: {{Fully Automated Optimal Placement}} of {{Log Printing Statements}} under {{Specified Overhead Threshold}}},
  booktitle = {Proceedings of the 26th {{Symposium}} on {{Operating Systems Principles}}},
  author = {Zhao, Xu and Rodrigues, Kirk and Luo, Yu and Stumm, Michael and Yuan, Ding and Zhou, Yuanyuan},
  date = {2017},
  series = {{{SOSP}} '17},
  pages = {565--581},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3132747.3132778},
  url = {https://doi.org/10.1145/3132747.3132778},
  abstract = {When systems fail in production environments, log data is often the only information available to programmers for postmortem debugging. Consequently, programmers' decision on where to place a log printing statement is of crucial importance, as it directly affects how effective and efficient postmortem debugging can be. This paper presents Log20, a tool that determines a near optimal placement of log printing statements under the constraint of adding less than a specified amount of performance overhead. Log20 does this in an automated way without any human involvement. Guided by information theory, the core of our algorithm measures how effective each log printing statement is in disambiguating code paths. To do so, it uses the frequencies of different execution paths that are collected from a production environment by a low-overhead tracing library. We evaluated Log20 on HDFS, HBase, Cassandra, and ZooKeeper, and observed that Log20 is substantially more efficient in code path disambiguation compared to the developers' manually placed log printing statements. Log20 can also output a curve showing the trade-off between the informativeness of the logs and the performance slowdown, so that a developer can choose the right balance.},
  isbn = {978-1-4503-5085-3},
  venue = {Shanghai, China},
  keywords = {distributed systems,information theory,Log placement,notion}
}

@article{zhaoPreciseLearningSource2022,
  title = {Precise {{Learning}} of {{Source Code Contextual Semantics}} via {{Hierarchical Dependence Structure}} and {{Graph Attention Networks}}},
  author = {Zhao, Zhehao and Yang, Bo and Li, Ge and Liu, Huai and Jin, Zhi},
  date = {2022-02},
  journaltitle = {Journal of Systems and Software},
  volume = {184},
  pages = {111108},
  publisher = {Elsevier BV},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2021.111108},
  url = {http://dx.doi.org/10.1016/j.jss.2021.111108},
  keywords = {notion}
}

@article{zhaoSendHardestProblems2019,
  title = {Send {{Hardest Problems My Way}}: {{Probabilistic Path Prioritization}} for {{Hybrid Fuzzing}}},
  author = {Zhao, Lei and Duan, Yue and Yin, Heng and Xuan, J.},
  date = {2019},
  journaltitle = {Proceedings 2019 Network and Distributed System Security Symposium},
  doi = {10.14722/ndss.2019.23504},
  keywords = {notion}
}

@inproceedings{zhaoSlicingConcurrentJava1999,
  title = {Slicing Concurrent {{Java}} Programs},
  booktitle = {Proceedings {{Seventh International Workshop}} on {{Program Comprehension}}},
  author = {Zhao, Jianjun},
  date = {1999},
  pages = {126--133},
  issn = {1092-8138},
  doi = {10.1109/WPC.1999.777751},
  abstract = {Although many slicing algorithms have been proposed for object oriented programs, no slicing algorithm has been proposed which can be used to handle the problem of slicing concurrent Java programs correctly. We propose a slicing algorithm for concurrent Java programs. To slice concurrent Java programs, we present a dependence based representation called multithreaded dependence graph, which extends previous dependence graphs to represent concurrent Java programs. We also show how static slices of a concurrent Java program can be computed efficiently based on its multithreaded dependence graph},
  keywords = {Concurrent Program,Java,notion,Program Slicing}
}

@inproceedings{zhengFLACKCounterexampleguidedFault2021,
  title = {{{FLACK}}: {{Counterexample-guided}} Fault Localization for Alloy Models},
  booktitle = {2021 {{IEEE}}/{{ACM}} 43rd {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Zheng, Guolong and Nguyen, ThanhVu and Brida, Simón Gutiérrez and Regis, Germán and Frias, Marcelo F and Aguirre, Nazareno and Bagheri, Hamid},
  date = {2021},
  pages = {637--648},
  publisher = {IEEE},
  keywords = {notion}
}

@inproceedings{zhengWujiAutomaticOnline2019,
  title = {Wuji: {{Automatic Online Combat Game Testing Using Evolutionary Deep Reinforcement Learning}}},
  booktitle = {2019 34th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Zheng, Y. and Xie, X. and Su, T. and Ma, L. and Hao, J. and Meng, Z. and Liu, Y. and Shen, R. and Chen, Y. and Fan, C.},
  date = {2019},
  pages = {772--784},
  keywords = {notion}
}

@inproceedings{zhifengyuHiddenDependenciesProgram2001,
  title = {Hidden Dependencies in Program Comprehension and Change Propagation},
  booktitle = {Proceedings 9th {{International Workshop}} on {{Program Comprehension}}. {{IWPC}} 2001},
  author = {{Zhifeng Yu} and Rajlich, V.},
  date = {2001-05},
  pages = {293--299},
  issn = {1092-8138},
  doi = {10.1109/WPC.2001.921739},
  keywords = {abstract system dependency graphs,Collaborative work,Computer science,data flow,data flow analysis,graphs,hidden dependencies,Independent component analysis,independent components,Java,large software systems,notion,Object oriented modeling,program comprehension,program dependency analysis,program understanding,Programming profession,Protocols,reverse engineering,software change propagation,software components,software maintenance,Software maintenance,Software systems,subroutines,Variable speed drives,warning algorithm}
}

@inproceedings{zhouBayesianAdaptationCovariate2021,
  title = {Bayesian {{Adaptation}} for {{Covariate Shift}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhou, Aurick and Levine, Sergey},
  editor = {Beygelzimer, A. and Dauphin, Y. and Liang, P. and Vaughan, J. Wortman},
  date = {2021},
  url = {https://openreview.net/forum?id=15HPeY8MGQ},
  keywords = {notion}
}

@misc{zhouConfInLogLeveragingSoftware2021,
  title = {{{ConfInLog}}: {{Leveraging Software Logs}} to {{Infer Configuration Constraints}}},
  author = {Zhou, Shulin and Liu, Xiaodong and Li, Shanshan and Jia, Zhouyang and Zhang, Yuanliang and Wang, Teng and Li, Wang and Liao, Xiangke},
  date = {2021},
  keywords = {notion}
}

@incollection{zhouDevignEffectiveVulnerability2019,
  title = {Devign: {{Effective Vulnerability Identification}} by {{Learning Comprehensive Program Semantics}} via {{Graph Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Zhou, Yaqin and Liu, Shangqing and Siow, Jingkai and Du, Xiaoning and Liu, Yang},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and family=Alché-Buc, given=F., prefix=d', useprefix=false and Fox, E. and Garnett, R.},
  date = {2019},
  pages = {10197--10207},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/9209-devign-effective-vulnerability-identification-by-learning-comprehensive-program-semantics-via-graph-neural-networks},
  keywords = {notion}
}

@inproceedings{zhouWhereShouldBugs2012,
  title = {Where {{Should}} the {{Bugs Be Fixed}}? - {{More Accurate Information Retrieval-based Bug Localization Based}} on {{Bug Reports}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Software Engineering}}},
  author = {Zhou, Jian and Zhang, Hongyu and Lo, David},
  date = {2012},
  series = {{{ICSE}} '12},
  pages = {14--24},
  publisher = {IEEE Press},
  location = {Piscataway, NJ, USA},
  url = {http://dl.acm.org/citation.cfm?id=2337223.2337226},
  isbn = {978-1-4673-1067-3},
  venue = {Zurich, Switzerland},
  keywords = {notion}
}

@inproceedings{zhuMassivelyParallelHighly2020,
  title = {Massively {{Parallel}}, {{Highly Efficient}}, but {{What About}} the {{Test Suite Quality}}? {{Applying Mutation Testing}} to {{GPU Programs}}},
  booktitle = {2020 {{IEEE}} 13th {{International Conference}} on {{Software Testing}}, {{Validation}} and {{Verification}} ({{ICST}})},
  author = {Zhu, Q. and Zaidman, A.},
  date = {2020},
  pages = {209--219},
  doi = {10.1109/ICST46399.2020.00030},
  keywords = {notion}
}

@inproceedings{zhuRegressionGreyboxFuzzing2021,
  title = {Regression {{Greybox Fuzzing}}},
  booktitle = {Proceedings of the 28th {{ACM Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Zhu, Xiaogang and Böhme, Marcel},
  date = {2021},
  series = {{{CCS}}},
  pages = {12},
  keywords = {notion}
}

@inproceedings{zhuSyntaxGuidedEditDecoder2021,
  title = {A {{Syntax-Guided Edit Decoder}} for {{Neural Program Repair}}},
  booktitle = {Proceedings of the 29th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Zhu, Qihao and Sun, Zeyu and Xiao, Yuan-an and Zhang, Wenjie and Yuan, Kang and Xiong, Yingfei and Zhang, Lu},
  date = {2021},
  series = {{{ESEC}}/{{FSE}} 2021},
  pages = {341--353},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3468264.3468544},
  url = {https://doi.org/10.1145/3468264.3468544},
  abstract = {Automated Program Repair (APR) helps improve the efficiency of software development and maintenance. Recent APR techniques use deep learning, particularly the encoder-decoder architecture, to generate patches. Though existing DL-based APR approaches have proposed different encoder architectures, the decoder remains to be the standard one, which generates a sequence of tokens one by one to replace the faulty statement. This decoder has multiple limitations: 1) allowing to generate syntactically incorrect programs, 2) inefficiently representing small edits, and 3) not being able to generate project-specific identifiers. In this paper, we propose Recoder, a syntax-guided edit decoder with placeholder generation. Recoder is novel in multiple aspects: 1) Recoder generates edits rather than modified code, allowing efficient representation of small edits; 2) Recoder is syntax-guided, with the novel provider/decider architecture to ensure the syntactic correctness of the patched program and accurate generation; 3) Recoder generates placeholders that could be instantiated as project-specific identifiers later. We conduct experiments to evaluate Recoder on 395 bugs from Defects4J v1.2, 420 additional bugs from Defects4J v2.0, 297 bugs from IntroClassJava and 40 bugs from QuixBugs. Our results show that Recoder repairs 53 bugs on Defects4J v1.2, which achieves 26.2\% (11 bugs) improvement over the previous state-of-the-art approach for single-hunk bugs (TBar). Importantly, to our knowledge, Recoder is the first DL-based APR approach that has outperformed the traditional APR approaches on this benchmark. Furthermore, Recoder repairs 19 bugs on the additional bugs from Defects4J v2.0, which is 137.5\% (11 bugs) more than TBar and 850\% (17 bugs) more than SimFix. Recoder also achieves 775\% (31 bugs) and 30.8\% (4 bugs) improvement on IntroClassJava and QuixBugs over the baselines respectively. These results suggest that Recoder has better generalizability than existing APR approaches.},
  isbn = {978-1-4503-8562-6},
  venue = {Athens, Greece},
  keywords = {Automated program repair,Neural networks,notion}
}

@inproceedings{zhuToolsBenchmarksAutomated2019,
  title = {Tools and {{Benchmarks}} for {{Automated Log Parsing}}},
  booktitle = {2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}}: {{Software Engineering}} in {{Practice}} ({{ICSE-SEIP}})},
  author = {Zhu, Jieming and He, Shilin and Liu, Jinyang and He, Pinjia and Xie, Qi and Zheng, Zibin and Lyu, Michael R.},
  date = {2019},
  pages = {121--130},
  doi = {10.1109/ICSE-SEIP.2019.00021},
  keywords = {notion}
}

@inproceedings{ziccardiEPCExtendedPath2015,
  title = {{{EPC}}: {{Extended Path Coverage}} for {{Measurement-Based Probabilistic Timing Analysis}}},
  booktitle = {2015 {{IEEE Real-Time Systems Symposium}}},
  author = {Ziccardi, Marco and Mezzetti, Enrico and Vardanega, Tullio and Abella, Jaume and Cazorla, Francisco Javier},
  date = {2015},
  pages = {338--349},
  doi = {10.1109/RTSS.2015.39},
  keywords = {notion}
}

@article{zouEmpiricalStudyFault2021,
  title = {An {{Empirical Study}} of {{Fault Localization Families}} and {{Their Combinations}}},
  author = {Zou, Daming and Liang, Jingjing and Xiong, Yingfei and Ernst, Michael D. and Zhang, Lu},
  date = {2021},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {47},
  number = {2},
  pages = {332--347},
  doi = {10.1109/TSE.2019.2892102},
  keywords = {notion}
}

@inproceedings{zulianiBayesianStatisticalModel2010,
  title = {Bayesian {{Statistical Model Checking}} with {{Application}} to {{Simulink}}/{{Stateflow Verification}}},
  booktitle = {Proceedings of the 13th {{ACM International Conference}} on {{Hybrid Systems}}: {{Computation}} and {{Control}}},
  author = {Zuliani, Paolo and Platzer, André and Clarke, Edmund M.},
  date = {2010},
  series = {{{HSCC}} '10},
  pages = {243--252},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1755952.1755987},
  url = {https://doi.org/10.1145/1755952.1755987},
  abstract = {We address the problem of model checking stochastic systems, i.e. checking whether a stochastic system satisfies a certain temporal property with a probability greater (or smaller) than a fixed threshold. In particular, we present a novel Statistical Model Checking (SMC) approach based on Bayesian statistics. We show that our approach is feasible for hybrid systems with stochastic transitions, a generalization of Simulink/Stateflow models. Standard approaches to stochastic (discrete) systems require numerical solutions for large optimization problems and quickly become infeasible with larger state spaces. Generalizations of these techniques to hybrid systems with stochastic effects are even more challenging. The SMC approach was pioneered by Younes and Simmons in the discrete and non-Bayesian case. It solves the verification problem by combining randomized sampling of system traces (which is very efficient for Simulink/Stateflow) with hypothesis testing or estimation. We believe SMC is essential for scaling up to large Stateflow/Simulink models. While the answer to the verification problem is not guaranteed to be correct, we prove that Bayesian SMC can make the probability of giving a wrong answer arbitrarily small. The advantage is that answers can usually be obtained much faster than with standard, exhaustive model checking techniques. We apply our Bayesian SMC approach to a representative example of stochastic discrete-time hybrid system models in Stateflow/Simulink: a fuel control system featuring hybrid behavior and fault tolerance. We show that our technique enables faster verification than state-of-the-art statistical techniques, while retaining the same error bounds. We emphasize that Bayesian SMC is by no means restricted to Stateflow/Simulink models: we have in fact successfully applied it to very large stochastic models from Systems Biology.},
  isbn = {978-1-60558-955-8},
  venue = {Stockholm, Sweden},
  keywords = {bayesian statis tics,hybrid systems,notion,probabilistic model checking,statistical model checking,stochastic systems}
}

@article{zuoMoreEfficientStatistical2023,
  title = {Toward {{More Efficient Statistical Debugging}} with {{Abstraction Refinement}}},
  author = {Zuo, Zhiqiang and Niu, Xintao and Zhang, Siyi and Fang, Lu and Khoo, Siau Cheng and Lu, Shan and Sun, Chengnian and Xu, Guoqing Harry},
  date = {2023},
  journaltitle = {ACM Transactions on Software Engineering and Methodology},
  volume = {32},
  number = {2},
  pages = {1--38},
  publisher = {ACM New York, NY},
  keywords = {notion}
}
